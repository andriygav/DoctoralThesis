В главе~\ref{chapter:complexity} был разработан единый теоретический аппарат для формализации соотношения между сложностью модели и сложностью данных. Ключевым результатом этой главы является введение формальных определений меры сложности выборки $\mu_D(D)$ и меры сложности модели $\mu_f(f)$, а также установление критерия обучаемости модели на выборке: $\mu_f(f) \leq \mu_D(D)$. 

В разделе~\ref{chapter:complexity:sample-size} главы~\ref{chapter:complexity} было показано, что частным случаем условной меры сложности данных $\mu_D(D|f)$ является достаточный размер выборки $m^*$~--- минимальный объем данных из выборки $D$, необходимый для обучения модели $f$. Для простой генеральной совокупности $\Gamma_C$, состоящей из объектов одинаковой сложности $C$, мера сложности выборки принимает вид $\mu_D(D) = C \cdot |D|$, что устанавливает прямую связь между размером выборки и ее сложностью.

В разделе~\ref{chapter:complexity:loss} главы~\ref{chapter:complexity} была введена ландшафтная мера сложности модели $\mu_f(f|D)$, определяемая через спектральные свойства матриц Гессе функции потерь. Установлено, что анализ сходимости ландшафта оптимизационной задачи при увеличении объема выборки сводится к анализу спектральной нормы матрицы Гессе, что позволяет количественно оценить влияние добавления новых объектов данных на локальную геометрию функции потерь в окрестности оптимума.

В главе~\ref{chapter:gesian} были получены конкретные оценки спектральных норм матриц Гессе для различных архитектур нейронных сетей. Теорема~\ref{theorem:hess-kiselev-theorem} устанавливает оценку для полносвязных сетей: $\|\mathbf{H}_i(\boldsymbol{\theta})\|_2 \propto L(hM)^{2L}$, теоремы~\ref{thm:1Dconv} и~\ref{thm:2Dconv}~--- для сверточных сетей, а теорема~\ref{thm:transformer_hessian_estimate}~--- для трансформерных моделей. Эти оценки обеспечивают вычислимо осуществимые методы оценки ландшафтной меры сложности модели $\mu_f(f|D)$, что создает теоретическую основу для практического применения формализма, разработанного в главе~\ref{chapter:complexity}.

Однако для практического использования введенного теоретического аппарата необходимо решить следующую задачу: как на основе имеющихся данных и выбранной модели определить конкретное численное значение достаточного размера выборки $m^*$? Теоретические результаты предыдущих глав устанавливают формальные связи между сложностью модели и сложностью данных, но не предоставляют алгоритмических процедур для вычисления $m^*$ в конкретных прикладных задачах. Более того, для моделей глубокого обучения прямое вычисление матриц Гессе и оценка ландшафтной меры сложности остаются вычислительно дорогостоящими, что ограничивает практическую применимость теоретических результатов.

В рамках настоящей главы рассматриваются различные методы определения достаточного размера выборки для различных моделей, от линейных до моделей глубокого обучения. Предлагаемые методы опираются на теоретический аппарат, разработанный в предыдущих главах, и обеспечивают практические инструменты для оценки необходимого объема данных при планировании экспериментов. В отличие от теоретических оценок, основанных на анализе матриц Гессе, предлагаемые методы используют наблюдаемые характеристики процесса обучения для определения момента, когда добавление новых объектов данных перестает существенно влиять на свойства модели.

Планирование эксперимента требует оценки минимального размера выборки: числа выполненных измерений набора характеристик, необходимых для построения сформулированных условий. Выбор метода оценки размера выборки зависит от решаемой задачи, которая определяет формулировку статистической гипотезы и статистики для ее проверки. 

В целом существуют различные подходы к определению достаточного размера выборки, такие как статистические, байесовские и эвристические методы. Каждый из этих подходов имеет свои преимущества и ограничения, которые определяют область их применимости.

Статистические методы предполагают, что выборка соответствует некоторым предварительным условиям, сформулированным ранее. Эти условия сформулированы как статистический критерий~\cite{self1988,self1992,shieh2000,demidenko2007}. Метод оценки размера выборки, связанный с этим критерием, гарантирует достижение фиксированной статистической мощности~$1-\beta$ со степенью ошибки первого рода, не превышающей установленное значение~$\alpha$. Такой размер выборки называется достаточным.

Однако практическое применение методов оценки размера выборки предполагает, что модель соответствует измеренным данным~\cite{kloek1975}. Эти модели выбираются в соответствии с постановкой задачи регрессии или классификации. В настоящей главе рассматриваются обобщенные линейные модели.

В работе~\cite{self1992} предложен подход к оценке мощности и размера выборки на основе теста отношения максимального правдоподобия. Этот подход оказался более точным для ряда независимых переменных. В работе~\cite{shieh2005} предложен метод оценки мощности для статистики Вальда. В работе~\cite{motrenko2014} в случае логистической регрессии предлагается использовать метод, использующий кривую ROC-AUC и концепцию сдвига.

Классические методы~\cite{self1988,self1992,shieh2000,shieh2005,demidenko2007} имеют ряд ограничений, связанных с практическим применением. Чтобы оценить размер выборки, необходимо знать дисперсию оценки параметра или, в более общем случае, иметь оценку параметра нецентральности в распределении статистики, используемой при альтернативной гипотезе. Указанные методы не предоставляют алгоритмических процедур для получения этих значений. Кроме того, дисперсия оценки и параметр нецентральности оцениваются с неопределенностью, влияние которой на результат оценки размера выборки не учитывается.

Статистические методы позволяют оценить размер выборки на основе предположений о распределении данных и информации о соответствии между наблюдаемыми значениями и предположениями нулевой гипотезы.

Когда размер исследуемой выборки является достаточным или чрезмерным, возможно применение методов, основанных на наблюдении изменения определенной характеристики процедуры построения модели при увеличении размера выборки. В частности, наблюдая за соотношением качества прогнозирования с контрольной выборкой и обучающей выборкой~\cite{motrenko2014}, определяется достаточный размер выборки, который соответствует началу переобучения.

В работе~\cite{qumsiyeh2013} для оценки достаточного размера выборки используется процедура бутстрапа. Превышение текущего размера выборки проверяется на основе анализа доверительных интервалов оцениваемого параметра. Ширина доверительного интервала с разными значениями объема выборки оценивается с помощью метода бутстрапа. Для этого выборки меньшего размера отбираются заданное число раз и вычисляется доверительный интервал ошибки при оценке параметра модели. Размер выборки считается достаточным, если ширина доверительного интервала не превышает заранее установленного значения.

Перечисленные выше ограничения статистических методов оценки размера выборки подробно исследуются в байесовской процедуре~\cite{lindley1997,rubin1998,wang2002}. В рамках данного подхода оценка размера выборки определяется на основе максимизации ожидаемого значения некоторой функции качества~\cite{lindley1997}. Функция качества может включать в себя явные функции распределения параметров и штрафы за увеличение размера выборки.

Альтернативой подходам~\cite{wang2002}, основанным на функции качества, является выбор размера выборки путем установления ограничений на определенный критерий качества оценки параметров модели. Примеры критериев: критерий средней апостериорной дисперсии (AVPC), критерий средней длины (ALC), критерий среднего покрытия (ACC). Для каждого перечисленного критерия оценка размера выборки определяется как минимальное значение размера выборки, для которого ожидаемое значение выбранного критерия не превышает какого-либо фиксированного порога.

В работе~\cite{motrenko2014} предлагается считать размер выборки достаточным, если расстояние Кульбака-Лейблера между распределениями, оцененными на основе подвыборок такого размера, достаточно мало. Такой подход не требует дальнейшего обобщения в случае нескольких переменных. Кроме того, оценка может производиться как при наличии предположений о распределении данных, так и при их отсутствии. Недостаток этого подхода заключается в том, что количественная оценка может быть получена только при чрезмерно большом размере выборки.

\section{Статистические методы определения достаточного размера выборки}

В настоящем разделе рассматриваются статистические методы определения достаточного размера выборки для обобщенных линейных моделей. Основой данных методов является использование информационной матрицы Фишера и статистических критериев для проверки гипотез о параметрах модели.

Рассмотрим выборку размера~$m$:
\[
\label{eq:ps:1}
\begin{aligned}
	\mathfrak{D}_{m} = \{\textbf{x}_i, y_i\}_{i = 1}^{m},
\end{aligned}
\]
где~$\textbf{x}_i\in \mathbb{R}^{n}$~--- вектор признаков,~$y_i\in \mathbb{Y}$~--- целевая переменная. Вектор признаков~$\textbf{x} = [\textbf{u}, \textbf{v}]$ объединяет~$\textbf{u}_i\in \mathbb{R}^{k}$ и~$\textbf{v}_i\in \mathbb{R}^{n-k}$.
Выборка~$\mathfrak{D}_{m}$ случайным образом разделяется на обучающую и тестовую части:
\[
\label{eq:ps:2}
\begin{aligned}
	\mathfrak{D}_{\mathcal{T}_{m}} = \{\textbf{x}_i, \textbf{y}_i\}_{i \in \mathcal{T}_{m}}, \quad \mathfrak{D}_{\mathcal{L}_{m}} = \{\textbf{x}_i, \textbf{y}_i\}_{i \in \mathcal{L}_{m}}, \quad  \mathcal{T}_{m}\sqcup\mathcal{L}_{m} = \{1, ..., m\}.
\end{aligned}
\]
Определим параметрическое семейство функций для аппроксимации неизвестного распределения~$p(y|\textbf{x}, \mathfrak{D}_{\mathcal{L}_{m}})$:
\[
\label{eq:ps:3}
\begin{aligned}
	\mathfrak{F} = \left\{f\left(y,\textbf{x}, \textbf{w}\right)|\textbf{w}\in\mathbb{W}, \int_{y\in \mathbb{Y}, \textbf{x}\in\mathbb{R}^{n}}f\left(y, \textbf{x}, \textbf{w}\right)dyd\textbf{x}=1\right\}.
\end{aligned}
\]

Для модели~$f$ с вектором параметров~$\textbf{w}$ определим функцию правдоподобия и логарифм функции правдоподобия выборки~$\mathfrak{D}$:
\[
\label{eq:ps:4}
\begin{aligned}
	L\left(\mathfrak{D}, \textbf{w}\right) = \prod f\left(y,\textbf{x}, \textbf{w}\right),\quad l\left(\mathfrak{D}, \textbf{w}\right) = \sum \log f\left(y,\textbf{x}, \textbf{w}\right),
\end{aligned}
\]
где~$f(y,\textbf{x}, \textbf{w})$ является оценкой плотности вероятности для объекта $(y, \textbf{x})$ при заданном векторе параметров~$\textbf{w}$.

Используя принцип максимального правдоподобия для оценки параметров~$\textbf{w}$:
\[
\label{eq:ps:5}
\begin{aligned}
	\hat{\textbf{w}} = \arg\max_{\textbf{w}\in\mathbb{W}}L\left(\mathfrak{D}_{\mathcal{L}}, \textbf{w}\right).
\end{aligned}
\]

Информационная матрица Фишера (англ. Fisher Information Matrix) имеет вид:
\[
\label{eq:ps:6}
\begin{aligned}
	\textbf{I}\left(\mathfrak{D}, \textbf{w}\right) = -\nabla\nabla^{\mathsf{T}}l\left(\mathfrak{D}, \textbf{w}\right), \quad  \textbf{V} = \textbf{I}^{-1}\left(\mathfrak{D}, \textbf{m}\right),
\end{aligned}
\]
где~$\textbf{V}$~--- ковариационная матрица оценок параметров. Статистические методы и байесовские методы используют информационную матрицу Фишера для оценки размера выборки.

Основным преимуществом методов, основанных на статистике, является их способность оценивать достаточный размер выборки при недостаточном наборе данных. Они позволяют прогнозировать необходимое число объектов на ранней стадии эксперимента.

Рассмотрим обобщенную линейную модель, в которой плотность распределения целевой переменной задается выражением
\[
\label{eq:sb:1}
\begin{aligned}
	p(y|\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}) = \exp\bigl(y\theta- b(\theta) + c\left(y\right)\bigr),
\end{aligned}
\]
где~$\theta$ является каноническим параметром распределения, получаемым с помощью функции связи~$\theta=\theta\bigr(\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}\bigr)$, а функции~$b(\theta)$ и~$c(y)$ определяют конкретный тип распределения.

Тестируемая гипотеза
\[
\label{eq:sb:2}
\begin{aligned}
	H_0: \textbf{m}_{u} = \textbf{m}^0_{u}, \quad H_1: \textbf{m}_{u} \not= \textbf{m}^0_{u}.
\end{aligned}
\]

Пусть статистики~$S_{m,u}\left(\textbf{w}_{u}, \textbf{w}_{v}\right)$ и~$S_{m,v}\left(\textbf{w}_{u}, \textbf{w}_{v}\right)$ представляют собой производные логарифма правдоподобия выборки~$\mathfrak{D}_{m}$ по параметрам~$\textbf{w}_{u}$ и~$\textbf{w}_{v}$ соответственно.
Рассмотрим~$\textbf{s}_{m} = S_{m,u}\left(\textbf{m}^{0}_{u}, \hat{\textbf{w}}^{0}_{v}\right)$, где~$\hat{\textbf{w}}^{0}_{v}$ получается из уравнения
\[
\label{eq:sb:3}
\begin{aligned}
	S_{m,v}\left(\textbf{m}^{0}_{u}, \textbf{w}_{v}\right) = 0.
\end{aligned}
\]
Статистика множителей Лагранжа (англ. Lagrange Multiplier) определяется как
\[
\label{eq:sb:4}
\begin{aligned}
	LM = \textbf{s}^{\mathsf{T}}_{m}\textbf{Q}_{m}^{-1}\textbf{s}_{m},
\end{aligned}
\]
где~$\textbf{Q}_{m}$~--- ковариационная матрица вектора~$\textbf{s}_{m}$.
	
В случае истинности гипотезы~$H_0$ статистика~$LM$ асимптотически имеет центральное распределение~$\chi^2(k)$. В~\cite{self1988} показано, что при альтернативной гипотезе~$H_1$ статистика~$LM$ асимптотически имеет нецентральное распределение~$\chi^2(k,\gamma)$, где~$\gamma$ является параметром нецентральности
\[
\label{eq:sb:5}
\begin{aligned}
	\gamma = \bm{\xi}_{m}^{\mathsf{T}}\bm{\Sigma}^{-1}_{m}\bm{\xi}_{m} = m\bm{\xi}^{\mathsf{T}}\bm{\Sigma}^{-1}\bm{\xi}= m\gamma^0,
\end{aligned}
\]
где~$\bm{\xi}_{m}$ и~$\bm{\Sigma}_{m}$~--- соответственно вектор математического ожидания и матрица ковариации~$\textbf{s}_{m}$. Обозначим~$\bm{\xi}_1 = \bm{\xi}$, ~$\bm{\Sigma}_1 = \bm{\Sigma}$. 
	
Альтернативный метод получения~$\gamma$ включает условия на уровне значимости~$\alpha$ и вероятность ошибки II рода~$\beta$:
\[
\label{eq:sb:6}
\begin{aligned}
	\gamma^*:\chi^2_{k, 1-\alpha} = \chi^2_{k, \beta}\left(\gamma\right).
\end{aligned}
\]
Используя соотношения~\eqref{eq:sb:5} и~\eqref{eq:sb:6}, получаем
\[
\label{eq:sb:7}
\begin{aligned}
	m^* = \frac{\gamma^*}{\gamma^0}.
\end{aligned}
\]
Полученное значение~$m^*$ представляет собой достаточный минимальный размер выборки, необходимый для различения вектора~$\textbf{m}_{u}$ от~$\textbf{m}^0_{u}$ с заданными уровнями значимости~$\alpha$ и мощности~$1-\beta$.

Рассмотрим случай, когда правдоподобие выборки задается выражением
\[
\label{eq:sb:8}
\begin{aligned}
	p(y|\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}) = \exp\left(\frac{y\theta- b(\theta)}{a(\phi)} + c\left(y, \phi\right)\right),
\end{aligned}
\]
где~$\theta$ является параметром распределения, который вычисляется с помощью функции связи~$\theta=\theta\bigr(\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}\bigr)$.

Тестируемая гипотеза
\[
\label{eq:sb:9}
\begin{aligned}
	H_0: \textbf{m}_{u} = \textbf{m}^0_{u}, \quad H_1: \textbf{m}_{u} \not= \textbf{m}^0_{u}.
\end{aligned}
\]
Определим логарифм статистики отношения правдоподобий:
\[
\label{eq:sb:10}
\begin{aligned}
	LR = 2\Big(l\left(\mathfrak{D}, \hat{\textbf{w}}\right) - l\left(\mathfrak{D}, \hat{\textbf{w}}^0\right)\Big),
\end{aligned}
\]
где~$\hat{\textbf{w}} = [\hat{\textbf{w}}_{u},\hat{\textbf{w}}_{v}]$~--- вектор параметров, максимизирующий правдоподобие \eqref{eq:sb:8}, а~$\hat{\textbf{w}}^{0} = [\textbf{m}^{0}_{u},\hat{\textbf{w}}^{0}_{v}]$~--- вектор параметров, максимизирующий правдоподобие \eqref{eq:sb:8} при фиксированном подвекторе параметров~$\textbf{m}^{0}_{u}$.

В случае истинности гипотезы~$H_0$ статистика~$LR$ асимптотически имеет центральное распределение~$\chi^2(k)$. В~\cite{shieh2000} показано, что при альтернативной гипотезе~$H_1$ статистика~$LR$ асимптотически имеет нецентральное распределение~$\chi^2(k,\gamma)$, где~$\gamma$ является параметром нецентральности
\[
\label{eq:sb:11}
\begin{aligned}
	\gamma = m\Delta^*, \quad \Delta^* = \mathsf{E}\left[2a^{-1}(\phi)\left\{\left(\theta - \theta^*\right)\nabla b(\theta) - b(\theta) + b(\theta^*)\right\}\right], 
\end{aligned}
\]
где параметры~$\theta$ и~$\theta^*$ рассчитываются с использованием параметров~$\textbf{w} = [\textbf{w}_{u}, \textbf{w}_{v}]$ и~$\textbf{w}^* = [\textbf{w}^{0}_{u}, \textbf{w}^{*}_{v}]$. Параметры~$\textbf{w}^{*}_{v}$ вычисляются на основе решения уравнения
\[
\label{eq:sb:12}
\begin{aligned}
	\lim_{m\to\infty}m^{-1}\mathsf{E}\left(\frac{\partial l\left(\mathfrak{D}, \left[\textbf{m}^{0}_{u}, \textbf{w}_{v}\right]\right)}{\partial \textbf{w}_{v}}\right) = 0.
\end{aligned}
\]
	
Тогда с учетом~$\alpha$ и~$\beta$ достаточный размер выборки~$m^*$ вычисляется
\[
\label{eq:sb:13}
\begin{aligned}
	m^* = \frac{\gamma^*}{\Delta^*}, \quad \gamma^*:\chi^2_{k, 1-\alpha} = \chi^2_{k, \beta}\left(\gamma\right), 
\end{aligned}
\]
где~$\chi^2_{k, 1-\alpha}$ и~$\chi^2_{k, \beta}\left(\gamma^*\right)$~--- квантили распределений~$\chi^{2}_k$ и~$\chi^2_{k}\left(\gamma^*\right)$ соответственно.
Правдоподобие выборки:
\[
\label{eq:sb:14}
\begin{aligned}
	p(y|\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}) = \exp\left(\frac{y\theta- b(\theta)}{a(\phi)} + c\left(y, \phi\right)\right),
\end{aligned}
\]
где~$\theta$ является параметром распределения, который вычисляется с помощью функции связи~$\theta=\theta\bigr(\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}\bigr)$.

Тестируемая гипотеза:
\[
\label{eq:sb:15}
\begin{aligned}
	H_0: \textbf{m}_{u} = \textbf{m}_{u}^{0}, \quad H_1: \textbf{m}_{u} \not=\textbf{m}_{u}^{0}.
\end{aligned}
\]
Тест Вальда для гипотезы:
\[
\label{eq:sb:16}
\begin{aligned}
	W = \left(\hat{\textbf{w}}_{u} - \textbf{m}_{u}^{0}\right)^{\mathsf{T}}\hat{\textbf{V}}_{u}^{-1}\left(\hat{\textbf{w}}_{u} - \textbf{m}_{u}^{0}\right),
\end{aligned}
\]
где~$\hat{\textbf{w}} = [\hat{\textbf{w}}_{u},\hat{\textbf{w}}_{v}]$ вектор параметров, который максимизирует правдоподобие выборки \eqref{eq:sb:14}, где матрица~$\hat{\textbf{V}}_u$ задается в выражении \eqref{eq:ps:6}.

В случае истинности гипотезы~$H_0$ статистика Вальда~$W$ асимптотически имеет центральное распределение~$\chi^2(k)$. В~\cite{shieh2005} показано, что в случае истинности альтернативной гипотезы~$H_1$ статистика Вальда~$W$ асимптотически имеет нецентральное распределение~$\chi^2(k,\gamma)$ с параметром нецентральности~$\gamma$:
\[
\label{eq:sb:17}
\begin{aligned}
	\gamma = m\delta, \quad \delta = \left(\hat{\textbf{w}}_{u} - \textbf{m}_{u}^{0}\right)^{\mathsf{T}}\bm{\Sigma}^{-1}_u\left(\hat{\textbf{w}}_{u} - \textbf{m}_{u}^{0}\right), \quad \bm{\Sigma}_u = m\hat{\textbf{V}}_u.
\end{aligned}
\]

Используя заданный уровень значимости~$\alpha$ и заданную ошибку второго рода~$\beta$, определим оптимальный размер выборки:
\[
\label{eq:sb:18}
\begin{aligned}
	m^* = \frac{\gamma^*}{\delta}, \quad \gamma^*:\chi^2_{k, 1-\alpha^{*}} = \chi^2_{k, \beta}\left(\gamma\right),
\end{aligned}
\]
где~$\chi^2_{k, 1-\alpha^*}$ и~$\chi^2_{k, \beta}\left(\gamma^*\right)$~--- квантили соответствующих распределений, а параметр~$\alpha^*$ представляет собой поправку на уровень значимости:
\[
\label{eq:sb:19}
\begin{aligned}
	\alpha^* = P\left(\bm{\xi}^{\mathsf{T}}\bm{\Sigma}^{*-1} \bm{\xi} > \chi^2_{k,1 - \alpha}\right), \quad \Sigma^* = \textbf{I}^{-1}\left(\mathfrak{D}, \textbf{w}^*\right),
\end{aligned}
\]
где~$\textbf{w}^{*} = \left[\textbf{m}_{u}^{0}, \textbf{w}^{*}_v\right]$ представляет собой решение уравнения
\[
\label{eq:sb:20}
\begin{aligned}
	\lim_{m\to\infty}m^{-1}\mathsf{E}\left(\frac{\partial l\left(\mathfrak{D}, \left[ \textbf{m}_{u}^{0}, \textbf{w}_{v}\right]\right)}{\partial \textbf{w}_{v}}\right) = 0.
\end{aligned}
\]

Статистические методы, рассмотренные выше, требуют знания дисперсии оценки параметра или параметра нецентральности, что ограничивает их практическое применение. В следующем разделе рассматриваются эвристические методы, которые не требуют таких предположений и могут применяться в более широком классе задач.

\section{Эвристические методы определения достаточного размера выборки}

В настоящем разделе рассматриваются эвристические методы определения достаточного размера выборки, основанные на популярных статистических эвристиках, таких как бутстрап, перекрестная проверка и задание функции полезности. В отличие от статистических методов, эвристические подходы не требуют строгих предположений о распределении данных и могут применяться в ситуациях, когда теоретические гарантии недоступны.

Определим набор индексов~$\mathcal {A}~$ для параметров логистической регрессии~$\textbf {w}~$. Тестируется гипотеза
\[
\label{eq:hb:1}
\begin{aligned}
	H_0: j \not\in\mathcal{A} \left(\text{w}_{j} = 0\right), \quad H_1: j \in \mathcal{A}^* \left(\text{w}_{j} \not= 0\right),
\end{aligned}
\]
где~$\text{w}_{j}$ является~$j$-м элементом вектора~$\textbf{w}$.
Установим параметр отступа~$ c_0~$ для задачи логистической регрессии:
\[
\label{eq:hb:2}
\begin{aligned}
	H_0: 1-c_0 = p_0, \quad H_1: 1-c_0 = p_1,
\end{aligned}
\]
где~$c_0$ оптимальное решение, когда исключен~$j$-й элемент вектора.
Используя статистику
\[
\label{eq:hb:3}
\begin{aligned}
	Z = \frac{\hat{p}-p_0}{\sqrt{p_0(1-p_0)}}\sqrt{m}, \quad \hat{p} = \frac{1}{m}\sum_{i=1}^{m}y_i.
\end{aligned}
\]
В случае истинности нулевой гипотезы~$H_0$ статистика~$Z$ асимптотически имеет распределение~$\mathcal{N}\left(0, 1\right)$. В случае истинности альтернативной гипотезы~$H_1$ статистика~$Z$  асимптотически имеет распределение~$ \mathcal{N}\left(p_1-p_0, \sqrt{\frac{p_1(1-p_1)}{p_0(1-p_0)}}\right)$.
      
Достаточный объем выборки задается выражением
\[
\label{eq:hb:4}
\begin{aligned}
	m^* = \frac{p_0(1-p_0)\left(Z_{1-\alpha/2} + Z_{1-\beta}\sqrt{\frac{p_1(1-p_1)}{p_0(1-p_0)}}\right)^2}{(p_1-p_0)^2},
\end{aligned}
\]
где~$Z_{1-\alpha/2}$ и~$Z_{1-\beta}$~--- квантили стандартного нормального распределения~$\mathcal{N}\left(0, 1\right)$.
    
Данный метод не рассматривается далее, поскольку его можно использовать только в задаче логистической регрессии.

Рассмотрим метод на основе кросс-валидации (англ. cross-validation). Определим критерий переобучения как
\[
\label{eq:hb:5}
\begin{aligned}
	RS(m) = \ln\frac{L(\mathfrak{D}_{\mathcal{L}(m)}, \hat{\textbf{w}})}{L(\mathfrak{D}_{\mathcal{T}(m)}, \hat{\textbf{w}})}, \quad \frac{|\mathcal{T}(m)|}{|\mathcal{L}(m)|} = \text{const} \leq 0.5.
\end{aligned}
\]
Справедливо следующее предельное соотношение:
\[
\label{eq:hb:6}
\begin{aligned}
	\lim_{m\to \infty}RS(m) = 0.
\end{aligned}
\]

Достаточный размер выборки~$m^*$ определяется согласно условию:
\[
\label{eq:hb:7}
\begin{aligned}
	m^*: \forall m \geq m^* \mathsf{E}_{\mathfrak{D}_{m}}RS(m) \leq \varepsilon,
\end{aligned}
\]
где~$\varepsilon$ некоторый параметр, который задается экспертно.

Этот метод предполагает, что длины доверительных интервалов квантиля не превышают некоторого фиксированного значения~$l$. Для некоторого размера выборки~$m$ вычисляются квантильные доверительные интервалы~$\left (a^m_1, b^m_1\right), \left(a^m_2, b^m_2 \right), ..., \left(a^m_n, b^m_n \right)$ с уровнем значимости~$\alpha$ с использованием начальной загрузки для каждого параметра модели. Достаточный размер выборки задается выражением:
\[
\label{eq:hb:8}
\begin{aligned}
	m^*: \forall m\geq m^* \max_i\left(b^m_i - a^m_i\right) < l.
\end{aligned}
\]
    
Данный метод является покоординатным, и следовательно для повышения точности прогноза требуется значительное увеличение размера выборки.

Эвристические методы, рассмотренные выше, основаны на наблюдении за поведением модели при изменении размера выборки, но не предоставляют строгих теоретических гарантий. В следующем разделе рассматриваются байесовские методы, которые позволяют формализовать задачу определения достаточного размера выборки в рамках вероятностного подхода.

\section{Байесовские методы определения достаточного размера выборки}

В настоящем разделе рассматриваются байесовские методы оценки размера выборки, основанные на ограничении некоторых характеристик модели. Для анализа эффективности определяется функция размера выборки. Увеличение этой функции интерпретируется как снижение эффективности модели. Размер выборки~$m^*$ выбирается таким, чтобы исследуемая функция принимала значения меньше некоторого порогового значения~$\varepsilon$.

Размер выборки~$m^*$ определяется условием:
\[
\label{eq:bs:1}
\begin{aligned}
	\forall m \geq m^*    \mathsf{E}_{\mathfrak{D}_m}\mathsf{D}\left[\hat{\textbf{w}}|\mathfrak{D}_m\right] \leq l.
\end{aligned}
\]
где~$l$ некоторый заданный экспертно параметр, который количественно определяет неопределенность оценки параметра.

Обозначим через~$A\left(\mathfrak{D}\right) \subset \mathbb{R}^n$ некоторый набор параметров модели~$\textbf{w}$:
\[
\label{eq:bs:2}
\begin{aligned}
	A\left(\mathfrak{D}\right) = \left\{\textbf{w}:||\textbf{w} - \hat{\textbf{w}}||\leq l\right\},
\end{aligned}
\]
где~$l$~--- некоторый фиксированный радиус шара.
Размер выборки~$m^*$ определяется критерием среднего покрытия:
\[
\label{eq:bs:3}
\begin{aligned}
	\forall m \geq m^*    \mathsf{E}_{\mathfrak{D}_m}\mathsf{P}\left\{\textbf{w} \in A\left(\mathfrak{D}_m\right)\right\} \geq 1-\alpha,
\end{aligned}
\]
где~$\alpha$ некоторый параметр заданный экспертно.

Определим функцию~$A\left(\mathfrak{D}\right)$:
\[
\label{eq:bs:4}
\begin{aligned}
	\mathsf{P}\left(A\left(\mathfrak{D}\right)\right) =  1- \alpha.
\end{aligned}
\]
Оценка критерия средней длины~$m^*$, заданная в \eqref{eq:bs:3}:
	
\[
\label{eq:bs:5}
\begin{aligned}
	\forall m \geq m^*    \mathsf{E}_{\mathfrak{D}_m}r_m\leq l,
\end{aligned}
\]
где~$r_m$ является радиусом шара~$A\left(\mathfrak{D}_{m}\right)$.

Следующие методы максимизируют ожидание некоторой функции полезности~$u\left(\mathfrak{D}, \textbf{w}\right)$ по размеру выборки:
\[
\label{eq:bs:6}
\begin{aligned}
	m^* = \arg\max_{m} \mathsf{E}_{\mathfrak{D}_m}\int_{\textbf{w}}u\left(\mathfrak{D}_m, \textbf{w}\right)p(\textbf{w}|\mathfrak{D}_m)d\textbf{w},
\end{aligned}
\]
где функция полезности~$u\left(\mathfrak{D}, \textbf{w}\right)$ задается в виде:

\[
\label{eq:bs:7}
\begin{aligned}
	u\left(\mathfrak{D}_m, \textbf{w}\right) = l\left(\mathfrak{D}_m, \textbf{w}\right) - cm,
\end{aligned}
\]
где~$c$~--- коэффициент штрафа для каждого элемента в наборе выборки.
	 
Назовем индексы~$\mathcal{B}_1,\mathcal{B}_2 \subset \{1,...,m\}$ по соседству, если
\[
\label{eq:bs:8}
\begin{aligned}
	\left|\mathcal{B}_1 \Delta \mathcal{B}_2\right| = 1.
\end{aligned}
\]
Таким образом,~$\mathcal{B}_2~$ можно преобразовать в~$\mathcal{B}_1$ путем удаления, замены или добавления одного элемента. В~\cite{motrenko2014} показано, что если размер набора выборок~$\mathfrak {D}_{\mathcal {B}_1}$ достаточно велик, то параметры модели~$\hat{\textbf {w}}_1$, оптимизированные с помощью~$\mathfrak{D}_{\mathcal{B}_1}$, должны находиться в окрестности параметров модели~$\hat{\textbf{w}}_2~$, которые оптимизированы с помощью~$\mathfrak{D}_{\mathcal {B}_2}$.
	 
Используя дивергенцию Кульбака-Лейблера в качестве функции близости между распределениями параметров модели, оптимизированных с помощью~$\mathfrak{D}_{\mathcal{B}_1}$ и~$\mathfrak{D}_{\mathcal{B}_2}$:
\[
\label{eq:bs:9}
\begin{aligned}
	D_\text{KL}\left(p_1, p_2\right) = \int_{\textbf{w}\in\mathbb{W}}p_1(\textbf{w})\log\frac{p_1(\textbf{w})}{p_2(\textbf{w})}d\textbf{w},
\end{aligned}
\]
где~$p_1$ и~$p_2$~--- апостериорные распределения вектора параметров~$\textbf{w}$, рассчитанные на подвыборках~$\mathfrak{D}_{\mathcal{B}_1}$ и~$\mathfrak{D}_{\mathcal{B}_2}$ соответственно. Также предполагается, что~$\mathfrak{D}_{\mathcal{B}_1}$ и~$\mathfrak{D}_{\mathcal{B}_2}$ находятся по соседству.
Достаточный размер выборки~$m^*$ оценивается:
\[
\label{eq:bs:10}
\begin{aligned}
	\forall \mathfrak{D}_{\mathcal{B}_1}: \left|\mathfrak{D}_{\mathcal{B}_1}\right| \geq m^*    \mathsf{E}_{\mathfrak{D}_{\mathcal{B}_2}}D_{KL}\left(p_1, p_2\right) \leq \varepsilon.
\end{aligned}
\]

Рассмотренные выше классические методы определения достаточного размера выборки имеют существенные ограничения: статистические методы требуют знания дисперсии оценок параметров, байесовские методы~--- вычислительно сложны для моделей с большим числом параметров, а эвристические методы не предоставляют строгих теоретических гарантий. В следующих разделах предлагаются новые методы, основанные на анализе стабильности функции правдоподобия и близости апостериорных распределений, которые преодолевают указанные ограничения.

\section{Метод определения достаточного размера выборки на основе сэмплирования эмпирической функции ошибки}

В настоящем разделе рассматривается метод определения достаточного размера выборки, основанный на анализе стабильности функции правдоподобия при изменении объема данных. Предполагается, что выполняется условие~$m^* \leqslant m$, где~$m$~--- размер доступной выборки~$D$, а~$m^*$~--- искомый достаточный размер. Таким образом, требуется определить минимальный объем выборки, который следует считать достаточным для обучения модели, при условии наличия достаточного количества объектов в самой выборке~$D$.

Для определения достаточности используется функция правдоподобия. Когда доступно достаточное количество объектов, естественно ожидать, что полученная оценка параметров не будет существенно изменяться от одной реализации выборки к другой~\cite{joseph1997,joseph1995}. Аналогичное утверждение справедливо и для функции правдоподобия. Таким образом, формализуем критерии, позволяющие определить достаточный объем выборки.

Критерий определяется в определении~\ref{sufficient-variance}.

\begin{definition}[D-достаточный размер выборки]\label{sufficient-variance}
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ назовем D-достаточным, если для всех~$k\geqslant m^*$ выполняется условие:
    \[
        D(k) = \mathbb{D}_{\hat{\mathbf{w}}_{k}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \leqslant \varepsilon.
    \]
\end{definition}

Определение~\ref{sufficient-variance} формализует идею о том, что при достаточном размере выборки дисперсия функции правдоподобия по различным реализациям подвыборок должна быть мала, что указывает на стабильность оценки параметров.

С другой стороны, при наличии достаточного количества объектов естественно ожидать, что при добавлении еще одного объекта к рассмотрению результирующая оценка параметра изменится незначительно; на основе данного свойства получаем определение~\ref{sufficient-difference}.

\begin{definition}[M-достаточный размер выборки]\label{sufficient-difference}
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ назовем M-достаточным, если для всех~$k\geqslant m^*$ выполняется условие:
    \[
        M(k) = \left| \mathbb{E}_{\hat{\mathbf{w}}_{k+1}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E}_{\hat{\mathbf{w}}_{k}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \right| \leqslant \varepsilon.
    \]
\end{definition}

Определение~\ref{sufficient-difference} формализует условие, при котором добавление нового объекта к выборке не приводит к существенному изменению математического ожидания функции правдоподобия, что указывает на достижение достаточного объема данных для стабильной оценки параметров модели.

В приведенных выше определениях вместо функции правдоподобия~$L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$ рассматривается ее логарифм~$l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$, что упрощает математический анализ за счет перехода от произведения к сумме.

Предположим, что~$\mathbb{W} = \mathbb{R}^n$, информационная матрица Фишера задана матрицей:
\[
    \left[\mathcal{I}(\mathbf{w})\right]_{ij} = - \mathbb{E}\left[ \frac{\partial^2 \log p(\mathbf{y} | \mathbf{x}, \mathbf{w})}{\partial w_i \partial w_j} \right],
\]
Известным результатом является асимптотическая нормальность оценки максимального правдоподобия:
\[
    \sqrt{k}\left(\hat{\mathbf{w}}_k -\mathbf{w}\right)\xrightarrow{d}\mathcal{N}\left(0, \mathcal{I}^{-1}(\mathbf{w})\right),
\]
где~$\xrightarrow{d}$ обозначает сходимость по распределению. Следует отметить, что сходимость по распределению, вообще говоря, не влечет сходимости моментов случайного вектора.
Однако если предположить сходимость моментов, то в некоторых моделях можно доказать корректность предложенного определения M-достаточного размера выборки.

Для удобства обозначим параметры распределения~$\hat{\mathbf{w}}_k$ следующим образом: математическое ожидание~$\mathbb{E}\hat{\mathbf{w}}_k=\mathbf{m}_k$ и матрица ковариаций~$\mathbb{D} \hat{\mathbf{w}}_k = \mathbf{\Sigma}_k$.
Тогда справедлива теорема~\ref{chapter:samplesize:theorem-kiselev-likelihood-bootstraping}, которая доказывает сходимость параметров.

\begin{theorem}[Корректность M-достаточного размера выборки]\label{chapter:samplesize:theorem-kiselev-likelihood-bootstraping}
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и~$\|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_{F}\to 0$ при~$k\to \infty$. 
    Тогда в модели линейной регрессии определение M-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$M(k)\leqslant\varepsilon$.
\end{theorem}
\begin{proof}
Рассмотрим определение M-достаточного размера выборки в терминах логарифма функции правдоподобия. В модели линейной регрессии
\begin{align}
    L\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) &= p(\mathbf{y} | \mathbf{X}, \hat{\mathbf{w}}_k) = \prod_{i=1}^{m} p(y_i | \mathbf{x}_i, \hat{\mathbf{w}}_k) = \prod_{i=1}^{m} \mathcal{N}\left( y_i | \hat{\mathbf{w}}_k^{\top} \mathbf{x}_i, \sigma^2 \right) =\\
    &= \left(2\pi\sigma^2 \right)^{-m/2} \exp\left(-\frac{1}{2\sigma^2}\|\mathbf{y} -\mathbf{X} \hat{\mathbf{w}}_k\|_2^2 \right).
\end{align}
Возьмем логарифм:
\[
    l\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) = \log p(\mathbf{y} | \mathbf{X}, \hat{\mathbf{w}}_k) = -\frac{m}{2}\log\left( 2\pi\sigma^2 \right) - \frac{1}{2\sigma^2} \| \mathbf{y} - \mathbf{X} \hat{\mathbf{w}}_k \|_2^2.
\]
Возьмем математическое ожидание по~$\mathfrak{D}_k$, учитывая что~$\mathbb{E}_{\mathfrak{D}_k}\hat{\mathbf{w}}_k=\mathbf{m}_k$ и~$\text{cov}(\hat{\mathbf{w}}_k) = \mathbf{\Sigma}_k$:
\[
    \mathbb{E}_{\mathfrak{D}_k} l\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) = -\frac{m}{2}\log\left( 2\pi\sigma^2 \right) - \frac{1}{2\sigma^2} \Big( \| \mathbf{y} - \mathbf{X} \mathbf{m}_k \|_2^2 + \text{tr}\left( \mathbf{X}^{\top}\mathbf{X} \mathbf{\Sigma}_k \right) \Big).
\]
Запишем выражение для разности математических ожиданий:
\begin{align}
    &\mathbb{E}_{\mathfrak{D}_{k+1}} l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E}_{\mathfrak{D}_k} l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) = \\
    &\quad= \frac{1}{2\sigma^2} \Big( \| \mathbf{y} - \mathbf{X} \mathbf{m}_k \|_2^2 - \| \mathbf{y} - \mathbf{X} \mathbf{m}_{k+1} \|_2^2 \Big) + \frac{1}{2\sigma^2} \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \Big) \right) = \\
    &\quad= \frac{1}{2\sigma^2} \Big( 2 \mathbf{y}^{\top} \mathbf{X} (\mathbf{m}_{k+1} - \mathbf{m}_k) + (\mathbf{m}_k - \mathbf{m}_{k+1})^{\top} \mathbf{X}^{\top}\mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \Big) + \\
    &\qquad+ \frac{1}{2\sigma^2} \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \right) \Big).
\end{align}
Значение функции~$M(k)$ представляет собой модуль от приведенного выше выражения. Применим неравенство треугольника для модуля, а затем оценим каждое слагаемое.\\
Оценим первое слагаемое, используя неравенство Коши-Буняковского:
\[
    \big| \mathbf{y}^{\top}\mathbf{X}(\mathbf{m}_{k+1}-\mathbf{m}_k)\big| \leqslant \| \mathbf{X}^{\top}\mathbf{y} \|_2 \|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2.
\]
Второе слагаемое оцениваем с помощью неравенства Коши-Буняковского, свойства согласованности спектральной нормы матрицы, а также ограниченности последовательности векторов~$\mathbf{m}_k$, что следует из приведенного условия сходимости:
\begin{align}
    \big| (\mathbf{m}_k - \mathbf{m}_{k+1})^{\top} \mathbf{X}^{\top}\mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \big| &\leqslant \| \mathbf{X} (\mathbf{m}_k - \mathbf{m}_{k+1}) \|_2 \| \mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \|_2 \leqslant \\
    &\leqslant \| \mathbf{X} \|_2^2 \| \mathbf{m}_k - \mathbf{m}_{k+1} \|_2 \| \mathbf{m}_k + \mathbf{m}_{k+1} \|_2 \leqslant \\
    &\leqslant C \| \mathbf{X} \|_2^2 \| \mathbf{m}_k - \mathbf{m}_{k+1} \|_2.
\end{align}
Последнее слагаемое оцениваем, используя неравенство Гельдера для нормы Фробениуса:
\[
    \Big| \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \right) \Big) \Big| \leqslant \| \mathbf{X}^{\top}\mathbf{X} \|_F \| \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \|_F.
\]
Наконец, поскольку~$\|\mathbf{m}_k - \mathbf{m}_{k+1} \|_2\to 0$ и~$\|\mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1}\|_{F}\to 0$ при~$k\to\infty$, то~$M(k)\to 0$ при~$k\to \infty$, что и доказывает теорему.
\end{proof}

Теорема~\ref{chapter:samplesize:theorem-kiselev-likelihood-bootstraping} устанавливает корректность определения M-достаточного размера выборки для модели линейной регрессии при выполнении условий сходимости моментов оценок параметров. Указанные условия являются естественными для асимптотически нормальных оценок максимального правдоподобия и выполняются при стандартных предположениях о регулярности модели.

\begin{corollary}[Корректность M-достаточного размера выборки при сходимости к истинным параметрам]\label{chapter:samplesize:corollary-kiselev-likelihood-bootstraping}
    Пусть~$\|\mathbf{m}_k - \mathbf{w}\|_2\to 0$ и~$\|\mathbf{\Sigma}_k - \left[k\mathcal{I}(\mathbf{w})\right]^{-1}\|_{F}\to 0$ при~$k \to \infty$. 
    
    Тогда в модели линейной регрессии определение M-достаточного размера выборки корректно.
\end{corollary}

По условию, задана только одна выборка, а следовательно, в эксперименте невозможно вычислить математическое ожидание и дисперсию, указанные в определениях~\ref{sufficient-variance} и~\ref{sufficient-difference}.
Поэтому для их оценки используется метод бутстрэпирования (англ. bootstrap): из заданной выборки~$\mathfrak{D}_m$ генерируется некоторое число~$B$ подвыборок размера~$k$ с возвращением.
Для каждой подвыборки получается оценка параметров~$\hat{\mathbf{w}}_{k}$ и вычисляется значение~$L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$.
Для оценки математического ожидания и дисперсии используются выборочное среднее и несмещенная выборочная дисперсия соответственно. Количество подвыборок~$B$ выбирается достаточно большим (обычно~$B \geqslant 1000$) для обеспечения точности оценок.

Предложенные выше определения также могут быть применены в тех задачах, где минимизируется произвольная функция потерь, а не максимизируется функция правдоподобия. В этом случае вместо функции правдоподобия~$L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$ используется функция потерь~$\mathcal{L}(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$, а критерии достаточности формулируются аналогичным образом.
Строгое теоретическое обоснование данного обобщения отсутствует, однако эмпирические результаты демонстрируют применимость таких методов на практике.

Метод, рассмотренный выше, основан на анализе стабильности функции правдоподобия при изменении объема данных. Альтернативный подход заключается в анализе близости апостериорных распределений параметров модели на близких подвыборках, что составляет содержание следующего раздела.

\section{Метод определения достаточного размера выборки на основе близости апостериорных распределений}

В настоящем разделе рассматривается метод определения достаточного размера выборки, основанный на анализе близости апостериорных распределений параметров модели. В работе~\cite{motrenko2014} предлагается использовать расхождение Кульбака-Лейблера для оценки достаточного размера выборки в задаче бинарной классификации.

Идея метода основана на том, что если две подвыборки отличаются друг от друга одним объектом, то полученные по ним апостериорные распределения должны быть близки. Эта близость определяется расхождением Кульбака-Лейблера.

\begin{figure}[h!t]\center
    \includesvg[width=0.7\textwidth]{figures/chapter-2/posterior_ru}
    \caption{Визуализация сдвига апостериорных распределений параметров модели при последовательном добавлении объектов в выборку. Иллюстрация демонстрирует концепцию близости распределений, используемую в методах KL- и S-достаточности для определения достаточного размера выборки.}
    \label{fig-chapter-2-posterior-ru}
\end{figure}

В рамках данного раздела предлагается использовать не только расхождение Кульбака-Лейблера, но и функцию схожести s-score из работы~\cite{aduenko2017}, для этого рассмотрим две подвыборки~$\mathfrak{D}^1\subseteq\mathfrak{D}_m$ и~$\mathfrak{D}^2\subseteq\mathfrak{D}_m$.
Пусть~$\mathcal{I}_1 \subseteq \mathcal{I} = \{1, \ldots, m\}$ и~$\mathcal{I}_2 \subseteq \mathcal{I} =\{1, \ldots,m\}$~--- соответствующие им подмножества индексов.

\begin{definition}[Близкие подвыборки]
    Подвыборки~$\mathfrak{D}^1$ и~$\mathfrak{D}^2$ назовем близкими, если~$\mathcal{I}_2$ может быть получено из~$\mathcal{I}_1$ путем удаления, замены или добавления одного элемента. Формально это условие записывается как
    \[
        \left| \mathcal{I}_1 \triangle \mathcal{I}_2 \right| = \left| \left( \mathcal{I}_1 \setminus \mathcal{I}_2 \right) \cup \left( \mathcal{I}_2 \setminus \mathcal{I}_1 \right) \right| = 1,
    \]
    где~$\triangle$ обозначает симметрическую разность множеств.
\end{definition}

Рассмотрим две близкие подвыборки~$\mathfrak{D}_k = (\mathbf{X}_k,\mathbf{y}_k)$ и~$\mathfrak{D}_{k+1} = (\mathbf{X}_{k+1}, \mathbf{y}_{k+1})$ размеров~$k$ и~$k+1$ соответственно, где выборка~$\mathfrak{D}_{k+1}$ получена путем добавления одного элемента к выборке~$\mathfrak{D}_k$.

Вычислим апостериорное распределение параметров модели по каждой из этих подвыборок:
\[
    p_j(\mathbf{w}) = p(\mathbf{w} | \mathfrak{D}_j) = \frac{p(\mathfrak{D}_j | \mathbf{w}) p(\mathbf{w})}{p(\mathfrak{D}_j)} \propto p(\mathfrak{D}_j | \mathbf{w}) p(\mathbf{w}), \quad j = k, k+1.
\]

\begin{definition}[KL-достаточный размер выборки]
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ называется KL-достаточным, если для всех~$k\geqslant m^*$
    \[
        KL(k) = D_{KL}(p_k \| p_{k+1}) = \int p_k(\mathbf{w}) \log{\frac{p_k(\mathbf{w})}{p_{k+1}(\mathbf{w})}} d\mathbf{w} \leqslant \varepsilon.
    \]
\end{definition}

Для пары нормальных распределений расхождение Кульбака-Лейблера имеет аналитическое выражение. Предположив, что апостериорное распределение является нормальным,~$p_k(\mathbf{w}) = \mathcal{N}\left(\mathbf{w}|\mathbf{m}_k, \mathbf{\Sigma}_k\right)$, где~$\mathbf{m}_k$~--- вектор математического ожидания, а~$\mathbf{\Sigma}_k$~--- ковариационная матрица, получаем формулировку теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity}

\begin{theorem}[Корректность KL-достаточного размера выборки]\label{chapter:samplesize:theorem-kiselev-posterior-similiarity}
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и~$\|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_{F}\to 0$ при~$k\to \infty$.
    
    Тогда в модели с нормальным апостериорным распределением параметров определение KL-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$KL(k)\leqslant\varepsilon$.
\end{theorem}
\begin{proof}
Рассмотрим выражение для расхождения Кульбака-Лейблера между двумя нормальными апостериорными распределениями~$p_k = \mathcal{N}(\mathbf{m}_k, \mathbf{\Sigma}_k)$ и~$p_{k+1} = \mathcal{N}(\mathbf{m}_{k+1}, \mathbf{\Sigma}_{k+1})$.
Для двух многомерных нормальных распределений данная метрика имеет аналитическое выражение:
\begin{align}
    &D_{\text{KL}}\left( p_k \| p_{k+1} \right) =\\
    &=\frac{1}{2} \left( \mathrm{tr}\left( \mathbf{\Sigma}_{k+1}^{-1} \mathbf{\Sigma}_k \right) + (\mathbf{m}_{k+1} - \mathbf{m}_k)^{\top} \mathbf{\Sigma}_{k+1}^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) - n + \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} \right).
\end{align}
Для анализа поведения каждого слагаемого при~$k \to \infty$ введем обозначение для разности ковариационных матриц:~$\mathbf{\Sigma}_{k+1} = \mathbf{\Sigma}_k + \Delta\mathbf{\Sigma}$, где по условию теоремы~$\|\Delta \mathbf{\Sigma}\|_F = \|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_F \to 0$. Первое слагаемое представляет собой след произведения матриц:
\begin{align}
    \mathrm{tr}\left( \mathbf{\Sigma}_{k+1}^{-1} \mathbf{\Sigma}_k \right) &= \mathrm{tr}\left(\left(\mathbf{\Sigma}_k + \Delta \mathbf{\Sigma} \right)^{-1} \mathbf{\Sigma}_k \right).
\end{align}
Используя разложение в ряд для обратной матрицы при малых~$\Delta\mathbf{\Sigma}$, получаем:
\[
    (\mathbf{\Sigma}_k + \Delta \mathbf{\Sigma})^{-1} = \mathbf{\Sigma}_k^{-1} - \mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma} \mathbf{\Sigma}_k^{-1} + O(\|\Delta \mathbf{\Sigma}\|_F^2).
\]
Тогда:
\[
    (\mathbf{\Sigma}_k + \Delta \mathbf{\Sigma})^{-1} \mathbf{\Sigma}_k = \mathbf{I}_n - \mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma} + O(\|\Delta \mathbf{\Sigma}\|_F^2).
\]
Взяв след от этого выражения и учитывая, что~$\mathrm{tr}(\mathbf{I}_n) = n$, а~$\|\mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}\|_F \to 0$ при~$\|\Delta \mathbf{\Sigma}\|_F \to 0$, получаем:
\[
    \mathrm{tr}\left( \mathbf{\Sigma}_{k+1}^{-1} \mathbf{\Sigma}_k \right) \to n \quad \text{при} \quad \| \Delta \mathbf{\Sigma} \|_F \to 0.
\]
Второе слагаемое представляет собой квадратичную форму:
\begin{align}
    (\mathbf{m}_{k+1} - \mathbf{m}_k)^{\top} \mathbf{\Sigma}_{k+1}^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k).
\end{align}
По условию теоремы~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$. Квадратичная форма оценивается сверху следующим образом:
\[
    \left| (\mathbf{m}_{k+1} - \mathbf{m}_k)^{\top} \mathbf{\Sigma}_{k+1}^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) \right| \leqslant \| \mathbf{m}_{k+1} -\mathbf{m}_k\|_2^2 \cdot \|\mathbf{\Sigma}_{k+1}^{-1} \|_2.
\]
Поскольку ковариационная матрица~$\mathbf{\Sigma}_{k+1}$ является положительно определенной и сходится к некоторой предельной матрице, ее спектральная норма~$\|\mathbf{\Sigma}_{k+1}^{-1} \|_2$ ограничена. Следовательно, при~$\| \mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ данное слагаемое стремится к нулю.
Третье и четвертое слагаемые составляют:
\[
    -n + \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} = \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} - n.
\]
Преобразуем отношение определителей:
\[
    \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} = \frac{\det (\mathbf{\Sigma}_k + \Delta \mathbf{\Sigma})}{\det \mathbf{\Sigma}_{k}} = \det (\mathbf{I}_n + \mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}).
\]
Для малых~$\Delta \mathbf{\Sigma}$ используем приближение:
\[
    \det (\mathbf{I}_n + \mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}) = 1 + \mathrm{tr}(\mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}) + O(\|\Delta \mathbf{\Sigma}\|_F^2).
\]
Тогда:
\[
    \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} = \log \det (\mathbf{I}_n + \mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}) = \mathrm{tr}(\mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}) + O(\|\Delta \mathbf{\Sigma}\|_F^2).
\]
Поскольку~$\|\Delta \mathbf{\Sigma}\|_F \to 0$, то~$\mathrm{tr}(\mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}) \to 0$, и следовательно:
\[
    \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} \to 0.
\]

Таким образом, все четыре слагаемых в выражении для~$D_{\text{KL}}$ сходятся к своим пределам при~$k \to \infty$: первое слагаемое стремится к~$n$, второе~--- к 0, третье равно~$-n$, четвертое~--- к 0.
Сумма этих пределов равна~$n + 0 - n + 0 = 0$, что доказывает, что~$D_{\text{KL}}(p_k \| p_{k+1}) \to 0$ при~$k \to \infty$.
Следовательно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k \geqslant m^*$ выполняется~$KL(k) \leqslant \varepsilon$, что и требовалось доказать.
\end{proof}

Теорема~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity} устанавливает, что расхождение Кульбака-Лейблера между двумя нормальными апостериорными распределениями стремится к нулю по мере сходимости их векторов математических ожиданий и ковариационных матриц. Это позволяет использовать KL-дивергенцию в качестве критерия достаточности размера выборки, анализируя аналитические выражения для моментов апостериорных распределений.

Рассмотрим функцию схожести s-score из работы~\cite{aduenko2017} в качестве меры близости распределений по аналогии, как это было с KL-дивергенцией:
\[
    \text{s-score}(g_1, g_2) = \frac{\int_{\mathbf{w}} g_1(\mathbf{w}) g_2(\mathbf{w}) d\mathbf{w}}{\max_{\mathbf{b}} \int_{\mathbf{w}} g_1(\mathbf{w} - \mathbf{b}) g_2(\mathbf{w}) d\mathbf{w}}.
\]
\begin{definition}[S-достаточный размер выборки]
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ называется S-достаточным, если для всех~$k\geqslant m^*$
    \[
        S(k) = \text{s-score}(p_k, p_{k+1}) \geqslant 1-\varepsilon.
    \]
\end{definition}
Как и в случае с KL-достаточным размером выборки, в модели с нормальным апостериорным распределением можно записать выражение для используемого критерия, который записан в виде теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-s-score}

\begin{theorem}[Корректность S-достаточного размера выборки]\label{chapter:samplesize:theorem-kiselev-posterior-similiarity-s-score}
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2\to 0$ при~$k \to \infty$.
    
    Тогда в модели с нормальным апостериорным распределением параметров определение S-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$S(k)\geqslant 1-\varepsilon$.
\end{theorem}
\begin{proof}
Используем выражение для s-score пары нормальных апостериорных распределений из работы~\cite{aduenko2017}:
\[
    \text{s-score}(p_k, p_{k+1}) = \exp{\left( -\frac{1}{2} (\mathbf{m}_{k+1} - \mathbf{m}_k)^{\top} \left( \mathbf{\Sigma}_k + \mathbf{\Sigma}_{k+1} \right)^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) \right)}.
\]
Оценим квадратичную форму в показателе экспоненты:
\[
    \left| (\mathbf{m}_{k+1} - \mathbf{m}_k)^{\top} \left( \mathbf{\Sigma}_k + \mathbf{\Sigma}_{k+1} \right)^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) \right| \leqslant \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2^2 \| \left( \mathbf{\Sigma}_k + \mathbf{\Sigma}_{k+1} \right)^{-1} \|_2.
\]
Поскольку ковариационные матрицы~$\mathbf{\Sigma}_k$ и~$\mathbf{\Sigma}_{k+1}$ являются положительно определенными и сходятся к некоторой предельной матрице, спектральная норма~$\| \left( \mathbf{\Sigma}_k + \mathbf{\Sigma}_{k+1} \right)^{-1} \|_2$ ограничена. При условии~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2\to 0$ значение квадратичной формы в показателе экспоненты стремится к нулю.
Следовательно,~$\text{s-score}(p_k, p_{k+1}) \to 1$ при~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2\to 0$, что и требовалось доказать.
\end{proof}

Теорема~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-s-score} устанавливает корректность определения S-достаточного размера выборки при более слабых условиях, чем теорема~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity}. В отличие от KL-дивергенции, для сходимости s-score к единице требуется только сходимость математических ожиданий апостериорных распределений, что делает данный критерий менее консервативным и более применимым на практике.

Пусть в модели линейной регрессии задано нормальное априорное распределение параметров. В силу свойства сопряженности априорного распределения и правдоподобия, апостериорное распределение также будет нормальным.
Таким образом, приходим к одному из простейших примеров модели, для которой справедливы приведенные выше теоремы.
Фактически, для линейной регрессии могут быть сформулированы более простые утверждения.

\begin{theorem}[Сходимость апостериорных распределений в линейной регрессии]\label{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression}
    Пусть множества значений признаков и целевой переменной ограничены, то есть существует константа~$M\in \mathbb{R}$ такая, что~$\|\mathbf{x}\|_2\leqslant M$ и~$|y|\leqslant M$ для всех объектов выборки.
    Если~$\lambda_{\min}\left(\mathbf{X}^{\top}_k \mathbf{X}_k \right) = \omega(\sqrt{k})$ при~$k\to \infty$, то в модели линейной регрессии с нормальным априорным распределением параметров~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2\to 0$ и~$\|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_{F}\to 0$ при~$k\to \infty$.
\end{theorem}
\begin{proof}
Рассмотрим линейную регрессионную модель с нормальным априорным распределением параметров:~$p(\mathbf{w})=\mathcal{N}\left(\mathbf{w}|\mathbf{0}, \alpha^{-1}\mathbf{I}\right)$.
Данное априорное распределение является сопряженным для нормального правдоподобия, что существенно упрощает анализ.
Нормальное правдоподобие задается в виде:
\[
    p(\mathbf{y} | \mathbf{X}, \mathbf{w}) = \mathcal{N}\left(\mathbf{y} |\mathbf{X}\mathbf{w}, \sigma^2\mathbf{I}\right) =\left( 2\pi\sigma^2\right)^{-m/2} \exp\left( -\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|_2^2\right).
\]

Благодаря свойству сопряженности нормального априорного распределения и нормального правдоподобия, апостериорное распределение также является нормальным:
\[
    p(\mathbf{w} | \mathbf{X}, \mathbf{y}) = \mathcal{N}\left(\mathbf{w} | \mathbf{m}, \mathbf{\Sigma} \right),
\]
где параметры распределения имеют аналитическое выражение:
\begin{align}
    \mathbf{\Sigma} &= \left( \alpha \mathbf{I} + \frac{1}{\sigma^2} \mathbf{X}^{\top} \mathbf{X} \right)^{-1}, \\
    \mathbf{m} &= \frac{1}{\sigma^2} \mathbf{\Sigma} \mathbf{X}^{\top} \mathbf{y} = \left( \mathbf{X}^{\top} \mathbf{X} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}^{\top} \mathbf{y}.
\end{align}

Перейдем к анализу сходимости ковариационных матриц.
Рассмотрим подвыборки размера~$k$ и~$k+1$, полученные из исходных данных. Нас интересует поведение разности~$\|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_2$ при~$k \to \infty$.
Введем обозначение~$\mathbf{A}_k = \frac{1}{\sigma^2}\mathbf{X}_k^{\top}\mathbf{X}_k$ для нормированной матрицы ковариации признаков. Тогда разность обратных матриц можно преобразовать, используя матричное тождество:
\begin{align}
    \| \mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k \|_2 &= \left\| \left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)^{-1} - \left( \alpha \mathbf{I} + \mathbf{A}_k \right)^{-1} \right\|_2.
\end{align}
Применяя матричное тождество для разности обратных матриц, получаем:
\begin{align}
    \left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)^{-1} - \left( \alpha \mathbf{I} + \mathbf{A}_k \right)^{-1} = 
    \left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)^{-1} \left( \mathbf{A}_k - \mathbf{A}_{k+1} \right) \left( \alpha \mathbf{I} + \mathbf{A}_k \right)^{-1}.
\end{align}
Используя субмультипликативное свойство спектральной нормы, оцениваем:
\begin{align}
    \| \mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k \|_2 &\leqslant \left\| \left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)^{-1} \right\|_2 \left\| \left( \alpha \mathbf{I} + \mathbf{A}_k \right)^{-1} \right\|_2 \left\| \mathbf{A}_{k+1} - \mathbf{A}_k \right\|_2.
\end{align}
Проанализируем каждый из множителей в полученной оценке.
Спектральная норма обратной матрицы выражается через минимальное собственное значение исходной матрицы:
\[
    \left\| \left( \alpha \mathbf{I} + \mathbf{A} \right)^{-1} \right\|_2 = \frac{1}{\lambda_{\min}(\alpha \mathbf{I} + \mathbf{A})}.
\]
Поскольку~$\alpha > 0$ и матрица~$\mathbf{A}$ положительно полуопределена, имеем~$\lambda_{\min}(\alpha \mathbf{I} + \mathbf{A}) \geq \alpha + \lambda_{\min}(\mathbf{A})$.
Однако для получения точной асимптотики используем более слабую оценку:
\[
\left\| \left( \alpha \mathbf{I} + \mathbf{A} \right)^{-1} \right\|_2 \leq \frac{1}{\lambda_{\min}(\mathbf{A})}.
\]
Объединяя полученные оценки, получаем цепочку неравенств:
\begin{align}
    \| \mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k \|_2 &\leqslant \frac{1}{\lambda_{\min}\left( \mathbf{A}_{k+1} \right)} \frac{1}{\lambda_{\min}\left( \mathbf{A}_k \right)} \left\| \mathbf{A}_{k+1} - \mathbf{A}_k \right\|_2 = \\
    &= \sigma^2  \frac{1}{\lambda_{\min}\left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} \right)} \frac{1}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)} \left\| \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} - \mathbf{X}_k^{\top} \mathbf{X}_k \right\|_2.
\end{align}
Поскольку выборка~$\mathfrak{D}_{k+1}$ получается из~$\mathfrak{D}_k$ добавлением одного наблюдения, имеем:
\begin{align}
    \left\| \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} - \mathbf{X}_k^{\top} \mathbf{X}_k \right\|_2 &= \left\| \sum\limits_{i=1}^{k+1} \mathbf{x}_i \mathbf{x}_i^{\top} - \sum\limits_{i=1}^{k} \mathbf{x}_i \mathbf{x}_i^{\top} \right\|_2 = \left\| \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right\|_2.
\end{align}
Матрица~$\mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top}$ является матрицей ранга 1, и ее спектральная норма равна квадрату евклидовой нормы вектора~$\mathbf{x}_{k+1}$:
\[
    \left\| \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right\|_2 = \lambda_{\max}\left( \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right) = \| \mathbf{x}_{k+1}\|_2^2.
\]
Из условия ограниченности признаков следует~$\| \mathbf{x}_{k+1}\|_2^2 \leqslant M^2$. Таким образом:
\[
    \left\| \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} - \mathbf{X}_k^{\top} \mathbf{X}_k \right\|_2 \leqslant M^2.
\]

Теперь рассмотрим условие на минимальное собственное значение.
Из предположения~$\lambda_{\min}\left(\mathbf{X}_k^{\top}\mathbf{X}_k \right) = \omega(\sqrt{k})$ следует, что:
\[
    \frac{1}{\lambda_{\min}\left(\mathbf{X}_k^{\top}\mathbf{X}_k \right)} = o\left(\frac{1}{\sqrt{k}}\right).
\]
Комбинируя полученные оценки, приходим к:
\[
    \|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_2 \leqslant \sigma^2 M^2 \cdot o\left(\frac{1}{\sqrt{k}}\right) \cdot o\left(\frac{1}{\sqrt{k}}\right) = o\left(\frac{1}{k}\right).
\]
Для перехода к норме Фробениуса воспользуемся неравенством~$\| \mathbf{A} \|_F \leqslant \sqrt{n} \| \mathbf{A} \|_2$, где~$n$~--- размерность пространства параметров:
\[
    \|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k \|_F \leqslant \sqrt{n} \|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_2 = \sqrt{n} \cdot o\left(\frac{1}{k}\right) = o\left(\frac{1}{k}\right).
\]
Теперь перейдем к анализу сходимости математических ожиданий.
Требуется оценить:
\[
    \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 = \left\| \left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_{k+1}^{\top} \mathbf{y}_{k+1} - \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2.
\]
Представим расширенную матрицу признаков и вектор ответов через предыдущие значения:
\begin{align}
    \mathbf{X}_{k+1} &= \begin{bmatrix} \mathbf{X}_k \\ \mathbf{x}_{k+1}^{\top} \end{bmatrix}, &
    \mathbf{y}_{k+1} &= \begin{bmatrix} \mathbf{y}_k \\ y_{k+1} \end{bmatrix}.
\end{align}
Тогда матричные произведения принимают вид:
\begin{align}
    \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} &= \mathbf{X}_k^{\top} \mathbf{X}_k + \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top}, \\
    \mathbf{X}_{k+1}^{\top} \mathbf{y}_{k+1} &= \mathbf{X}_k^{\top} \mathbf{y}_k + \mathbf{x}_{k+1} y_{k+1}.
\end{align}
Подставляя эти выражения, получаем:
\begin{align}
    \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 &= \Bigg\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} + \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} \left( \mathbf{X}_k^{\top} \mathbf{y}_k + \mathbf{x}_{k+1} y_{k+1} \right) \\
    &\quad - \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_k^{\top} \mathbf{y}_k \Bigg\|_2.
\end{align}
Для упрощения первого слагаемого применим лемму о матричном обращении:
\begin{align}
    &\left(\mathbf{X}_k^{\top} \mathbf{X}_k + \alpha\sigma^2 \mathbf{I} + \mathbf{x}_{k+1}\mathbf{x}_{k+1}^{\top}\right)^{-1} = \\
    &\quad = \left(\mathbf{X}_k^{\top} \mathbf{X}_k + \alpha\sigma^2 \mathbf{I}\right)^{-1} - \frac{\left(\mathbf{X}_k^{\top} \mathbf{X}_k + \alpha\sigma^2 \mathbf{I}\right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \left(\mathbf{X}_k^{\top} \mathbf{X}_k + \alpha\sigma^2 \mathbf{I}\right)^{-1}}{1 + \mathbf{x}_{k+1}^{\top} \left(\mathbf{X}_k^{\top} \mathbf{X}_k + \alpha\sigma^2 \mathbf{I}\right)^{-1} \mathbf{x}_{k+1}}.
\end{align}
После алгебраических преобразований получаем:
\begin{align}
    &\| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 = \\
    &\quad = \Bigg\| \left[ \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} - \mathbf{I} \right] \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_k^{\top} \mathbf{y}_k \\
    &\qquad + \left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} y_{k+1} \Bigg\|_2.
\end{align}
Применяя неравенство треугольника и свойства норм, оцениваем каждый член отдельно:
\begin{align}
    &\| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 \leqslant \\
    &\leqslant \left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} - \mathbf{I} \right\|_2 \left\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2 \\
    &\quad + \left\| \left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{x}_{k+1} y_{k+1} \right\|_2.
\end{align}

Проанализируем первый множитель в первом слагаемом.
Используя матричное тождество, получаем:
\begin{align}
    &\left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} - \mathbf{I} \right\|_2 = \\
    &\quad = \left\| - \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right\|_2.
\end{align}
Применяя субмультипликативность нормы и учитывая, что спектральная норма произведения матриц не превышает произведения их норм, получаем оценку:
\begin{align}
    &\left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} - \mathbf{I} \right\|_2 \leqslant \\
    &\quad \leqslant \left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} \right\|_2 \left\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right\|_2.
\end{align}

Оценим каждый из этих множителей.
Для первого множителя используем тот факт, что матрица~$\left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top}$ имеет единичный ранг, и ее максимальное собственное значение равно:
\[
    \lambda_{\max}\left( \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right) = \mathbf{x}_{k+1}^{\top} \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1}.
\]
Тогда спектральная норма обратной матрицы оценивается как:
\[
    \left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} \right\|_2 \leqslant 1.
\]
Второй множитель оценивается через минимальное собственное значение:
\[
    \left\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \leqslant \frac{1}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)}.
\]
Третий множитель, как уже было установлено, равен~$\| \mathbf{x}_{k+1}\|_2^2 \leqslant M^2$.
Таким образом, получаем оценку для первого множителя:
\[
    \left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} - \mathbf{I} \right\|_2 \leqslant \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)}.
\]

Теперь оценим второй множитель первого слагаемого вместе с третьим множителем:
\[
    \left\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2 \leqslant \frac{\left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)}.
\]
Норма~$\left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2$ оценивается с использованием условия ограниченности:
\[
    \left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2 = \left\| \sum\limits_{i=1}^{k} \mathbf{x}_i y_i \right\|_2 \leqslant \sum\limits_{i=1}^{k} \left\| \mathbf{x}_i y_i \right\|_2 \leqslant k M^2.
\]
Следовательно:
\[
    \left\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2 \leqslant \frac{k M^2}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)}.
\]
Теперь рассмотрим второе слагаемое:
\[
    \left\| \left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{x}_{k+1} y_{k+1} \right\|_2 \leqslant \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} \right)}.
\]

Комбинируя все полученные оценки, приходим к итоговой оценке:
\begin{align}
    \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 &\leqslant \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)} \cdot \frac{k M^2}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)} + \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} \right)} \\
    &= \frac{k M^4}{\lambda_{\min}^2\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)} + \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} \right)}.
\end{align}

Из условия~$\lambda_{\min}\left(\mathbf{X}_k^{\top}\mathbf{X}_k \right) = \omega(\sqrt{k})$ следует:
\begin{align}
    \frac{1}{\lambda_{\min}\left(\mathbf{X}_k^{\top}\mathbf{X}_k \right)} &= o\left(\frac{1}{\sqrt{k}}\right), \\
    \frac{1}{\lambda_{\min}^2\left(\mathbf{X}_k^{\top}\mathbf{X}_k \right)} &= o\left(\frac{1}{k}\right).
\end{align}
Поэтому первое слагаемое оценивается как~$k \cdot o\left(\frac{1}{k}\right) = o(1)$, а второе слагаемое как~$o\left(\frac{1}{\sqrt{k}}\right) = o(1)$. Таким образом:
\[
    \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 = o(1) \quad \text{при} \quad k \to \infty.
\]

Это завершает доказательство сходимости как ковариационных матриц, так и математических ожиданий апостериорного распределения параметров.
\end{proof}

Теорема~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression} является ключевой в настоящем разделе, так как при слабых и понятных предположениях из нее следует сходимость моментов апостериорного распределения параметров.
Первое предположение в теореме~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression} касается ограничения на область значений признаков и целевой переменной.
Это условие обычно выполняется в практических приложениях, поэтому оно служит в первую очередь для целей теоретического анализа.
Второе условие теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression} представляет больший интерес, поскольку оно углубляется в поведение минимального собственного значения выборочной ковариационной матрицы признаков.
Следует отметить, что в рамках настоящей работы не приводятся строгие теоретические гарантии для данной сходимости, однако эмпирические результаты подтверждают выполнение указанного условия.

\section{Результаты вычислительных экспериментов}

В настоящем разделе представлены результаты вычислительных экспериментов для методов определения достаточного размера выборки, описанных в предыдущих разделах главы. Эксперименты направлены на валидацию теоретических результатов и сравнение эффективности различных подходов.

\subsection{Определения достаточного размера выборки на основе статистических методов}
\begin{table}[h!t]
\centering
\caption{Характеристики выборок, используемых для анализа качества методов определения достаточного размера выборки. Таблица содержит информацию о типе задачи, количестве признаков и общем размере выборки для каждого набора данных.}
\label{chapter:samplesize:experiment:static:table20}
\begin{tabular}{|l|l|c|c|}
\hline
	\centering Выборка & Задача & Число признаков & Размер выборки\\ \hline
	\hline 	Boston Housing 	&regression		&14 & 506\\
	\hline	Diabets  				& regression		&20  & 576\\
	\hline	Forest Fires 			& regression		& 13 & 517\\
  	\hline	Servo 					& regression 	& 4   & 167\\
	\hline	NBA				 		& classification	& 12 & 2235\\
\hline
\end{tabular}
\end{table} 


Проводится эксперимент для анализа свойств методов оценки достаточного размера выборки. Эксперимент состоит из трех частей.

В первой части рассматриваются оценки достаточного размера выборки для различных наборов данных с фиксированным набором гиперпараметров различных методов. В качестве данных использовались выборки, описанные в таблице~\ref{chapter:samplesize:experiment:static:table20}. Результаты представлены в таблице~\ref{chapter:samplesize:experiment:static:table2}, где показаны оценки размера выборки для соответствующих выборок.

Во второй части исследуется зависимость достаточного размера выборки от имеющегося размера выборки. В третьей части исследуется поведение методов в зависимости от изменения гиперпараметров методов.
 
\begin{table}[h!t]
\centering
\caption{Сравнение оценок достаточного размера выборки, полученных различными статистическими и байесовскими методами для пяти наборов данных. Результаты демонстрируют значительный разброс оценок между методами, что указывает на различную консервативность подходов.}
\label{chapter:samplesize:experiment:static:table2}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Методы                       & Boston & Diabetes & Forest Fires & Servo & NBA \\ \hline\hline
Lagrange Multipliers Test & 18             & 25       & 44          & 38    & 218 \\ \hline
Likelihood Ratio Test     & 17             & 25       & 43          & 18    & 110 \\ \hline
Wald Test                 & 66             & 51       & 46          & 76    & 200 \\ \hline
Cross Validation          & 178            & 441      & 172         & 120   & --   \\ \hline
Bootstrap                 & 113            & 117      & 86          & 60    & 405 \\ \hline
APVC                      & 98             & 167      & 351         & 20    & --   \\ \hline
ACC                       & 228            & 441      & 346         & 65    & --   \\ \hline
ALC                       & 98             & 267      & 516         & 25    & --   \\ \hline
Utility Function          & 148            & 172      & 206         & 105   & 925 \\ \hline
\end{tabular}
\end{table}

В данной части вычислительного эксперимента анализируется сходимость различных методов на различных выборках. В эксперименте используются выборки: Boston Housing~\cite{boston1978}, Diabetes, Forest Fires, Servo~\cite{servo1992}, NBA.
Результат анализа представлен в таблице~\ref{chapter:samplesize:experiment:static:table2}. Символ ``--'' обозначает, что исходный размер выборки недостаточный для прогноза.

Гиперпараметры каждого метода для всех выборок описаны в таблице~\ref{chapter:samplesize:experiment:static:table3}. Поскольку критерии Лагранжа, отношения правдоподобия и Вальда асимптотически эквивалентны, то параметры этих методов задавались одинаково. Параметры методов <<Average Coverage>> и <<Average Length>> также задаются одинаково.

\begin{table}[h!t]
\begin{center}
\caption{Гиперпараметры методов оценки достаточного размера выборки, установленные экспертно для экспериментов. Параметры включают уровни значимости $\alpha$, вероятности ошибки второго рода $\beta$, пороговые значения $\varepsilon$ и $l$, а также параметры обобщенных линейных моделей.}
\label{chapter:samplesize:experiment:static:table3}
\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline 
Method& GLM parameters&~$l$&~$\varepsilon$&~$\alpha$&~$\beta$\\ \hline
\hline	
Lagrange	Multipliers Test	&~$\textbf{w}_{u}^0$ & -- & 0.2& 0.05& 0.2\\
\hline	
Likelihood Ratio Test			&~$\textbf{w}_{u}^0$ & -- & 0.2& 0.05& 0.2\\
\hline	
Wald	Test								&~$\textbf{w}_{u}^0$ & -- & 0.2& 0.05& 0.2\\
\hline	
Cross Validation 					& -- & -- 	& 0.05& -- & --\\
\hline	
Bootstrap 								& -- & 0.5	& -- & 0.05& --\\
\hline	
APVC 									& -- & 0.5	& -- & -- & --\\
\hline	
ACC 									& -- & 0.25	& -- & 0.05& --\\
\hline	
ALC 										& -- & 0.5	& -- & 0.05& --\\
\hline	
Utility function 						& -- & -- 	& 0.005& -- & --\\
\hline
\end{tabular}
\end{center}
\end{table}


Вычислительный эксперимент проводился для анализа описанных методов. Выбирается некоторый размер выборки~$m$ и методом бутстрап семплируется множество подвыборок размером~$m$. Для разных значений~$m$ вычисляется~$m^*$.
    
\begin{figure}[h!t]\center
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/cross}
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/apvc}\\
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/acc}
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/alc}\\
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/bootstrap}
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/kl}\\
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/maxu}
    \caption{Зависимость статистических значений различных методов определения достаточного размера выборки от размера подвыборки для наборов данных Boston Housing, Diabetes, Forest Fires, Servo и NBA. Все представленные функции монотонны и асимптотически стремятся к константе, что подтверждает корректность методов.}
    \label{chapter:samplesize:experiment:static:fig1}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=0.85\textwidth]{thesis/figures/chapter-3/statical/graphs}
    \caption{Зависимость оцененного достаточного размера выборки $m^*$ от доступного размера выборки $m$ для различных методов на наборе данных Boston Housing. Результаты демонстрируют сходимость методов и низкую дисперсию оценок, что указывает на вычислительную устойчивость рассмотренных подходов.}
    \label{chapter:samplesize:experiment:static:fig2}
\end{figure}

На рис.~\ref{chapter:samplesize:experiment:static:fig1} демонстрируется зависимость статистических показателей каждого метода для разных выборок с фиксированным размером выборки~$m$. Пороговые значения для каждого метода устанавливаются экспертно, что позволяет контролировать различные статистические характеристики выборки.

Представленные функции являются монотонными и асимптотически стремятся к константе, что подтверждает корректность различных методов определения достаточного размера выборки.

На рис.~\ref{chapter:samplesize:experiment:static:fig2} показаны результаты методов на выборках различного размера. Наблюдается различие методов в дисперсии вычисленного~$m^*$. Все представленные методы демонстрируют сходимость, причем результат предсказания в асимптотике не зависит от доступного размера выборки~$m$.   

Небольшое значение дисперсии интерпретируется как вычислительная устойчивость рассмотренных методов.

Показано, что некоторые методы не дают оценку достаточного размера выборки, если доступный размер выборки недостаточен для применения метода. Это означает, что указанные методы не эффективны с точки зрения прогнозирования необходимого объема данных на ранних этапах эксперимента, однако могут быть использованы для ретроспективного анализа уже проведенных экспериментов.

Анализируется оценка достаточного размера выборки в зависимости от гиперпараметров для байесовских методов, а также эвристических методов. Для анализа рассмотрена выборка Boston Housing.

Байесовские методы используют решающее правило над скалярной функцией для определения достаточного размера выборки.
На рис.~\ref{chapter:samplesize:experiment:static:fig1} показана зависимость скалярных функций от размера подвыборки.
Наблюдается, что указанные функции являются монотонными.
Характер поведения функции определяется выбранным методом. Изменение ограничений, установленных экспертно, позволяет варьировать размер выборки, соответствующий заданным ограничениям.


\subsection{Определение достаточного размера выборки на основе сэмплирования эмпирической функции ошибки}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{thesis/figures/chapter-3/likelihood-bootstraping/synthetic-regression}
    \caption{Сходимость функций $D(k)$ и $M(k)$ для синтетического набора данных регрессии. Обе функции стремятся к нулю с увеличением размера выборки, что подтверждает теорему~\ref{chapter:samplesize:theorem-kiselev-likelihood-bootstraping}.}
    \label{chapter:samplesize:experiment:likelihood-bootstraping:fig:synthetic-regression}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{thesis/figures/chapter-3/likelihood-bootstraping/synthetic-classification}
    \caption{Сходимость функций $D(k)$ и $M(k)$ для синтетического набора данных классификации. Обе функции демонстрируют монотонное убывание к нулю с увеличением размера выборки, подтверждая применимость методов D- и M-достаточности для задач классификации.}
    \label{chapter:samplesize:experiment:likelihood-bootstraping:fig:synthetic-classification}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{thesis/figures/chapter-3/likelihood-bootstraping/liver-disorders}
    \caption{Сходимость функций $D(k)$ и $M(k)$ для набора данных Liver Disorders (345 объектов, 5 признаков, $B=1000$ бутстрэп-подвыборок). Обе функции демонстрируют сходимость к нулю, что подтверждает теоретические результаты и демонстрирует применимость методов на реальных данных.}
    \label{chapter:samplesize:experiment:likelihood-bootstraping:fig:liver-disorders}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{thesis/figures/chapter-3/likelihood-bootstraping/sufficient-vs-threshold}
    \caption{Зависимость достаточного размера выборки $m^*$ от порогового параметра $\varepsilon$ для методов D- и M-достаточности на трех наборах данных. С увеличением значения порога $\varepsilon$ достаточный размер выборки монотонно уменьшается, что позволяет выбирать меньше объектов для достижения заданного уровня стабильности функций $D(k)$ и $M(k)$.}
    \label{chapter:samplesize:experiment:likelihood-bootstraping:fig:sufficient-vs-threshold}
\end{figure}

\begin{table}[h!t]\center
    \caption{Сравнение оценок достаточного размера выборки методами D- и M-достаточности для 13 наборов данных с задачей регрессии. Методы демонстрируют сопоставимые результаты для большинства наборов данных, при этом M-достаточность иногда требует большего размера выборки.}\label{chapter:samplesize:experiment:likelihood-bootstraping:table}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    Dataset name & Objects~$m$ & Features~$n$ & D & M \\
    \hline
    Abalone & 4177 & 8 & 96 & 96  \\
    Auto MPG & 392 & 8 & 15 & 15 \\
    Automobile & 159 & 25 & 70 & 156  \\
    Liver Disorders & 345 & 6 & 12 & 19  \\
    Servo & 167 & 4 & 41 &~---  \\
    Forest fires & 517 & 12 & 208 &~--- \\
    Wine Quality & 6497 & 12 & 144 & 144  \\
    Energy Efficiency & 768 & 9 & 24 & 442  \\
    Student Performance & 649 & 32 & 129 & 177  \\
    Facebook Metrics & 495 & 18 & 31 & 388   \\
    Real Estate Valuation & 414 & 7 & 15 & 23  \\
    Heart Failure Clinical Records & 299 & 12 & 63 & 224  \\
    Bone marrow transplant: children & 142 & 36 &~--- &~--- \\
    \hline
    \end{tabular}
\end{table}

В настоящем разделе представлено эмпирическое исследование предложенных методов. Эксперименты проводились на синтетических данных и наборе данных Liver Disorders из~\cite{uci}.

Синтетические данные были сгенерированы из моделей линейной регрессии и логистической регрессии. Количество объектов составляет 1000, количество признаков~---~$20$.
Использовалось~$B=1000$ бутстрэп-подвыборок.
Вычислялись значения~$D(k)$ и~$M(k)$.
Набор данных регрессии Liver Disorders содержит 345 объектов и 5 признаков.
Также использовалось~$B=1000$ подвыборок, полученных методом бутстрэпа, для оценки математического ожидания и дисперсии функции потерь.

На рис.~\ref{chapter:samplesize:experiment:likelihood-bootstraping:fig:synthetic-regression} показаны полученные зависимости между доступным размером выборки~$k$ и предложенными функциями~$D(k)$ и~$M(k)$ для синтетического набора данных регрессии.
Результаты для синтетического набора данных классификации представлены на рис.~\ref{chapter:samplesize:experiment:likelihood-bootstraping:fig:synthetic-classification}.
На рис.~\ref{chapter:samplesize:experiment:likelihood-bootstraping:fig:liver-disorders} представлены графики для набора данных Liver Disorders. Наблюдается, что во всех случаях значения~$D(k)$ и~$M(k)$ приближаются к нулю с увеличением размера выборки.
Эти эмпирические результаты подтверждают полученные ранее теоретические выводы.

В определениях D-достаточности и M-достаточности присутствует гиперпараметр~$\varepsilon$, который соответствует порогу для достаточного размера выборки~$m^*$.
Для изучения зависимости между указанными величинами построена зависимость на рис.~\ref{chapter:samplesize:experiment:likelihood-bootstraping:fig:sufficient-vs-threshold}, которая демонстрирует возможные размеры выборки для обеспечения заданного уровня достоверности.

Для сравнения производительности предложенных методов на различных наборах данных были выбраны выборки из открытого репозитория~\cite{uci}.
Подробная информация о каждом наборе данных, количестве наблюдений и количестве признаков представлена в Таблице~\ref{chapter:samplesize:experiment:likelihood-bootstraping:table}.
В демонстрационных целях было выбрано значение гиперпараметра~$\varepsilon$, при котором значение целевой функции,~$D(k)$ или~$M(k)$, уменьшается вдвое.
Соответствующие результаты представлены в Таблице~\ref{chapter:samplesize:experiment:likelihood-bootstraping:table}.
Пропуски означают, что исходный размер выборки недостаточен.

\subsection{Определение достаточного размера выборки на основе
близости апостериорных распределений}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{figures/chapter-3/posterior-distribution/eigvals}
    \caption{Зависимость минимального собственного значения $\lambda_{\min}(\mathbf{X}_k^\top \mathbf{X}_k)$ от размера выборки $k$ для синтетических данных регрессии (500 объектов, 10 признаков) и набора данных Liver Disorders (345 объектов, 5 признаков). Асимптотическое поведение соответствует условию теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression}: $\lambda_{\min} = \omega(\sqrt{k})$ при $k \to \infty$.}
    \label{chapter:samplesize:experiment:fig:eigvals}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{figures/chapter-3/posterior-distribution/synthetic-regression}
    \caption{Сходимость функций $KL(k)$ и $S(k)$ для синтетического набора данных регрессии. Функция $KL(k)$ стремится к нулю, а $S(k)$ стремится к единице, что подтверждает теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity} и~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-s-score}.}
    \label{chapter:samplesize:experiment:fig:synthetic-regression}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{figures/chapter-3/posterior-distribution/liver-disorders}
    \caption{Сходимость функций $KL(k)$ и $S(k)$ для набора данных Liver Disorders (345 объектов, 5 признаков, нормальное априорное распределение, $B=100$ повторений). Результаты демонстрируют сходимость $KL(k)$ к нулю и $S(k)$ к единице, подтверждая теоретические предсказания и применимость методов KL- и S-достаточности на реальных данных.}
    \label{chapter:samplesize:experiment:fig:liver-disorders}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{figures/chapter-3/posterior-distribution/sufficient-vs-threshold}
    \caption{Зависимость достаточного размера выборки $m^*$ от порогового параметра $\varepsilon$ для методов KL- и S-достаточности на синтетических данных регрессии и наборе данных Liver Disorders. Метод S-достаточности требует более низких значений порога для достижения заданного уровня близости распределений, что указывает на его более строгие требования к качеству оценки.}
    \label{chapter:samplesize:experiment:fig:sufficient-vs-threshold}
\end{figure}

\begin{table}[h!t]\center
    \caption{Характеристики выборок, используемых для сравнения методов определения достаточного размера выборки на основе близости апостериорных распределений. Все наборы данных соответствуют задаче регрессии и используются для оценки методов KL- и S-достаточности.}\label{chapter:samplesize:experiment:table:descr}
    \begin{tabular}{lcc}
    \hline
        \toprule
        Выборка & Количество признаков,~$n$ & Количество объектов,~$m$ \\ 
        \midrule
        Boston Housing & 14 & 506 \\ 
        Diabetes & 10 & 576 \\ 
        Forest Fires & 13 & 517 \\ 
        Servo & 4 & 167 \\ 
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!t]\center
    \caption{Сравнение оценок достаточного размера выборки, полученных классическими методами и предложенными методами KL- и S-достаточности для четырех наборов данных регрессии. Метод KL-достаточности дает более консервативные оценки, требующие почти полной выборки, в то время как S-достаточность указывает на минимальные размеры выборки.}\label{chapter:samplesize:experiment:table:results}
    \begin{tabular}{lcccc}
    \hline
        \toprule
        Methods and sample sets & Boston & Diabetes & Forest Fires & Servo \\ 
        \midrule
        Lagrange Multipliers Test & 18 & 25 & 44 & 38 \\
        Likelihood Ratio Test & 17 & 25 & 43 & 18 \\
        Wald Test & 66 & 51 & 46 & 76 \\ 
        Cross Validation & 178 & 441 & 171 & 120 \\ 
        Bootstrap & 113 & 117 & 86 & 60 \\ 
        APVC & 98 & 167 & 351 & 20 \\ 
        ACC & 228 & 441 & 346 & 65 \\ 
        ALC & 98 & 267 & 516 & 25 \\ 
        Utility function & 148 & 172 & 206 & 105 \\ 
        \midrule
        KL (ours) & 493 & 437 & 86 & 165 \\ 
        S (ours) & 28 & 22 & 26 & 10 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{figures/chapter-3/posterior-distribution/dependence_on_available_sample_set.pdf}
    \caption{Зависимость оцененного достаточного размера выборки $m^*$ от доступного размера выборки $m$ для классических методов и предложенных методов KL- и S-достаточности на наборе данных Boston Housing. Критерий KL-достаточности является наиболее консервативным и требует почти полной выборки, в то время как S-достаточность указывает на минимальные размеры, что связано с высокой чувствительностью расхождения Кульбака-Лейблера к изменениям распределений.}
    \label{chapter:samplesize:experiment:fig:dependence_on_available_sample_set}
\end{figure}

В настоящем разделе представлено расширенное эмпирическое исследование предложенных методов определения достаточного размера выборки на основе близости апостериорных распределений. Эксперименты состоят из трех частей.

В первой части проверяются сходимости, полученные в ходе теоретического анализа. А именно, сначала рассматривается поведение минимального собственного значения матрицы~$\mathbf{X}_k^\top \mathbf{X}_k$ при увеличении размера выборки, что необходимо для выполнения условий теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression}. Затем исследуется сходимость предложенных функций~$KL(k)$ и~$S(k)$ к их предельным значениям. Наконец, изучается зависимость достаточного размера выборки от пороговых параметров~$\varepsilon$. Эксперимент проводится на двух наборах данных: синтетическая регрессия и Liver Disorders.

Во второй части оцениваются размеры выборок для различных наборов данных, используя разные подходы (KL- и S-достаточность, а также классические методы). В третьей части изучается зависимость достаточного размера выборки от объема доступных данных, что позволяет оценить стабильность методов при различных объемах выборок.

Синтетические данные генерируются из модели линейной регрессии. Количество объектов составляет 500, количество признаков~--- 10. Для генерации синтетического набора данных регрессии исходные признаки, параметры модели и шумовые остатки генерируются из стандартного нормального распределения. 

Априорное распределение параметров также задано как стандартное нормальное, как для синтетической регрессии, так и для набора данных Liver Disorders, который содержит 345 объектов и 5 признаков. Входные признаки предобрабатываются с использованием стандартного метода масштабирования данных~(англ. Standard Scaler).

Процедура эксперимента организована следующим образом. Один объект последовательно удалялся из заданной выборки до тех пор, пока количество объектов в подвыборке не становилось равным количеству признаков. Для каждого размера выборки~$k$ вычисляется минимальное собственное значение матрицы~$\mathbf{X}_k^\top \mathbf{X}_k$, а также значения~$KL(k)$ и~$S(k)$. Этот процесс повторялся~$B=100$ раз для обеспечения статистической надежности результатов.

На рис.~\ref{chapter:samplesize:experiment:fig:eigvals} показано асимптотическое поведение минимального собственного значения матрицы~$\mathbf{X}_k^\top \mathbf{X}_k$ при увеличении размера выборки.
Наблюдается, что при стремлении размера выборки к бесконечности минимальное собственное значение также стремится к бесконечности.
При этом, как и требуется для теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression}, график лежит выше~$\sqrt{k}$.

На рис.~\ref{chapter:samplesize:experiment:fig:synthetic-regression} представлены зависимости между доступным размером выборки~$k$ и предложенными функциями~$KL(k)$ и~$S(k)$ для синтетического набора данных регрессии.
На рис.~\ref{chapter:samplesize:experiment:fig:liver-disorders} представлены аналогичные графики для набора данных Liver Disorders.
Наблюдается, что в обоих случаях значение~$KL(k)$ приближается к нулю с увеличением размера выборки, а~$S(k)$ стремится к единице.
Эти эмпирические результаты подтверждают полученные ранее теоретические выводы.

В определениях KL-достаточности и S-достаточности присутствует гиперпараметр~$\varepsilon$, который соответствует порогу для достаточного размера выборки~$m^*$.
На рис.~\ref{chapter:samplesize:experiment:fig:sufficient-vs-threshold} показана зависимость между уровнем достоверности и размером выборки.
Для сравнения предложенных методов с базовыми использовалась следующая схема эксперимента.
Модель машинного обучения~--- линейная регрессия.
Выбрано 4 набора данных с задачей регрессии из открытых источников: Boston, Diabetes, Forestfires и Servo.
Их описательная статистика представлена в Таблице~\ref{chapter:samplesize:experiment:table:descr}.
К данным применяются 9 различных базовых методов оценки размера выборки: Тест множителей Лагранжа, Тест отношения правдоподобий, Тест Вальда, Кросс-валидация, Бутстрэп, Критерий средней апостериорной дисперсии (APVC), Критерий среднего покрытия (ACC), Критерий средней длины (ALC) и Функция полезности.

Далее в экспериментах достаточность определяется в терминах относительного изменения, а именно, размер выборки достаточным, если функция~$KL(k)$ имеет относительное отклонение от своего значения на всей выборке не более чем~$\varepsilon$.
Аналогично с функцией~$S(k)$, зафиксировав~$\varepsilon=0.05$ получаем результирующие размеры выборок.

Результаты в Таблице~\ref{chapter:samplesize:experiment:table:results} указывают на то, что критерий на основе расхождения Кульбака-Лейблера является более консервативным и требует большего размера выборки, в то время как критерий S-достаточности предполагает, что минимального размера выборки может быть достаточно.
Предполагается, что это типичный результат для функции схожести s-score, которая была разработана для сравнения различных моделей машинного обучения, особенно в случаях с неинформативными распределениями.
Если распределения имеют высокую дисперсию, функция близости приближается к единице, что приводит к тому, что критерий считает достаточным даже небольшой размер выборки.

Далее проводится комплексный анализ различных методов определения размера выборки.
Анализируется зависимость достаточного размера выборки от объема доступного набора данных.
В частности, при увеличении объема доступной выборки вычисляется достаточный размер на основе различных методов.
Результаты представлены на рис.~\ref{chapter:samplesize:experiment:fig:dependence_on_available_sample_set}, который сравнивает вышеупомянутые методы с точки зрения их консервативности.

Наблюдается, что S-достаточный размер выборки часто является минимальным.
KL-достаточный размер выборки, как правило, требует почти полной выборки.
Предполагается, что это связано с тем, что расхождение Кульбака-Лейблера чрезвычайно чувствительно к изменениям математического ожидания и дисперсии сравниваемых распределений.
Таким образом, стабилизация расстояния между ними происходит довольно поздно.

\section{Заключение по главе}

В настоящей главе решена задача разработки практических методов определения достаточного размера выборки для задач машинного обучения, что восполняет пробел между теоретическим аппаратом, разработанным в главах~\ref{chapter:complexity} и~\ref{chapter:gesian}, и практическими потребностями планирования экспериментов. В отличие от теоретических оценок, основанных на анализе матриц Гессе, предложенные методы используют наблюдаемые характеристики процесса обучения, что делает их применимыми в реальных задачах.

В главе проведен систематический обзор существующих подходов к определению достаточного размера выборки, включая статистические методы, байесовские методы и эвристические методы. Выявлены ключевые ограничения классических подходов: необходимость знания дисперсии оценки параметра или параметра нецентральности, отсутствие алгоритмических процедур для их получения, а также вычислительная сложность для моделей глубокого обучения.

Основным теоретическим вкладом главы является разработка двух новых методов определения достаточного размера выборки, основанных на анализе стабильности процесса обучения.

Первый метод основан на анализе функции правдоподобия при изменении объема данных. Введены два критерия достаточности: D-достаточность, использующая дисперсию функции правдоподобия на бутстрэп-подвыборках, и M-достаточность, анализирующая разность математических ожиданий функции правдоподобия при последовательном добавлении объектов в выборку. Теорема~\ref{chapter:samplesize:theorem-kiselev-likelihood-bootstraping} строго доказывает корректность определения M-достаточного размера выборки для модели линейной регрессии при выполнении условий сходимости математических ожиданий и ковариационных матриц оценок параметров. Следствие~\ref{chapter:samplesize:corollary-kiselev-likelihood-bootstraping} устанавливает достаточные условия сходимости к истинным значениям параметров и информационной матрице Фишера.

Второй метод основан на анализе близости апостериорных распределений параметров модели на близких подвыборках, отличающихся одним объектом. Введены критерии KL-достаточности и S-достаточности, использующие расхождение Кульбака-Лейблера и функцию схожести s-score соответственно. Для нормального апостериорного распределения получены аналитические выражения для этих мер близости, что позволило провести строгий теоретический анализ. Теорема~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity} доказывает корректность определения KL-достаточного размера выборки при сходимости математических ожиданий и ковариационных матриц апостериорных распределений. Теорема~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-s-score} устанавливает корректность S-достаточности при более слабых условиях~--- требуется только сходимость математических ожиданий. Теорема~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression} для модели линейной регрессии с нормальным априорным распределением устанавливает достаточные условия сходимости моментов апостериорного распределения при условии роста минимального собственного значения матрицы $\mathbf{X}_k^\top \mathbf{X}_k$ как $\omega(\sqrt{k})$ при $k \to \infty$.

Проведены обширные вычислительные эксперименты на синтетических и реальных данных, подтвердившие эффективность предложенных методов. Эксперименты на синтетических данных регрессии и классификации, а также на наборах данных Liver Disorders, Boston Housing, Diabetes, Forest Fires, Servo и других продемонстрировали сходимость функций $D(k)$, $M(k)$ и $KL(k)$ к нулю, а функции $S(k)$~--- к единице с ростом объема выборки, что согласуется с теоретическими предсказаниями. Сравнительный анализ с классическими методами выявил особенности различных критериев: KL-дивергенция дает более консервативные оценки, требующие почти полной выборки, в то время как S-достаточность часто указывает на достаточность минимального размера выборки, что связано с высокой чувствительностью расхождения Кульбака-Лейблера к изменениям распределений.

Практические рекомендации включают использование относительных отклонений от значений на полной выборке с порогами $0.05-0.1$ для методов на основе функции правдоподобия, а также применение бутстрэпирования с $B=1000$ подвыборок для оценки математических ожиданий и дисперсий. Для методов на основе близости апостериорных распределений рекомендуется использовать порог $\varepsilon=0.05$ для S-достаточности.

Основные ограничения методов связаны с вычислительной сложностью обращения ковариационных матриц для моделей с большим количеством параметров, а также с предположением о нормальности апостериорного распределения для методов KL- и S-достаточности. Кроме того, теоретическое обоснование методов в настоящее время ограничено моделями линейной регрессии и логистической регрессии.

Перспективными направлениями дальнейших исследований являются расширение теоретического обоснования методов на более сложные модели, в том числе нейронные сети, преодоление ограничения о нормальности апостериорного распределения, разработка приближенных методов для оценки близости распределений в высокомерных пространствах параметров, а также интеграция предложенных методов с теоретическими оценками ландшафтной меры сложности из главы~\ref{chapter:complexity}.

Полученные результаты создают основу для разработки практических инструментов оценки достаточного объема данных в прикладных задачах машинного обучения и обеспечивают мост между теоретическим формализмом сложности моделей и данных, разработанным в предыдущих главах, и практическими потребностями планирования экспериментов.