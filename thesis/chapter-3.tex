В главе~\ref{chapter:complexity} было показано, что частным случаем условной меры сложности данных является достаточный размер выборки. В рамках текущей главы рассмотрим различные методы определения достаточного размера выборки для различных моделей, от линейных до моделей глубокого обучения.

Планирование эксперимента требует оценки минимального размера выборки: числа выполненных измерений набора характеристик, необходимых для построения сформулированных условий. Выбор метода оценки размера выборки зависит от решаемой задачи, которая определяет формулировку статистической гипотезы и статистики для ее проверки. В целом существуют различные подходы к определению достаточного объема выборки, такие как статистические, байесовские и эвристические методы.

Статистические методы предполагают, что выборка соответствует некоторым предварительным условиям, сформулированным ранее. Эти условия сформулированы как статистический критерий~\cite{self1988,self1992,shieh2000,demidenko2007}. Метод оценки размера выборки, связанный с этим критерием, гарантирует достижение фиксированной статистической мощности~$1-\beta$ со степенью ошибки первого рода, не превышающей установленное значение~$\alpha$. Такой размер выборки называется достаточным.

Однако практическое применение методов оценки размера выборки предполагает, что модель соответствует измеренным данным~\cite{kloek1975}. Эти модели выбираются в соответствии с постановкой задачи регрессии или классификации. В этой статье представлены обобщенные линейные модели. В статье~\cite{self1992} предложен подход к оценке мощности и размера выборки, связанной с ней, на основе теста отношения максимального правдоподобия. Этот подход оказался более точным для ряда независимых переменных. Кроме того, в статье~\cite{shieh2005} предложен метод оценки мощности для статистики Вальда. В статье~\cite{motrenko2014} в случае логистической регрессии предлагается использовать метод, использующий кривую ROC-AUC и концепцию сдвига. Классические методы~\cite{self1988,self1992,shieh2000,shieh2005,demidenko2007} имеют ряд ограничений, связанных с практическим применением этих методов. Чтобы оценить размер выборки, необходимо знать дисперсию оценки параметра или, в более общем случае, иметь оценку параметра нецентральности в распределении статистики, используемой, когда альтернативная гипотеза верна. Эти методы не показывают как получить эти значения. Кроме того, дисперсия оценки и параметр нецентральности не будут получены с определенной дисперсией, влияние которой на результат оценки размера выборки не имеет значения.

Статистические методы позволяют оценить размер выборки на основе предположений о распределении данных и информации о соответствии между наблюдаемыми значениями и предположениями нулевой гипотезы. Когда размер исследуемой выборки является достаточным или чрезмерным, можно использовать методы, основанные на наблюдении изменения определенной характеристики процедуры построения модели при увеличении размера выборки. В частности, наблюдая за соотношением качества прогнозирования с контрольной выборкой и обучающей выборкой~\cite{motrenko2014}, определяется достаточный размер выборки, который соответствует началу переобучения. В статье~\cite{qumsiyeh2013} для оценки достаточного размера выборки используется процедуру бутстрап. Превышение текущего размера выборки проверяется на основе анализа доверительных интервалов оцениваемого параметра. Ширина доверительного интервала с разными значениями объема выборки оценивается с помощью метода бутстрапа. Для этого выборки меньшего размера отбираются заданное число раз и вычисляется доверительный интервал ошибки при оценке параметра модели. Размер выборки считается достаточным, если ширина доверительного интервала не превышает заранее установленного значения.

Перечисленные выше ограничения статистических методов оценки размера выборки подробно исследуются в байесовской процедуре~\cite{lindley1997,rubin1998,wang2002}, где оценка размера выборки определяется на основе максимизации ожидаемого значение некоторой функции качества~\cite{lindley1997}. Функция качества может включать в себя явные функции распределения параметров и штрафы за увеличение размера выборки. Альтернативой подходам~\cite{wang2002}, основанным на функции качества, является выборка размера выборки путем установления ограничений на определенный критерий качества оценки параметров модели. Примеры критериев: критерий средней апостериорной дисперсии (AVPC), критерий средней длины (ALC), критерий среднего покрытия (ACC). Для каждого перечисленного критерия оценка размера выборки определяется как минимальное значение размера выборки, для которого ожидаемое значение выбранного критерия не превышает какого-либо фиксированного порога. В статье~\cite{motrenko2014} предлагается считать размер выборки достаточным, если расстояние Кульбака-Лейблера между распределениями, оцененными на основе подвыборок такого размера, достаточно мало. Такой подход не требует дальнейшего обобщения в случае нескольких переменных. Кроме того, оценка может производиться как при наличии предположений о распределении данных, так и при их отсутствии. Недостаток этого подхода заключается в том, что количественная оценка может быть получена только при чрезмерно большом размере выборки.

\section{Статистические методы определения достаточного размера выборки}

Задана выборка размера~$m$:
\[
\label{eq:ps:1}
\begin{aligned}
	\mathfrak{D}_{m} = \{\textbf{x}_i, y_i\}_{i = 1}^{m},
\end{aligned}
\]
где~$\textbf{x}_i\in \mathbb{R}^{n}$,~$y_i\in \mathbb{Y}$. Вектор признаков~$\textbf{x} = [\textbf{u}, \textbf{v}]$ соединяет~$\textbf{u}_i\in \mathbb{R}^{k}$ and~$ \textbf{v}_i\in \mathbb{R}^{n-k}$.
Выборка~$\mathfrak{D}_{m}$ случайным образом делиться на обучающую и тестовую части:
\[
\label{eq:ps:2}
\begin{aligned}
	\mathfrak{D}_{\mathcal{T}_{m}} = \{\textbf{x}_i, \textbf{y}_i\}_{i \in \mathcal{T}_{m}}, \quad \mathfrak{D}_{\mathcal{L}_{m}} = \{\textbf{x}_i, \textbf{y}_i\}_{i \in \mathcal{L}_{m}}, \quad  \mathcal{T}_{m}\sqcup\mathcal{L}_{m} = \{1, ..., m\}.
\end{aligned}
\]
Введем параметрическое семейство функций для аппроксимации неизвестного распределения~$p(y|\textbf{x}, \mathfrak{D}_{\mathcal{L}_{m}})$:
\[
\label{eq:ps:3}
\begin{aligned}
	\mathfrak{F} = \left\{f\left(y,\textbf{x}, \textbf{w}\right)|\textbf{w}\in\mathbb{W}, \int_{y\in \mathbb{Y}, \textbf{x}\in\mathbb{R}^{n}}f\left(y, \textbf{x}, \textbf{w}\right)dyd\textbf{x}=1\right\}.
\end{aligned}
\]

Для модели~$f$ с вектором параметров~$\textbf{w}$ определим функцию правдоподобия и логарифм функции прадоподобия выборки~$\mathfrak{D}$:
\[
\label{eq:ps:4}
\begin{aligned}
	L\left(\mathfrak{D}, \textbf{w}\right) = \prod f\left(y,\textbf{x}, \textbf{w}\right),\quad l\left(\mathfrak{D}, \textbf{w}\right) = \sum \log f\left(y,\textbf{x}, \textbf{w}\right),
\end{aligned}
\]
где~$f(y,\textbf{x}, \textbf{w})$ является оценкой правдоподобия выборки~$\mathfrak{D}_{\mathcal{L}}$ с заданым вектором параметров~$\textbf{w}$.
Используя принцип максимального правдоподобия для оценки параметров~$\textbf{w}$
\[
\label{eq:ps:5}
\begin{aligned}
	\hat{\textbf{w}} = \arg\max_{\textbf{w}\in\mathbb{W}}L\left(\mathfrak{D}_{\mathcal{L}}, \textbf{w}\right).
\end{aligned}
\]

Информационная матрица Фишера имеет вид:
\[
\label{eq:ps:6}
\begin{aligned}
	\textbf{I}\left(\mathfrak{D}, \textbf{w}\right) = -\nabla\nabla^{\mathsf{T}}l\left(\mathfrak{D}, \textbf{w}\right), \quad  \textbf{V} = \textbf{I}^{-1}\left(\mathfrak{D}, \textbf{m}\right),
\end{aligned}
\]
статистические методы и байесовские методы используют информационную матрицу Фишера для оценки размера выборки.

Основным преимуществом методов, основанных на статистике, является их способность оценивать достаточный размер выборки при недостаточном наборе выборки. Они позволяют прогнозировать необходимое число образцов на ранней стадии эксперимента.

Плотность распределения целевой переменной
\[
\label{eq:sb:1}
\begin{aligned}
	p(y|\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}) = \exp\bigl(y\theta- b(\theta) + c\left(y\right)\bigr),
\end{aligned}
\]
где~$\theta$ является параметром распределения, полученный с помощью функции связи~$\theta=\theta\bigr(\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}\bigr)$.

Тестируемая гипотеза
\[
\label{eq:sb:2}
\begin{aligned}
	H_0: \textbf{m}_{u} = \textbf{m}^0_{u}, \quad H_1: \textbf{m}_{u} \not= \textbf{m}^0_{u}.
\end{aligned}
\]

Пусть статистики~$S_{m,u}\left(\textbf{w}_{u}, \textbf{w}_{v}\right)$ и~$S_{m,v}\left(\textbf{w}_{u}, \textbf{w}_{v}\right)$ являются производными логарифма правдоподобия выборки~$\mathfrak{D}_{m}$ в точках~$\textbf{w}_{u}$ и~$\textbf{w}_{v}$.
Рассмотрим~$\textbf{s}_{m} = S_{m,u}\left(\textbf{m}^{0}_{u}, \hat{\textbf{w}}^{0}_{v}\right)$, где~$\hat{\textbf{w}}^{0}_{v}$ получается из уравнения
\[
\label{eq:sb:3}
\begin{aligned}
	S_{m,v}\left(\textbf{m}^{0}_{u}, \textbf{w}_{v}\right) = 0.
\end{aligned}
\]
Статистика Лагранджа равняется
\[
\label{eq:sb:4}
\begin{aligned}
	LM = \textbf{s}^{\mathsf{T}}_{m}\textbf{Q}_{m}^{-1}\textbf{s}_{m}.
\end{aligned}
\]
где~$\textbf{Q}_{m}$ ковариационная матрица вектора~$\textbf{s}_{m}$.
	
В случае истинности гипотезы~$H_0$ статистика~$LM$ асимптотически имеет распределения~$\chi^2(k)$.  В~\cite{self1988} показано, что при альтернативной гипотезе~$H_1$ статистика ~$LM$ асимптотически имеет распределения~$\chi^2(k,\gamma)$, где~$\gamma$ является параметром нецентральности
\[
\label{eq:sb:5}
\begin{aligned}
	\gamma = \bm{\xi}_{m}^{\mathsf{T}}\bm{\Sigma}^{-1}_{m}\bm{\xi}_{m} = m\bm{\xi}^{\mathsf{T}}\bm{\Sigma}^{-1}\bm{\xi}= m\gamma^0,
\end{aligned}
\]
где~$\bm{\xi}_{m}$ и~$\bm{\Sigma}_{m}$ матрицы математического ожидания и ковариации~$\textbf{s}_{m}$. Обозначим~$\bm{\xi}_1 = \bm{\xi}$, ~$\bm{\Sigma}_1 = \bm{\Sigma}$. 
	
Альтернативный метод получения~$\gamma$ включает условия на уровне значимости~$\alpha$ и вероятность ошибки II рода~$\beta$:
\[
\label{eq:sb:6}
\begin{aligned}
	\gamma^*:\chi^2_{k, 1-\alpha} = \chi^2_{k, \beta}\left(\gamma\right).
\end{aligned}
\]
Используя \eqref{eq:sb:5} и \eqref{eq:sb:6} получаем
\[
\label{eq:sb:7}
\begin{aligned}
	m^* = \frac{\gamma^*}{\gamma^0}.
\end{aligned}
\]
Это достаточный минимальный размер выборки, чтобы различить вектор~$\textbf{m}_{u}$ от~$\textbf{m}^0_{u}$.

Пусть правдоподобие выборки задается выражением
\[
\label{eq:sb:8}
\begin{aligned}
	p(y|\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}) = \exp\left(\frac{y\theta- b(\theta)}{a(\phi)} + c\left(y, \phi\right)\right),
\end{aligned}
\]
где~$\theta$ является параметром распределения, который вычисляются с помощью функции связи~$\theta=\theta\bigr(\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}\bigr)$.

Проверяемая гипотеза
\[
\label{eq:sb:9}
\begin{aligned}
	H_0: \textbf{m}_{u} = \textbf{m}^0_{u}, \quad H_1: \textbf{m}_{u} \not= \textbf{m}^0_{u}.
\end{aligned}
\]
Введем логарифм статистики отношения правдоподобий:
\[
\label{eq:sb:10}
\begin{aligned}
	LR = 2\Big(l\left(\mathfrak{D}, \hat{\textbf{w}}\right) - l\left(\mathfrak{D}, \hat{\textbf{w}}^0\right)\Big),
\end{aligned}
\]
где~$\hat{\textbf{w}} = [\hat{\textbf{w}}_{u},\hat{\textbf{w}}_{v}]$ является вектором, который максимизирует правдоподобие \eqref{eq:sb:8},~$\hat{\textbf{w}}^{0} = [\textbf{m}^{0}_{u},\hat{\textbf{w}}^{0}_{v}]$ является вектором, который максимизирует правдоподобие \eqref{eq:sb:8} с фиксируемым подвектором параметров~$\textbf{m}^{0}_{u}$.

В случае истинности гипотезы~$H_0$ статистика~$LR$ асимптотически имеет распределения~$\chi^2(k)$.  В~\cite{shieh2000} показано, что при альтернативной гипотезе~$H_1$ статистика ~$LR$ асимптотически имеет распределения~$\chi^2(k,\gamma)$, где~$\gamma$ является параметром нецентральности
\[
\label{eq:sb:11}
\begin{aligned}
	\gamma = m\Delta^*, \quad \Delta^* = \mathsf{E}\left[2a^{-1}(\phi)\left\{\left(\theta - \theta^*\right)\nabla b(\theta) - b(\theta) + b(\theta^*)\right\}\right], 
\end{aligned}
\]
где параметры~$\theta$ и~$\theta^*$ рассчитываются с использованием параметров~$\textbf{w} = [\textbf{w}_{u}, \textbf{w}_{v}]$ и~$\textbf{w}^* = [\textbf{w}^{0}_{u}, \textbf{w}^{*}_{v}]$. Параметры~$\textbf{w}^{*}_{v}$ вычисляются на основе решения уравнения
\[
\label{eq:sb:12}
\begin{aligned}
	\lim_{m\to\infty}m^{-1}\mathsf{E}\left(\frac{\partial l\left(\mathfrak{D}, \left[\textbf{m}^{0}_{u}, \textbf{w}_{v}\right]\right)}{\partial \textbf{w}_{v}}\right) = 0.
\end{aligned}
\]
	
Тогда с учетом~$\alpha$ и~$\beta$ достаточный размер выборки~$m^*$ вычисляется
\[
\label{eq:sb:13}
\begin{aligned}
	m^* = \frac{\gamma^*}{\Delta^*}, \quad \gamma^*:\chi^2_{k, 1-\alpha} = \chi^2_{k, \beta}\left(\gamma\right), 
\end{aligned}
\]
где~$\chi^2_{k, 1-\alpha}$,~$\chi^2_{k, \beta}\left(\gamma^*\right)$ квантили распределений~$\chi^{2}_k$ and~$\chi^2_{k}\left(\gamma^*\right)$.
Правдоподобие выборки:
\[
\label{eq:sb:14}
\begin{aligned}
	p(y|\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}) = \exp\left(\frac{y\theta- b(\theta)}{a(\phi)} + c\left(y, \phi\right)\right),
\end{aligned}
\]
где~$\theta$ является параметром распределения, который вычисляются с помощью функции связи~$\theta=\theta\bigr(\textbf{u},\textbf{v},\textbf{w}_{u},\textbf{w}_{v}\bigr)$.

Тестируемая гипотеза:
\[
\label{eq:sb:15}
\begin{aligned}
	H_0: \textbf{m}_{u} = \textbf{m}_{u}^{0}, \quad H_1: \textbf{m}_{u} \not=\textbf{m}_{u}^{0}.
\end{aligned}
\]
Тест Вальда для гипотезы:
\[
\label{eq:sb:16}
\begin{aligned}
	W = \left(\hat{\textbf{w}}_{u} - \textbf{m}_{u}^{0}\right)^{\mathsf{T}}\hat{\textbf{V}}_{u}^{-1}\left(\hat{\textbf{w}}_{u} - \textbf{m}_{u}^{0}\right),
\end{aligned}
\]
где~$\hat{\textbf{w}} = [\hat{\textbf{w}}_{u},\hat{\textbf{w}}_{v}]$ вектор параметров, который максимизирует правдоподобие выборки \eqref{eq:sb:14}, где матрица~$\hat{\textbf{V}}_u$ задается в выражении \eqref{eq:ps:6}.

В случае истинности гипотезы~$H_0$ статистика Вальда~$W$ асимптотически имеет распределение~$\chi^2$. В~\cite{shieh2005} показано, что в случае истинности альтернативной гипотезы~$H_1$ статистика Вальда~$W$ асимптотически имеет распределение~$\chi^2(k,\gamma)$ с параметром нецентральности~$\gamma$:
\[
\label{eq:sb:17}
\begin{aligned}
	\gamma = m\delta, \quad \delta = \left(\hat{\textbf{w}}_{u} - \textbf{m}_{u}^{0}\right)^{\mathsf{T}}\bm{\Sigma}^{-1}_u\left(\hat{\textbf{w}}_{u} - \textbf{m}_{u}^{0}\right), \quad \bm{\Sigma}_u = m\hat{\textbf{V}}_u.
\end{aligned}
\]

Использую заданный уровень значимости~$\alpha$ и заданную ошибку второго рода~$\beta$ определим оптимальный размер выборки:
\[
\label{eq:sb:18}
\begin{aligned}
	m^* = \frac{\gamma^*}{\delta}, \quad \gamma^*:\chi^2_{k, 1-\alpha^{*}} = \chi^2_{k, \beta}\left(\gamma\right),
\end{aligned}
\]
где~$\chi^2_{k, 1-\alpha^*}$,~$\chi^2_{k, \beta}\left(\gamma^*\right)$ квантили распределения, а параметр~$\alpha^*$ это поправка на уровень значимости:
\[
\label{eq:sb:19}
\begin{aligned}
	\alpha^* = P\left(\bm{\xi}^{\mathsf{T}}\bm{\Sigma}^{*-1} \bm{\xi} > \chi^2_{k,1 - \alpha}\right), \quad \Sigma^* = \textbf{I}^{-1}\left(\mathfrak{D}, \textbf{w}^*\right),
\end{aligned}
\]
где~$\textbf{w}^{*} = \left[\textbf{m}_{u}^{0}, \textbf{w}^{*}_v\right]$ является решением уравнения
\[
\label{eq:sb:20}
\begin{aligned}
	\lim_{m\to\infty}m^{-1}\mathsf{E}\left(\frac{\partial l\left(\mathfrak{D}, \left[ \textbf{m}_{u}^{0}, \textbf{w}_{v}\right]\right)}{\partial \textbf{w}_{v}}\right) = 0.
\end{aligned}
\]

\section{Эвристические методы определения достаточного размера выборки}
В методе, основанном на эвристике, используются популярные статистические эвристики, такие как бутстрап, перекрестная проверка и задание функции полезности.
Введем набор индексов~$\mathcal {A}~$ для параметров логистической регрессии~$\textbf {w}~$. Тестируется гипотеза
\[
\label{eq:hb:1}
\begin{aligned}
	H_0: j \not\in\mathcal{A} \left(\text{w}_{j} = 0\right), \quad H_1: j \in \mathcal{A}^* \left(\text{w}_{j} \not= 0\right),
\end{aligned}
\]
где~$\text{w}_{j}$ является~$j$-м элементом вектора~$\textbf{w}$.
Установим параметр отступа~$ c_0~$ для задачи логистической регрессии:
\[
\label{eq:hb:2}
\begin{aligned}
	H_0: 1-c_0 = p_0, \quad H_1: 1-c_0 = p_1,
\end{aligned}
\]
где~$c_0$ оптимальное решение, когда исключен~$j$-й элемент вектора.
Используя статистику
\[
\label{eq:hb:3}
\begin{aligned}
	Z = \frac{\hat{p}-p_0}{\sqrt{p_0(1-p_0)}}\sqrt{m}, \quad \hat{p} = \frac{1}{m}\sum_{i=1}^{m}y_i.
\end{aligned}
\]
В случае истинности нулевой гипотезы~$H_0$ статистика~$Z$ асимптотически имеет распределение~$\mathcal{N}\left(0, 1\right)$. В случае истинности альтернативной гипотезы~$H_1$ статистика~$Z$  асимптотически имеет распределение~$ \mathcal{N}\left(p_1-p_0, \sqrt{\frac{p_1(1-p_1)}{p_0(1-p_0)}}\right)$.
      
Достаточный объем выборки задается выражением
\[
\label{eq:hb:4}
\begin{aligned}
	m^* = \frac{p_0(1-p_0)\left(Z_{1-\alpha/2} + Z_{1-\beta}\sqrt{\frac{p_1(1-p_1)}{p_0(1-p_0)}}\right)^2}{(p_1-p_0)^2},
\end{aligned}
\]
где~$Z_{1-\alpha/2}$ и~$Z_{1-\beta}$ являются квантилями распределения~$\mathcal{N}\left(0, 1\right)$.
    
Данный метод не рассматривается далее, поскольку его можно использовать только в задаче логистической регрессии.

Рассмотрим метод на основе кроссвалидации. Определим критерий переобучения как
\[
\label{eq:hb:5}
\begin{aligned}
	RS(m) = \ln\frac{L(\mathfrak{D}_{\mathcal{L}(m)}, \hat{\textbf{w}})}{L(\mathfrak{D}_{\mathcal{T}(m)}, \hat{\textbf{w}})}, \quad \frac{|\mathcal{T}(m)|}{|\mathcal{L}(m)|} = \text{const} \leq 0.5.
\end{aligned}
\]
Заметим, что
\[
\label{eq:hb:6}
\begin{aligned}
	\lim_{m\to \infty}RS(m) \to 0.
\end{aligned}
\]

Достаточный размер выборки~$m^*$ определяется согласно условию:
\[
\label{eq:hb:7}
\begin{aligned}
	m^*: \forall m \geq m^* \mathsf{E}_{\mathfrak{D}_{m}}RS(m) \leq \varepsilon,
\end{aligned}
\]
где~$\varepsilon$ некоторый параметр, который задается экспертно.

Этот метод предполагает, что длины доверительных интервалов квантиля не превышают некоторого фиксированного значения~$l$. Для некоторого размера выборки~$m$ вычисляются квантильные доверительные интервалы~$\left (a^m_1, b^m_1\right), \left(a^m_2, b^m_2 \right), ..., \left(a^m_n, b^m_n \right)$ с уровнем значимости~$\alpha$ с использованием начальной загрузки для каждого параметра модели. Достаточный размер выборки задается выражением:
\[
\label{eq:hb:8}
\begin{aligned}
	m^*: \forall m\geq m^* \max_i\left(b^m_i - a^m_i\right) < l.
\end{aligned}
\]
    
Важно, что этот метод является покоординатным и следовательно для повышения точности прогноза требуется значительное увеличение размера выборки.

\section{Байесовские методы определения достаточного размера}
Байесовские методы оценки размера выборки основаны на ограничении некоторых характеристик модели. Для анализа эффективности определяется функция размера выборки. Увеличение этой функции интерпретируется как снижение эффективности модели. Размер выборки~$m^*$ выбирается таким, чтобы исследуемая функция принимала значения меньше некоторого порогового значения~$\varepsilon$.

Размер выборки~$m^*$ определяется условием:
\[
\label{eq:bs:1}
\begin{aligned}
	\forall m \geq m^*    \mathsf{E}_{\mathfrak{D}_m}\mathsf{D}\left[\hat{\textbf{w}}|\mathfrak{D}_m\right] \leq l.
\end{aligned}
\]
где~$l$ некоторый заданный экспертно параметр, который количественно определяет неопределенность оценки параметра.

Обозначим через~$A\left(\mathfrak{D}\right) \subset \mathbb{R}^n$ некоторый набор параметров модели~$\textbf{w}$:
\[
\label{eq:bs:2}
\begin{aligned}
	A\left(\mathfrak{D}\right) = \left\{\textbf{w}:||\textbf{w} - \hat{\textbf{w}}||\leq l\right\},
\end{aligned}
\]
где~$l$ --- некоторый фиксированный радиус шара.
Размер выборки~$m^*$ определяется критерием среднего покрытия:
\[
\label{eq:bs:3}
\begin{aligned}
	\forall m \geq m^*    \mathsf{E}_{\mathfrak{D}_m}\mathsf{P}\left\{\textbf{w} \in A\left(\mathfrak{D}_m\right)\right\} \geq 1-\alpha,
\end{aligned}
\]
где~$\alpha$ некоторый параметр заданный экспертно.

Определим функцию~$A\left(\mathfrak{D}\right)$:
\[
\label{eq:bs:4}
\begin{aligned}
	\mathsf{P}\left(A\left(\mathfrak{D}\right)\right) =  1- \alpha.
\end{aligned}
\]
Оценки критерия средней длины~$m^*$ заданный в \eqref{eq:bs:3}:
	
\[
\label{eq:bs:5}
\begin{aligned}
	\forall m \geq m^*    \mathsf{E}_{\mathfrak{D}_m}r_m\leq l,
\end{aligned}
\]
где~$r_m$ является радиусом шара~$A\left(\mathfrak{D}_{m}\right)$.

Следующие методы максимизируют ожидание некоторой функции полезности~$u\left(\mathfrak{D}, \textbf{w}\right)$ по размеру выборки:
\[
\label{eq:bs:6}
\begin{aligned}
	m^* = \arg\max_{m} \mathsf{E}_{\mathfrak{D}_m}\int_{\textbf{w}}u\left(\mathfrak{D}_m, \textbf{w}\right)p(\textbf{w}|\mathfrak{D}_m)d\textbf{w},
\end{aligned}
\]
где функция полезности~$u\left(\mathfrak{D}, \textbf{w}\right)$ задается в виде:

\[
\label{eq:bs:7}
\begin{aligned}
	u\left(\mathfrak{D}_m, \textbf{w}\right) = l\left(\mathfrak{D}_m, \textbf{w}\right) - cm,
\end{aligned}
\]
 где~$c$ функция штрафов для каждого элемента в наборе выборки.
	 
Назовем индексы~$\mathcal{B}_1,\mathcal{B}_2 \subset \{1,...,m\}$ по соседству, если
\[
\label{eq:bs:8}
\begin{aligned}
	\left|\mathcal{B}_1 \Delta \mathcal{B}_2\right| = 1.
\end{aligned}
\]
Таким образом,~$\mathcal{B}_2~$ можно преобразовать в~$\mathcal{B}_1$ путем удаления, замены или добавления одного элемента. В~\cite{motrenko2014} показано, что если размер набора выборок~$\mathfrak {D}_{\mathcal {B}_1}$ достаточно велик, чем параметры модели~$\hat{\textbf {w}}_1$, оптимизированные с помощью~$\mathfrak{D}_{\mathcal{B}_1}$, должны находиться в окрестности параметров модели~$\hat{\textbf{w}}_2~$, которые оптимизированы с помощью~$\mathfrak{D}_{\mathcal {B}_2}$.
	 
Используя дивергенцию Кульбака-Лейблера в качестве функции близости между распределениями параметров модели, оптимизированных с помощью~$\mathfrak{D}_{\mathcal{B}_1}$ и~$\mathfrak{D}_{\mathcal{B}_2}$:
\[
\label{eq:bs:9}
\begin{aligned}
	D_\text{KL}\left(p_1, p_2\right) = \int_{\textbf{w}\in\mathbb{W}}p_1(\textbf{w})\log\frac{p_1(\textbf{w})}{p_2(\textbf{w})}d\textbf{w},
\end{aligned}
\]
где~$p_1$ and~$p_2$ апостериорные вероятности вектора параметров~$\textbf{w}$ рассчитаные на подвыборках~$\mathfrak{D}_{\mathcal{B}_1}$ и~$\mathfrak{D}_{\mathcal{B}_2}$ соответсвенно. Также предполагается, что~$\mathfrak{D}_{\mathcal{B}_1}$ и~$\mathfrak{D}_{\mathcal{B}_2}$ находятся по соседству.
Достаточный размер выборки~$m^*$ оценивается:
\[
\label{eq:bs:10}
\begin{aligned}
	\forall \mathfrak{D}_{\mathcal{B}_1}: \left|\mathfrak{D}_{\mathcal{B}_1}\right| \geq m^*    \mathsf{E}_{\mathfrak{D}_{\mathcal{B}_2}}D_{KL}\left(p_1, p_2\right) \leq \varepsilon.
\end{aligned}
\]

\section{Метод определения достаточного размера выборки на основе сэмлирования эмпирической функции ошибки}

В данном разделе предполагаем, что выполняется условие~$m^* \leqslant m$. 
Это означает, что требуется определить, какой объем выборки можно считать достаточным, и что для этого есть достаточно объектов в самой выборке~$D$.
Для определения достаточности мы будем использовать функцию правдоподобия. 
Когда доступно достаточное количество объектов, естественно ожидать, что полученная оценка параметров не будет существенно изменяться от одной реализации выборки к другой~\cite{joseph1997,joseph1995}. 
Аналогичное утверждение справедливо и для функции правдоподобия. 
Таким образом, мы формализуем критерии, позволяющие определить достаточный объем выборки.
Критерий определяется в определении~\ref{sufficient-variance}.

\begin{definition}\label{sufficient-variance}
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ назовем D-достаточным, если для всех~$k\geqslant m^*$ выполняется условие:
    \[
        D(k) = \mathbb{D}_{\hat{\mathbf{w}}_{k}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \leqslant \varepsilon.
    \]
\end{definition}

С другой стороны, при наличии достаточного количества объектов вполне естественно, что при добавлении еще одного объекта к рассмотрению результирующая оценка параметра изменится незначительно, на основе данного свойства получаем определение~\ref{sufficient-difference}.

\begin{definition}\label{sufficient-difference}
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ назовем M-достаточным, если для всех~$k\geqslant m^*$ выполняется условие:
    \[
        M(k) = \left| \mathbb{E}_{\hat{\mathbf{w}}_{k+1}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E}_{\hat{\mathbf{w}}_{k}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \right| \leqslant \varepsilon.
    \]
\end{definition}

В приведенных выше определениях вместо функции правдоподобия~$L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$ мы можем рассмотреть ее логарифм~$l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$.
Предположим, что~$\mathbb{W} = \mathbb{R}^n,$ информацией Фишера задана матрицей:
\[
    \left[\mathcal{I}(\mathbf{w})\right]_{ij} = - \mathbb{E}\left[ \frac{\partial^2 \log p(\mathbf{y} | \mathbf{x}, \mathbf{w})}{\partial w_i \partial w_j} \right],
\]
заметим, что известным результатом является асимптотическая нормальность оценки максимального правдоподобия, то есть
\[
    \sqrt{k}\left(\hat{\mathbf{w}}_k -\mathbf{w}\right)\xrightarrow{d}\mathcal{N}\left(0, \mathcal{I}^{-1}(\mathbf{w})\right).
\]
Сходимость по распределению, вообще говоря, не влечет сходимости моментов случайного вектора.
Тем не менее, если предположить последнее, то в некоторых моделях можно доказать корректность нашего предложенного определения M-достаточного размера выборки.

Для удобства обозначим параметры распределения~$\hat{\mathbf{w}}_k$ следующим образом: математическое ожидание~$\mathbb{E}\hat{\mathbf{w}}_k=\mathbf{m}_k$ и матрица ковариаций~$\mathbb{D} \hat{\mathbf{w}}_k = \mathbf{\Sigma}_k$.
Тогда справедлива теорема~\ref{chapter:samplesize:theorem-kiselev-likelihood-bootstraping}, которая доказывает сходимость параметров.

\begin{theorem}\label{chapter:samplesize:theorem-kiselev-likelihood-bootstraping}
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и~$\|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_{F}\to 0$ при~$k\to \infty$. 
    Тогда в модели линейной регрессии определение M-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$M(k)\leqslant\varepsilon$.
\end{theorem}
\begin{proof}
Рассмотрим определение M-достаточного размера выборки в терминах логарифма функции правдоподобия. В модели линейной регрессии
\begin{align}
    L\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) &= p(\mathbf{y} | \mathbf{X}, \hat{\mathbf{w}}_k) = \prod_{i=1}^{m} p(y_i | \mathbf{x}_i, \hat{\mathbf{w}}_k) = \prod_{i=1}^{m} \mathcal{N}\left( y_i | \hat{\mathbf{w}}_k^{\top} \mathbf{x}_i, \sigma^2 \right) =\\
    &= \left(2\pi\sigma^2 \right)^{-m/2} \exp\left(-\frac{1}{2\sigma^2}\|\mathbf{y} -\mathbf{X} \hat{\mathbf{w}}_k\|_2^2 \right).
\end{align}
Возьмем логарифм:
\[
    l\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) = \log p(\mathbf{y} | \mathbf{X}, \hat{\mathbf{w}}_k) = -\frac{m}{2}\log\left( 2\pi\sigma^2 \right) - \frac{1}{2\sigma^2} \| \mathbf{y} - \mathbf{X} \hat{\mathbf{w}}_k \|_2^2.
\]
Возьмем математическое ожидание по~$\mathfrak{D}_k$, учитывая что~$\mathbb{E}_{\mathfrak{D}_k}\hat{\mathbf{w}}_k=\mathbf{m}_k$ и~$\text{cov}(\hat{\mathbf{w}}_k) = \mathbf{\Sigma}_k$:
\[
    \mathbb{E}_{\mathfrak{D}_k} l\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) = -\frac{m}{2}\log\left( 2\pi\sigma^2 \right) - \frac{1}{2\sigma^2} \Big( \| \mathbf{y} - \mathbf{X} \mathbf{m}_k \|_2^2 + \text{tr}\left( \mathbf{X}^{\top}\mathbf{X} \mathbf{\Sigma}_k \right) \Big).
\]
Запишем выражение для разности математических ожиданий:
\begin{align}
    &\mathbb{E}_{\mathfrak{D}_{k+1}} l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E}_{\mathfrak{D}_k} l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) = \\
    &\quad= \frac{1}{2\sigma^2} \Big( \| \mathbf{y} - \mathbf{X} \mathbf{m}_k \|_2^2 - \| \mathbf{y} - \mathbf{X} \mathbf{m}_{k+1} \|_2^2 \Big) + \frac{1}{2\sigma^2} \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \Big) \right) = \\
    &\quad= \frac{1}{2\sigma^2} \Big( 2 \mathbf{y}^{\top} \mathbf{X} (\mathbf{m}_{k+1} - \mathbf{m}_k) + (\mathbf{m}_k - \mathbf{m}_{k+1})^{\top} \mathbf{X}^{\top}\mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \Big) + \\
    &\qquad+ \frac{1}{2\sigma^2} \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \right) \Big).
\end{align}
Значение функции~$M(k)$ представляет собой модуль от приведенного выше выражения. Применим неравенство треугольника для модуля, а затем оценим каждое слагаемое.\\
Оценим первое слагаемое, используя неравенство Коши-Буняковского:
\[
    \big| \mathbf{y}^{\top}\mathbf{X}(\mathbf{m}_{k+1}-\mathbf{m}_k)\big| \leqslant \| \mathbf{X}^{\top}\mathbf{y} \|_2 \|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2.
\]
Второе слагаемое оцениваем с помощью неравенства Коши-Буняковского, свойства согласованности спектральной нормы матрицы, а также ограниченности последовательности векторов~$\mathbf{m}_k$, что следует из приведенного условия сходимости:
\begin{align}
    \big| (\mathbf{m}_k - \mathbf{m}_{k+1})^{\top} \mathbf{X}^{\top}\mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \big| &\leqslant \| \mathbf{X} (\mathbf{m}_k - \mathbf{m}_{k+1}) \|_2 \| \mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \|_2 \leqslant \\
    &\leqslant \| \mathbf{X} \|_2^2 \| \mathbf{m}_k - \mathbf{m}_{k+1} \|_2 \| \mathbf{m}_k + \mathbf{m}_{k+1} \|_2 \leqslant \\
    &\leqslant C \| \mathbf{X} \|_2^2 \| \mathbf{m}_k - \mathbf{m}_{k+1} \|_2.
\end{align}
Последнее слагаемое оцениваем, используя неравенство Гельдера для нормы Фробениуса:
\[
    \Big| \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \right) \Big) \Big| \leqslant \| \mathbf{X}^{\top}\mathbf{X} \|_F \| \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \|_F.
\]
Наконец, поскольку~$\|\mathbf{m}_k - \mathbf{m}_{k+1} \|_2\to 0$ и~$\|\mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1}\|_{F}\to 0$ при~$k\to\infty$, то~$M(k)\to 0$ при~$k\to \infty$, что и доказывает теорему.
\end{proof}

\begin{corollary}\label{chapter:samplesize:corollary-kiselev-likelihood-bootstraping}
    Пусть~$\|\mathbf{m}_k - \mathbf{w}\|_2\to 0$ и~$\|\mathbf{\Sigma}_k - \left[k\mathcal{I}(\mathbf{w})\right]^{-1}\|_{F}\to 0$ при~$k \to \infty$. 
    
    Тогда в модели линейной регрессии определение M-достаточного размера выборки корректно.
\end{corollary}

По условию, задана только одна выборка, а следовательно, в эксперименте невозможно вычислить математическое ожидание и дисперсию, указанные в определениях.
Поэтому для их оценки используется метод бутстрэпирования, то есть сгенерируем из заданной~$\mathfrak{D}_m$ некоторое число~$B$ подвыборок размера~$k$ с возвращением.
Для каждой из них получим оценку параметров~$\hat{\mathbf{w}}_{k}$ и вычисляем значение~$L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$.
Для оценки  используется выборочное среднее и несмещенную выборочная дисперсия.

Предложенные выше определения также могут быть применены в тех задачах, где минимизируется произвольная функция потерь, а не максимизируется функция правдоподобия.
Мы не приводим какого-либо теоретического обоснования для этого, но на практике такая эвристика оказывается вполне успешной.

\section{Метод определения достаточного размера выборки на основе близости апостериорных распределений}

В работе~\cite{motrenko2014} предлагается использовать расхождение Кульбака-Лейблера для оценки достаточного размера выборки в задаче бинарной классификации.
Идея основана на том, что если две подвыборки отличаются друг от друга одним объектом, то полученные по ним апостериорные распределения должны быть близки. Эта близость определяется расхождением Кульбака-Лейблера.

\begin{figure}[h!t]\center
    \includesvg[width=0.7\textwidth]{figures/chapter-2/posterior_ru}
    \caption{Пример сдвига распределений при добавления объектов}
    \label{fig-chapter-2-posterior-ru}
\end{figure}

In this paper, the question of the correctness of this approach is considered. The method is studied in an arbitrary probabilistic model. As a measure of proximity, it is proposed to use not only the Kullback-Leibler divergence, but also the s-score similarity function from~\cite{aduenko2017}.

В рамках данного раздела предлагается использовать не только расхождение Кульбака-Лейблера, но и функцию схожести s-score из работы~\cite{aduenko2017}, для этого рассмотрим две подвыборки~$\mathfrak{D}^1\subseteq\mathfrak{D}_m$ и~$\mathfrak{D}^2\subseteq\mathfrak{D}_m$.
Пусть~$\mathcal{I}_1 \subseteq \mathcal{I} = \{1, \ldots, m\}$ и~$\mathcal{I}_2 \subseteq \mathcal{I} =\{1, \ldots,m\}$~--- соответствующие им подмножества индексов.

\begin{definition}
    Подвыборки~$\mathfrak{D}^1$ и~$\mathfrak{D}^2$ назовем \textbf{близкими}, если~$\mathcal{I}_2$ может быть получено из~$\mathcal{I}_1$ путем удаления, замены или добавления одного элемента, то есть
    \[
        \left| \mathcal{I}_1 \triangle \mathcal{I}_2 \right| = \left| \left( \mathcal{I}_1 \setminus \mathcal{I}_2 \right) \cup \left( \mathcal{I}_2 \setminus \mathcal{I}_1 \right) \right| = 1.
    \]
\end{definition}

Consider two similar subsamples~$\mathfrak{D}_k = (\mathbf{X}_k,\mathbf{y}_k)$ and~$\mathfrak{D}_{k+1} = (\mathbf{X}_{k+1}, \mathbf{y}_{k+1})$ of sizes~$k$ and~$k+1$, respectively. This means that the larger one is obtained by adding one element to the smaller one. Let's find the posterior distribution of the model parameters over these subsamples:
\[
    p_j(\mathbf{w}) = p(\mathbf{w} | \mathfrak{D}_j) = \frac{p(\mathfrak{D}_j | \mathbf{w}) p(\mathbf{w})}{p(\mathfrak{D}_j)} \propto p(\mathfrak{D}_j | \mathbf{w}) p(\mathbf{w}), \quad j = k, k+1.
\]

Рассмотрим две похожие подвыборки~$\mathfrak{D}_k = (\mathbf{X}_k,\mathbf{y}_k)$ и~$\mathfrak{D}_{k+1} = (\mathbf{X}_{k+1}, \mathbf{y}_{k+1})$ размеров~$k$ и~$k+1$ соответственно, то есть выборка~$\mathfrak{D}_{k+1}~$ получена путём добавления одного элемента к выборке~$\mathfrak{D}_k$.
Найдем апостериорное распределение параметров модели по этим подвыборкам:
\[
    p_j(\mathbf{w}) = p(\mathbf{w} | \mathfrak{D}_j) = \frac{p(\mathfrak{D}_j | \mathbf{w}) p(\mathbf{w})}{p(\mathfrak{D}_j)} \propto p(\mathfrak{D}_j | \mathbf{w}) p(\mathbf{w}), \quad j = k, k+1.
\]

\begin{definition}
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ называется \textbf{KL-достаточным}, если для всех~$k\geqslant m^*$
    \[
        KL(k) = D_{KL}(p_k \| p_{k+1}) = \int p_k(\mathbf{w}) \log{\frac{p_k(\mathbf{w})}{p_{k+1}(\mathbf{w})}} d\mathbf{w} \leqslant \varepsilon.
    \]
\end{definition}

For a pair of normal distributions, the Kullback-Leibler divergence has a fairly simple form.
Assume that the posterior distribution is normal, that is,~$p_k(\mathbf{w}) = \mathcal{N}\left(\mathbf{w}|\mathbf{m}_k, \mathbf{\Sigma}_k\right)$.
Guided by the heuristic that the convergence of the moments of such a distribution should entail the proximity of posterior distributions on similar subsamples, the following statement can be formulated.

Для пары нормальных распределений расхождение Кульбака-Лейблера имеет достаточно простой вид, а следовательно предположив, что апостериорное распределение является нормальным, то есть~$p_k(\mathbf{w}) = \mathcal{N}\left(\mathbf{w}|\mathbf{m}_k, \mathbf{\Sigma}_k\right)$ получаем формулировку теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity}

\begin{theorem}\label{chapter:samplesize:theorem-kiselev-posterior-similiarity}
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и~$\|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_{F}\to 0$ при~$k\to \infty$.
    
    Тогда в модели с нормальным апостериорным распределением параметров определение KL-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$KL(k)\leqslant\varepsilon$.
\end{theorem}
\begin{proof}
Рассмотрим выражение для расхождения Кульбака-Лейблера между двумя нормальными апостериорными распределениями~$p_k = \mathcal{N}(\mathbf{m}_k, \mathbf{\Sigma}_k)$ и~$p_{k+1} = \mathcal{N}(\mathbf{m}_{k+1}, \mathbf{\Sigma}_{k+1})$.
Для двух многомерных нормальных распределений данная метрика имеет аналитическое выражение:
\begin{align}
    &D_{\text{KL}}\left( p_k \| p_{k+1} \right) =\\
    &=\frac{1}{2} \left( \mathrm{tr}\left( \mathbf{\Sigma}_{k+1}^{-1} \mathbf{\Sigma}_k \right) + (\mathbf{m}_{k+1} - \mathbf{m}_k)^{\top} \mathbf{\Sigma}_{k+1}^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) - n + \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} \right).
\end{align}
Для анализа поведения каждого слагаемого при~$k \to \infty$ введем обозначение для разности ковариационных матриц:~$\mathbf{\Sigma}_{k+1} = \mathbf{\Sigma}_k + \Delta\mathbf{\Sigma}$, где по условию теоремы~$\|\Delta \mathbf{\Sigma}\|_F = \|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_F \to 0$. Первое слагаемое представляет собой след произведения матриц:
\begin{align}
    \mathrm{tr}\left( \mathbf{\Sigma}_{k+1}^{-1} \mathbf{\Sigma}_k \right) &= \mathrm{tr}\left(\left(\mathbf{\Sigma}_k + \Delta \mathbf{\Sigma} \right)^{-1} \mathbf{\Sigma}_k \right).
\end{align}
Используя разложение в ряд для обратной матрицы при малых~$\Delta\mathbf{\Sigma}$, получаем:
\[
    (\mathbf{\Sigma}_k + \Delta \mathbf{\Sigma})^{-1} = \mathbf{\Sigma}_k^{-1} - \mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma} \mathbf{\Sigma}_k^{-1} + O(\|\Delta \mathbf{\Sigma}\|_F^2).
\]
Тогда:
\[
    (\mathbf{\Sigma}_k + \Delta \mathbf{\Sigma})^{-1} \mathbf{\Sigma}_k = \mathbf{I}_n - \mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma} + O(\|\Delta \mathbf{\Sigma}\|_F^2).
\]
Взяв след от этого выражения и учитывая, что~$\mathrm{tr}(\mathbf{I}_n) = n$, а~$\|\mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}\|_F \to 0$ при~$\|\Delta \mathbf{\Sigma}\|_F \to 0$, получаем:
\[
    \mathrm{tr}\left( \mathbf{\Sigma}_{k+1}^{-1} \mathbf{\Sigma}_k \right) \to n \quad \text{при} \quad \| \Delta \mathbf{\Sigma} \|_F \to 0.
\]
Второе слагаемое представляет собой квадратичную форму:
\begin{align}
    (\mathbf{m}_{k+1} - \mathbf{m}_k)^{\top} \mathbf{\Sigma}_{k+1}^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k).
\end{align}
По условию теоремы~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$. Квадратичная форма оценивается сверху следующим образом:
\[
    \left| (\mathbf{m}_{k+1} - \mathbf{m}_k)^{\top} \mathbf{\Sigma}_{k+1}^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) \right| \leqslant \| \mathbf{m}_{k+1} -\mathbf{m}_k\|_2^2 \cdot \|\mathbf{\Sigma}_{k+1}^{-1} \|_2.
\]
Поскольку ковариационная матрица~$\mathbf{\Sigma}_{k+1}$ является положительно определенной и сходится к некоторой предельной матрице, ее спектральная норма~$\|\mathbf{\Sigma}_{k+1}^{-1} \|_2$ ограничена. Следовательно, при~$\| \mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ данное слагаемое стремится к нулю.
Третье и четвертое слагаемые составляют:
\[
    -n + \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} = \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} - n.
\]
Преобразуем отношение определителей:
\[
    \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} = \frac{\det (\mathbf{\Sigma}_k + \Delta \mathbf{\Sigma})}{\det \mathbf{\Sigma}_{k}} = \det (\mathbf{I}_n + \mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}).
\]
Для малых~$\Delta \mathbf{\Sigma}$ используем приближение:
\[
    \det (\mathbf{I}_n + \mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}) = 1 + \mathrm{tr}(\mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}) + O(\|\Delta \mathbf{\Sigma}\|_F^2).
\]
Тогда:
\[
    \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} = \log \det (\mathbf{I}_n + \mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}) = \mathrm{tr}(\mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}) + O(\|\Delta \mathbf{\Sigma}\|_F^2).
\]
Поскольку~$\|\Delta \mathbf{\Sigma}\|_F \to 0$, то~$\mathrm{tr}(\mathbf{\Sigma}_k^{-1} \Delta \mathbf{\Sigma}) \to 0$, и следовательно:
\[
    \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} \to 0.
\]

Таким образом, все четыре слагаемых в выражении для~$D_{\text{KL}}$ сходятся к своим пределам: первое слагаемое стремится к~$n$, второе~--- к 0, третье равно~$-n$, четвертое~--- к 0.
Сумма этих пределов равна~$n + 0 - n + 0 = 0$, что доказывает, что~$D_{\text{KL}}(p_k \| p_{k+1}) \to 0$ при~$k \to \infty$.
Следовательно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k \geqslant m^*$ выполняется~$KL(k) \leqslant \varepsilon$, что и требовалось доказать.
\end{proof}

Теорема~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity} implies что расстояние между двумя гауссовскими распределениями стремится к нулю по мере сходимости их векторов математических ожиданий и ковариационных матриц.
Это позволяет нам глубже изучить вопрос сходимости расхождения Кульбака-Лейблера, анализируя аналитические выражения для математических ожиданий и дисперсий.

Рассмотрим функцию схожести s-score из работы~\cite{aduenko2017} в качестве меры близости распределений по аналогии, как это было с KL-дивергенцией:
\[
    \text{s-score}(g_1, g_2) = \frac{\int_{\mathbf{w}} g_1(\mathbf{w}) g_2(\mathbf{w}) d\mathbf{w}}{\max_{\mathbf{b}} \int_{\mathbf{w}} g_1(\mathbf{w} - \mathbf{b}) g_2(\mathbf{w}) d\mathbf{w}}.
\]
\begin{definition}
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ называется \textbf{S-достаточным}, если для всех~$k\geqslant m^*$
    \[
        S(k) = \text{s-score}(p_k, p_{k+1}) \geqslant 1-\varepsilon.
    \]
\end{definition}
Как и в случае с KL-достаточным размером выборки, в модели с нормальным апостериорным распределением можно записать выражение для используемого критерия, который записан в виде теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-s-score}

\begin{theorem}\label{chapter:samplesize:theorem-kiselev-posterior-similiarity-s-score}
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2\to 0$ при~$k \to \infty$.
    
    Тогда в модели с нормальным апостериорным распределением параметров определение S-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$S(k)\geqslant 1-\varepsilon$.
\end{theorem}
\begin{proof}
Let's use the s-score expression for a pair of normal posterior distributions from~\cite{aduenko2017}:
\[
    \text{s-score}(p_k, p_{k+1}) = \exp{\left( -\frac{1}{2} (\mathbf{m}_{k+1} - \mathbf{m}_k)\top \left( \mathbf{\Sigma}_k + \mathbf{\Sigma}_{k+1} \right)^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) \right)}.
\]
Because
\[
    \left| (\mathbf{m}_{k+1} - \mathbf{m}_k) \left( \mathbf{\Sigma}_k + \mathbf{\Sigma}_{k+1} \right)^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) \right| \leqslant \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2^2 \| \left( \mathbf{\Sigma}_k + \mathbf{\Sigma}_{k+1} \right)^{-1} \|_2 \to 0
\]
if~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2\to 0$, then the value of the quadratic form inside the exponent tends to zero.
Therefore,~$\text{s-score}(p_k, p_{k+1}) \to 1$ as~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2\to 0$.
\end{proof}

Значимость теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-s-score} аналогична значимости теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity}.
По существу, близость нормальных распределений в терминах функции схожести s-score сводится к сходимости их математических ожиданий.
Примечательно, что в отличие от теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity}, сходимость ковариационных матриц здесь не требуется.

Пусть в модели линейной регрессии задано нормальное априорное распределение параметров. В силу свойства сопряженности априорного распределения и правдоподобия, апостериорное распределение также будет нормальным.
Таким образом, мы приходим к одному из простейших примеров модели, для которой справедливы приведенные выше теоремы.
Фактически, для линейной регрессии могут быть сформулированы более простые утверждения.

\begin{theorem}\label{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression}
    Пусть множества значений признаков и целевой переменной ограничены, то есть существует~$M\in \mathbb{R}$ такое, что~$\|\mathbf{x}\|_2\leqslant M$ и~$|y|\leqslant M$.
    Если~$\lambda_{\min}\left(\mathbf{X}^{\top}_k \mathbf{X}_k \right) = \omega(\sqrt{k})$ при~$k\to \infty$, то в модели линейной регрессии с нормальным априорным распределением параметров~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2\to 0$ и~$\|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_{F}\to 0$ при~$k\to \infty$.
\end{theorem}
\begin{proof}
Рассмотрим линейную регрессионную модель с нормальным априорным распределением параметров:~$p(\mathbf{w})=\mathcal{N}\left(\mathbf{w}|\mathbf{0}, \alpha^{-1}\mathbf{I}\right)$.
Данное априорное распределение является сопряженным для нормального правдоподобия, что существенно упрощает анализ.
Нормальное правдоподобие задается в виде:
\[
    p(\mathbf{y} | \mathbf{X}, \mathbf{w}) = \mathcal{N}\left(\mathbf{y} |\mathbf{X}\mathbf{w}, \sigma^2\mathbf{I}\right) =\left( 2\pi\sigma^2\right)^{-m/2} \exp\left( -\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|_2^2\right).
\]

Благодаря свойству сопряженности нормального априорного распределения и нормального правдоподобия, апостериорное распределение также является нормальным:
\[
    p(\mathbf{w} | \mathbf{X}, \mathbf{y}) = \mathcal{N}\left(\mathbf{w} | \mathbf{m}, \mathbf{\Sigma} \right),
\]
где параметры распределения имеют аналитическое выражение:
\begin{align}
    \mathbf{\Sigma} &= \left( \alpha \mathbf{I} + \frac{1}{\sigma^2} \mathbf{X}^{\top} \mathbf{X} \right)^{-1}, \\
    \mathbf{m} &= \frac{1}{\sigma^2} \mathbf{\Sigma} \mathbf{X}^{\top} \mathbf{y} = \left( \mathbf{X}^{\top} \mathbf{X} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}^{\top} \mathbf{y}.
\end{align}

Перейдем к анализу сходимости ковариационных матриц.
Рассмотрим подвыборки размера~$k$ и~$k+1$, полученные из исходных данных. Нас интересует поведение разности~$\|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_2$ при~$k \to \infty$.
Введем обозначение~$\mathbf{A}_k = \frac{1}{\sigma^2}\mathbf{X}_k^{\top}\mathbf{X}_k$ для нормированной матрицы ковариации признаков. Тогда разность обратных матриц можно преобразовать, используя матричное тождество:
\begin{align}
    \| \mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k \|_2 &= \left\| \left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)^{-1} - \left( \alpha \mathbf{I} + \mathbf{A}_k \right)^{-1} \right\|_2.
\end{align}
Применяя матричное тождество для разности обратных матриц, получаем:
\begin{align}
    \left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)^{-1} - \left( \alpha \mathbf{I} + \mathbf{A}_k \right)^{-1} = 
    \left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)^{-1} \left( \mathbf{A}_k - \mathbf{A}_{k+1} \right) \left( \alpha \mathbf{I} + \mathbf{A}_k \right)^{-1}.
\end{align}
Используя субмультипликативное свойство спектральной нормы, оцениваем:
\begin{align}
    \| \mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k \|_2 &\leqslant \left\| \left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)^{-1} \right\|_2 \left\| \left( \alpha \mathbf{I} + \mathbf{A}_k \right)^{-1} \right\|_2 \left\| \mathbf{A}_{k+1} - \mathbf{A}_k \right\|_2.
\end{align}
Теперь проанализируем каждый из множителей.
Спектральная норма обратной матрицы выражается через минимальное собственное значение:
\[
    \left\| \left( \alpha \mathbf{I} + \mathbf{A} \right)^{-1} \right\|_2 = \frac{1}{\lambda_{\min}(\alpha \mathbf{I} + \mathbf{A})}.
\]
Поскольку~$\alpha > 0$ и матрица~$\mathbf{A}$ положительно полуопределена, имеем~$\lambda_{\min}(\alpha \mathbf{I} + \mathbf{A}) \geq \alpha + \lambda_{\min}(\mathbf{A})$.
Однако для получения точной асимптотики мы используем более слабую оценку:
\[
\left\| \left( \alpha \mathbf{I} + \mathbf{A} \right)^{-1} \right\|_2 \leq \frac{1}{\lambda_{\min}(\mathbf{A})}.
\]
Таким образом, получаем цепочку неравенств:
\begin{align}
    \| \mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k \|_2 &\leqslant \frac{1}{\lambda_{\min}\left( \mathbf{A}_{k+1} \right)} \frac{1}{\lambda_{\min}\left( \mathbf{A}_k \right)} \left\| \mathbf{A}_{k+1} - \mathbf{A}_k \right\|_2 = \\
    &= \sigma^2  \frac{1}{\lambda_{\min}\left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} \right)} \frac{1}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)} \left\| \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} - \mathbf{X}_k^{\top} \mathbf{X}_k \right\|_2.
\end{align}
Поскольку выборка~$\mathfrak{D}_{k+1}$ получается из~$\mathfrak{D}_k$ добавлением одного наблюдения, имеем:
\begin{align}
    \left\| \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} - \mathbf{X}_k^{\top} \mathbf{X}_k \right\|_2 &= \left\| \sum\limits_{i=1}^{k+1} \mathbf{x}_i \mathbf{x}_i^{\top} - \sum\limits_{i=1}^{k} \mathbf{x}_i \mathbf{x}_i^{\top} \right\|_2 = \left\| \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right\|_2.
\end{align}
Матрица~$\mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top}$ имеет единичный ранг, и ее спектральная норма равна квадрату евклидовой нормы вектора~$\mathbf{x}_{k+1}$:
\[
    \left\| \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right\|_2 = \lambda_{\max}\left( \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right) = \| \mathbf{x}_{k+1}\|_2^2.
\]
Из условия ограниченности признаков следует~$\| \mathbf{x}_{k+1}\|_2^2 \leqslant M^2$. Таким образом:
\[
    \left\| \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} - \mathbf{X}_k^{\top} \mathbf{X}_k \right\|_2 \leqslant M^2.
\]

Теперь рассмотрим условие на минимальное собственное значение.
Из предположения~$\lambda_{\min}\left(\mathbf{X}_k^{\top}\mathbf{X}_k \right) = \omega(\sqrt{k})$ следует, что:
\[
    \frac{1}{\lambda_{\min}\left(\mathbf{X}_k^{\top}\mathbf{X}_k \right)} = o\left(\frac{1}{\sqrt{k}}\right).
\]
Комбинируя полученные оценки, приходим к:
\[
    \|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_2 \leqslant \sigma^2 M^2 \cdot o\left(\frac{1}{\sqrt{k}}\right) \cdot o\left(\frac{1}{\sqrt{k}}\right) = o\left(\frac{1}{k}\right).
\]
Для перехода к норме Фробениуса воспользуемся неравенством~$\| \mathbf{A} \|_F \leqslant \sqrt{n} \| \mathbf{A} \|_2$, где~$n$~--- размерность пространства параметров:
\[
    \|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k \|_F \leqslant \sqrt{n} \|\mathbf{\Sigma}_{k+1} - \mathbf{\Sigma}_k\|_2 = \sqrt{n} \cdot o\left(\frac{1}{k}\right) = o\left(\frac{1}{k}\right).
\]
Теперь перейдем к анализу сходимости математических ожиданий.
Требуется оценить:
\[
    \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 = \left\| \left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_{k+1}^{\top} \mathbf{y}_{k+1} - \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2.
\]
Представим расширенную матрицу признаков и вектор ответов через предыдущие значения:
\begin{align}
    \mathbf{X}_{k+1} &= \begin{bmatrix} \mathbf{X}_k \\ \mathbf{x}_{k+1}^{\top} \end{bmatrix}, &
    \mathbf{y}_{k+1} &= \begin{bmatrix} \mathbf{y}_k \\ y_{k+1} \end{bmatrix}.
\end{align}
Тогда матричные произведения принимают вид:
\begin{align}
    \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} &= \mathbf{X}_k^{\top} \mathbf{X}_k + \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top}, \\
    \mathbf{X}_{k+1}^{\top} \mathbf{y}_{k+1} &= \mathbf{X}_k^{\top} \mathbf{y}_k + \mathbf{x}_{k+1} y_{k+1}.
\end{align}
Подставляя эти выражения, получаем:
\begin{align}
    \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 &= \Bigg\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} + \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} \left( \mathbf{X}_k^{\top} \mathbf{y}_k + \mathbf{x}_{k+1} y_{k+1} \right) \\
    &\quad - \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_k^{\top} \mathbf{y}_k \Bigg\|_2.
\end{align}
Для упрощения первого слагаемого применим лемму о матричном обращении:
\begin{align}
    &\left(\mathbf{X}_k^{\top} \mathbf{X}_k + \alpha\sigma^2 \mathbf{I} + \mathbf{x}_{k+1}\mathbf{x}_{k+1}^{\top}\right)^{-1} = \\
    &\quad = \left(\mathbf{X}_k^{\top} \mathbf{X}_k + \alpha\sigma^2 \mathbf{I}\right)^{-1} - \frac{\left(\mathbf{X}_k^{\top} \mathbf{X}_k + \alpha\sigma^2 \mathbf{I}\right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \left(\mathbf{X}_k^{\top} \mathbf{X}_k + \alpha\sigma^2 \mathbf{I}\right)^{-1}}{1 + \mathbf{x}_{k+1}^{\top} \left(\mathbf{X}_k^{\top} \mathbf{X}_k + \alpha\sigma^2 \mathbf{I}\right)^{-1} \mathbf{x}_{k+1}}.
\end{align}
После алгебраических преобразований получаем:
\begin{align}
    &\| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 = \\
    &\quad = \Bigg\| \left[ \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} - \mathbf{I} \right] \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_k^{\top} \mathbf{y}_k \\
    &\qquad + \left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} y_{k+1} \Bigg\|_2.
\end{align}
Применяя неравенство треугольника и свойства норм, оцениваем каждый член отдельно:
\begin{align}
    &\| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 \leqslant \\
    &\leqslant \left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} - \mathbf{I} \right\|_2 \left\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2 \\
    &\quad + \left\| \left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{x}_{k+1} y_{k+1} \right\|_2.
\end{align}

Проанализируем первый множитель в первом слагаемом.
Используя матричное тождество, получаем:
\begin{align}
    &\left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} - \mathbf{I} \right\|_2 = \\
    &\quad = \left\| - \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right\|_2.
\end{align}
Применяя субмультипликативность нормы и учитывая, что спектральная норма произведения матриц не превышает произведения их норм, получаем оценку:
\begin{align}
    &\left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} - \mathbf{I} \right\|_2 \leqslant \\
    &\quad \leqslant \left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} \right\|_2 \left\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right\|_2.
\end{align}

Оценим каждый из этих множителей.
Для первого множителя используем тот факт, что матрица~$\left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top}$ имеет единичный ранг, и ее максимальное собственное значение равно:
\[
    \lambda_{\max}\left( \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right) = \mathbf{x}_{k+1}^{\top} \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1}.
\]
Тогда спектральная норма обратной матрицы оценивается как:
\[
    \left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} \right\|_2 \leqslant 1.
\]
Второй множитель оценивается через минимальное собственное значение:
\[
    \left\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \leqslant \frac{1}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)}.
\]
Третий множитель, как уже было установлено, равен~$\| \mathbf{x}_{k+1}\|_2^2 \leqslant M^2$.
Таким образом, получаем оценку для первого множителя:
\[
    \left\| \left( \mathbf{I} + \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^{\top} \right)^{-1} - \mathbf{I} \right\|_2 \leqslant \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)}.
\]

Теперь оценим второй множитель первого слагаемого вместе с третьим множителем:
\[
    \left\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2 \leqslant \frac{\left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)}.
\]
Норма~$\left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2$ оценивается с использованием условия ограниченности:
\[
    \left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2 = \left\| \sum\limits_{i=1}^{k} \mathbf{x}_i y_i \right\|_2 \leqslant \sum\limits_{i=1}^{k} \left\| \mathbf{x}_i y_i \right\|_2 \leqslant k M^2.
\]
Следовательно:
\[
    \left\| \left( \mathbf{X}_k^{\top} \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{X}_k^{\top} \mathbf{y}_k \right\|_2 \leqslant \frac{k M^2}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)}.
\]
Теперь рассмотрим второе слагаемое:
\[
    \left\| \left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{x}_{k+1} y_{k+1} \right\|_2 \leqslant \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} \right)}.
\]

Комбинируя все полученные оценки, приходим к итоговой оценке:
\begin{align}
    \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 &\leqslant \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)} \cdot \frac{k M^2}{\lambda_{\min}\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)} + \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} \right)} \\
    &= \frac{k M^4}{\lambda_{\min}^2\left( \mathbf{X}_k^{\top} \mathbf{X}_k \right)} + \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_{k+1}^{\top} \mathbf{X}_{k+1} \right)}.
\end{align}

Из условия~$\lambda_{\min}\left(\mathbf{X}_k^{\top}\mathbf{X}_k \right) = \omega(\sqrt{k})$ следует:
\begin{align}
    \frac{1}{\lambda_{\min}\left(\mathbf{X}_k^{\top}\mathbf{X}_k \right)} &= o\left(\frac{1}{\sqrt{k}}\right), \\
    \frac{1}{\lambda_{\min}^2\left(\mathbf{X}_k^{\top}\mathbf{X}_k \right)} &= o\left(\frac{1}{k}\right).
\end{align}
Поэтому первое слагаемое оценивается как~$k \cdot o\left(\frac{1}{k}\right) = o(1)$, а второе слагаемое как~$o\left(\frac{1}{\sqrt{k}}\right) = o(1)$. Таким образом:
\[
    \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 = o(1) \quad \text{при} \quad k \to \infty.
\]

Это завершает доказательство сходимости как ковариационных матриц, так и математических ожиданий апостериорного распределения параметров.
\end{proof}

Теорема~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression} является ключевой в данном разделе, так как при слабых и понятных предположениях из нее следует сходимость моментов апостериорного распределения параметров.
Первое предположение в теореме~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression} касается ограничения на область значений признаков и целевой переменной.
Это условие обычно выполняется в практических приложениях, поэтому оно служит в первую очередь для целей теоретического анализа.
Второе условие теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression} представляет больший интерес, поскольку оно углубляется в поведение минимального собственного значения выборочной ковариационной матрицы признаков.
К сожалению, в данной работе не приводятся теоретические гарантии для этой сходимости, хотя сходимость проверяется экспериментально.

\section{Результаты вычислительных экспериментов}

В данном разделе описываются результаты вычислительных экспериментов для методов, описанных в данной главе.

\subsection{Определения достаточного размера выборки на основе статистических методов}
\section{Анализ методов определения достаточного размера выборки}
\begin{table}[h!t]
\centering
\caption{Описание выборок для анализа качества определения оптимального размера выборки}
\label{chapter:samplesize:experiment:static:table20}
\begin{tabular}{|l|l|c|c|}
\hline
	\centering Выборка & Задача & Число признаков & Размер выборки\\ \hline
	\hline 	Boston Housing 	&regression		&14 & 506\\
	\hline	Diabets  				& regression		&20  & 576\\
	\hline	Forest Fires 			& regression		& 13 & 517\\
  	\hline	Servo 					& regression 	& 4   & 167\\
	\hline	NBA				 		& classification	& 12 & 2235\\
\hline
\end{tabular}
\end{table} 


Проводиться эксперимент для анализа свойств методов оценки достаточного размера выборки. Эксперимент состоит из трех частей.
В первой части рассматриваются оценки достаточного размера выборки для разных наборов данных с фиксированным набором гиперпараметров различных методов.
Во второй части исследуется зависимость достаточного размера выборки от имеющегося размера выборки. В третьей части исследуется поведение методов в зависимости от изменения гиперпараметров методов. В качестве данных использовались выборки, описанные в таблице \ref{chapter:samplesize:experiment:static:table20}. Методы в строках таблицы \ref{chapter:samplesize:experiment:static:table2} показывают оценки размера выборки для соответствующих выборок.
 
\begin{table}[h!t]
\centering
\caption{Эксперимент по оценке размера выборки для различных наборов выборок}
\label{chapter:samplesize:experiment:static:table2}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Методы и наборы данных                        & Boston Housing & Diabetes & Forest Fires & Servo & NBA \\ \hline\hline
Lagrange Multipliers Test & 18             & 25       & 44          & 38    & 218 \\ \hline
Likelihood Ratio Test     & 17             & 25       & 43          & 18    & 110 \\ \hline
Wald Test                 & 66             & 51       & 46          & 76    & 200 \\ \hline
Cross Validation          & 178            & 441      & 172         & 120   & --   \\ \hline
Bootstrap                 & 113            & 117      & 86          & 60    & 405 \\ \hline
APVC                      & 98             & 167      & 351         & 20    & --   \\ \hline
ACC                       & 228            & 441      & 346         & 65    & --   \\ \hline
ALC                       & 98             & 267      & 516         & 25    & --   \\ \hline
Utility Function          & 148            & 172      & 206         & 105   & 925 \\ \hline
\end{tabular}
\end{table}

В этой части вычислительного эксперимента анализируется сходимость разных методов на разных выборках. В эксперименте используются выборки: Boston Housing~\cite{boston1978}, Diabetes, Forest Fires, Servo~\cite{servo1992}, NBA.
Результат анализа представлен в таблице~\ref{chapter:samplesize:experiment:static:table2}. Символ ``--'' обозначает, что исходный размер выборки недостаточный для прогноза.

Гиперпараметры каждого метода для всех выборок описаны в таблице~\ref{chapter:samplesize:experiment:static:table3}. Поскольку критерии Лагранжа, отношения правдоподобия и Вальда асимптотически эквивалентны, то параметры этих методов задавались одинаково. Параметры методов <<Average Coverage>> и <<Average Length>> также задаются одинаково.

\begin{table}[h!t]
\begin{center}
\caption{Экспертные оценки гиперпараметров для разных методов оценки объема выборки}
\label{chapter:samplesize:experiment:static:table3}
\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline 
Method& GLM parameters&~$l$&~$\varepsilon$&~$\alpha$&~$\beta$\\ \hline
\hline	
Lagrange	Multipliers Test	&~$\textbf{w}_{u}^0$ & -- & 0.2& 0.05& 0.2\\
\hline	
Likelihood Ratio Test			&~$\textbf{w}_{u}^0$ & -- & 0.2& 0.05& 0.2\\
\hline	
Wald	Test								&~$\textbf{w}_{u}^0$ & -- & 0.2& 0.05& 0.2\\
\hline	
Cross Validation 					& -- & -- 	& 0.05& -- & --\\
\hline	
Bootstrap 								& -- & 0.5	& -- & 0.05& --\\
\hline	
APVC 									& -- & 0.5	& -- & -- & --\\
\hline	
ACC 									& -- & 0.25	& -- & 0.05& --\\
\hline	
ALC 										& -- & 0.5	& -- & 0.05& --\\
\hline	
Utility function 						& -- & -- 	& 0.005& -- & --\\
\hline
\end{tabular}
\end{center}
\end{table}


Вычислительный эксперимент проводился для анализа описанных методов. Выбирается некоторый размер выборки~$m$ и методом бутстрап семплируется множество подвыборок размером~$m$. Для разных значений~$m$ вычисляется~$m^*$.
    
\begin{figure}[h!t]\center
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/cross}
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/apvc}\\
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/acc}
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/alc}\\
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/bootstrap}
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/kl}\\
    \includegraphics[width=0.49\textwidth]{thesis/figures/chapter-3/statical/maxu}
    \caption{Зависимость статистических значений различных методов}
    \label{chapter:samplesize:experiment:static:fig1}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=0.85\textwidth]{thesis/figures/chapter-3/statical/graphs}
    \caption{Анализ методов в зависимости от доступного размера выборки}
    \label{chapter:samplesize:experiment:static:fig2}
\end{figure}

Рис.~\ref{chapter:samplesize:experiment:static:fig1} демонстрирует зависимость статических значений каждого метода для разных выборок с фиксированным размером выборки~$m$.
Пороговые значения для каждого метода устанавливаются экспертно, что позволяет контролировать различные статистические характеристики выборки.
Рис.~\ref{chapter:samplesize:experiment:static:fig1} показывает адекватность различных методов определения достаточного размера выборки.
Представленные функции монотонны и асимптотически стремятся к константе.
На рис.~\ref{chapter:samplesize:experiment:static:fig2} показаны результаты методов на выборках разных размера. Показано различие методов в дисперсии вычисленного~$m^*$.
Анализируются различные методы в случае небольшого размера выборки.
Все представленные методы сходятся, причем результат предсказания в асимптотике не зависит от доступного размера выборки~$m$.   

Небольшое значение дисперсии интерпретируется как вычислительная устойчивость рассмотренных методов.
Показано, что некоторые методы не дают оценку достаточного размера выборки, если у них нет соответствующего размера выборки.
Это значит, что они не эффективны с точки зрения прогноза, но могут быть использованы для ретроспективы и анализа уже проведенного эксперимента.

Анализируется оценка достаточного размера выборки в зависимости от гиперпараметров для байесовских методов, а также эврестических методов. Для анализа рассмотрена выборка {Boston Housing}.
Байесовские методы используют решающее правила над скалярной функцией для определения достаточного размера выборки.
На рис.~\ref{chapter:samplesize:experiment:static:fig1} показана зависимость скалярных функций от размера подвыборки.
На рис.~\ref{chapter:samplesize:experiment:static:fig1} показано, что эти функции монотонны.
Тип поведения функции зависит от метода. Изменяя ограничения, установленные экспертно, можно изменить размер выборки, который будет соответствовать этим ограничениям.


\subsection{Определение достаточного размера выборки на основе
сэмлирования эмпирической функции ошибки}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{thesis/figures/chapter-3/likelihood-bootstraping/synthetic-regression}
    \caption{Сходимость предложенных функций~$D(k)$ и~$M(k)$ для синтетического набора данных регрессии, то есть модели линейной регрессии. Обе функции стремятся к нулю с увеличением размера выборки.}
    \label{chapter:samplesize:experiment:likelihood-bootstraping:fig:synthetic-regression}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{thesis/figures/chapter-3/likelihood-bootstraping/synthetic-classification}
    \caption{Сходимость предложенных функций~$D(k)$ и~$M(k)$ для синтетического набора данных классификации, то есть модели логистической регрессии. Обе функции стремятся к нулю с увеличением размера выборки.}
    \label{chapter:samplesize:experiment:likelihood-bootstraping:fig:synthetic-classification}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{thesis/figures/chapter-3/likelihood-bootstraping/liver-disorders}
    \caption{Сходимость предложенных функций~$D(k)$ и~$M(k)$ для набора данных Liver Disorders. Обе функции стремятся к нулю с увеличением размера выборки.}
    \label{chapter:samplesize:experiment:likelihood-bootstraping:fig:liver-disorders}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{thesis/figures/chapter-3/likelihood-bootstraping/sufficient-vs-threshold}
    \caption{Зависимость достаточного размера выборки от порога для трех наборов данных: синтетическая регрессия, синтетическая классификация и Liver Disorders. С увеличением значения порога~$\varepsilon$ достаточный размер выборки уменьшается. Это означает, что можно выбрать меньше объектов для достижения желаемых значений предложенных функций~$D(k)$ и~$M(k)$.}
    \label{chapter:samplesize:experiment:likelihood-bootstraping:fig:sufficient-vs-threshold}
\end{figure}

\begin{table}[h!t]\center
    \caption{Сравнение предложенных методов определения размера выборки: на основе функций~$D(k)$ и~$M(k)$. Для каждой из предложенных функций пороговое значение~$\varepsilon$ подбиралось таким образом, чтобы начальное значение функции уменьшалось вдвое. Результаты получены для различных наборов данных с задачей регрессии. Прочерки в таблице означают, что исходный размер выборки недостаточен.}\label{chapter:samplesize:experiment:likelihood-bootstraping:table}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    Dataset name & Objects~$m$ & Features~$n$ & D & M \\
    \hline
    Abalone & 4177 & 8 & 96 & 96  \\
    Auto MPG & 392 & 8 & 15 & 15 \\
    Automobile & 159 & 25 & 70 & 156  \\
    Liver Disorders & 345 & 6 & 12 & 19  \\
    Servo & 167 & 4 & 41 & ---  \\
    Forest fires & 517 & 12 & 208 & --- \\
    Wine Quality & 6497 & 12 & 144 & 144  \\
    Energy Efficiency & 768 & 9 & 24 & 442  \\
    Student Performance & 649 & 32 & 129 & 177  \\
    Facebook Metrics & 495 & 18 & 31 & 388   \\
    Real Estate Valuation & 414 & 7 & 15 & 23  \\
    Heart Failure Clinical Records & 299 & 12 & 63 & 224  \\
    Bone marrow transplant: children & 142 & 36 & --- & --- \\
    \hline
    \end{tabular}
\end{table}

В данном разделе представлено эмпирическое исследование предложенных методов. Эксперименты проводились на синтетических данных и наборе данных Liver Disorders из~\cite{uci}.

Синтетические данные были сгенерированы из моделей линейной регрессии и логистической регрессии. Количество объектов составляет 1000, количество признаков~---~$20$.
Использовалось~$B=1000$ бутстрэп-подвыборок.
Вычислялись значения~$D(k)$ и~$M(k)$.
Набор данных регрессии Liver Disorders содержит 345 объектов и 5 признаков.
Мы также использовали~$B=1000$ подвыборок, полученных методом бутстрэпа, для оценки математического ожидания и дисперсии функции потерь.

Рис.~\ref{chapter:samplesize:experiment:likelihood-bootstraping:fig:synthetic-regression} показывает полученные зависимости между доступным размером выборки~$k$ и предложенными функциями~$D(k)$ и~$M(k)$ для синтетического набора данных регрессии.
Результаты для синтетического набора данных классификации представлены на Рис.~\ref{chapter:samplesize:experiment:likelihood-bootstraping:fig:synthetic-classification}.
В то же время, на Рис.~\ref{chapter:samplesize:experiment:likelihood-bootstraping:fig:liver-disorders} видны графики для набора данных Liver Disorders. Можно заметить, что во всех случаях значения~$D(k)$ и~$M(k)$ приближаются к нулю с увеличением размера выборки.
Эти эмпирические результаты подтверждают полученные ранее теоретические выводы.

В определениях D-достаточности и M-достаточности присутствует гиперпараметр~$\varepsilon$, который соответствует порогу для достаточного размера выборки~$m^*$.
Чтобы изучить зависимость между ними, построена зависимость на Рис.~\ref{chapter:samplesize:experiment:likelihood-bootstraping:fig:sufficient-vs-threshold}, который показывает, какие размеры выборки можно выбрать для обеспечения определенного уровня достоверности.

Для сравнения производительности предложенных методов на различных наборах данных были выбраны выборки из открытого репозитория~\cite{uci}.
Подробная информация о каждом наборе данных, количестве наблюдений и количестве признаков представлена в Таблице~\ref{chapter:samplesize:experiment:likelihood-bootstraping:table}.
В демонстрационных целях было выбрано значение гиперпараметра~$\varepsilon$, при котором значение целевой функции,~$D(k)$ или~$M(k)$, уменьшается вдвое.
Соответствующие результаты представлены в Таблице~\ref{chapter:samplesize:experiment:likelihood-bootstraping:table}.
Пропуски означают, что исходный размер выборки недостаточен.

\subsection{Определение достаточного размера выборки на основе
близости апостериорных распределений}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{figures/chapter-3/posterior-distribution/eigvals}
    \caption{Зависимость минимального собственного значения от размера доступной выборки. Оценочный график, изображенный синим цветом, для больших размеров выборки лежит выше оранжевого графика степенной функции. Такое асимптотическое поведение минимального собственного значения соответствует полученным теоретическим результатам.}
    \label{chapter:samplesize:experiment:fig:eigvals}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{figures/chapter-3/posterior-distribution/synthetic-regression}
    \caption{Синтетический набор данных регрессии демонстрирует результаты сходимости предложенных функций оценки размера выборки. График слева соответствует расхождению Кульбака-Лейблера и стремится к нулю, в то время как график справа стремится к единице, отражая поведение функции схожести s-score.}
    \label{chapter:samplesize:experiment:fig:synthetic-regression}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{figures/chapter-3/posterior-distribution/liver-disorders}
    \caption{Набор данных Liver Disorders демонстрирует результаты сходимости предложенных функций оценки размера выборки. Слева представлено расхождение Кульбака-Лейблера, которое стремится к нулю с увеличением размера выборки. Справа представлена функция схожести s-score, которая стремится к единице при приближении размера выборки к бесконечности.}
    \label{chapter:samplesize:experiment:fig:liver-disorders}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{figures/chapter-3/posterior-distribution/sufficient-vs-threshold}
    \caption{Зависимость достаточного размера выборки от порогового параметра. Для S-достаточного размера выборки требуются более низкие значения порога. Таким образом, он оказывается более требовательным к этому значению.}
    \label{chapter:samplesize:experiment:fig:sufficient-vs-threshold}
\end{figure}

\begin{table}[h!t]\center
    \caption{Описание выборок, используемых в эксперименте.}\label{chapter:samplesize:experiment:table:descr}
    \begin{tabular}{lcc}
    \hline
        \toprule
        Выборка & Количество признаков,~$n$ & Количество объектов,~$m$ \\ 
        \midrule
        Boston Housing & 14 & 506 \\ 
        Diabetes & 10 & 576 \\ 
        Forest Fires & 13 & 517 \\ 
        Servo & 4 & 167 \\ 
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!t]\center
    \caption{Экспериментальная оценка достаточного размера выборки, согласно предложенным методам для различных наборов данных.}\label{chapter:samplesize:experiment:table:results}
    \begin{tabular}{lcccc}
    \hline
        \toprule
        Methods and sample sets & Boston & Diabetes & Forest Fires & Servo \\ 
        \midrule
        Lagrange Multipliers Test & 18 & 25 & 44 & 38 \\
        Likelihood Ratio Test & 17 & 25 & 43 & 18 \\
        Wald Test & 66 & 51 & 46 & 76 \\ 
        Cross Validation & 178 & 441 & 171 & 120 \\ 
        Bootstrap & 113 & 117 & 86 & 60 \\ 
        APVC & 98 & 167 & 351 & 20 \\ 
        ACC & 228 & 441 & 346 & 65 \\ 
        ALC & 98 & 267 & 516 & 25 \\ 
        Utility function & 148 & 172 & 206 & 105 \\ 
        \midrule
        KL (ours) & 493 & 437 & 86 & 165 \\ 
        S (ours) & 28 & 22 & 26 & 10 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{figures/chapter-3/posterior-distribution/dependence_on_available_sample_set.pdf}
    \caption{Зависимость оцененного достаточного размера выборки~$m^*$ от доступного размера выборки~$m$ для каждого метода. Критерий на основе расхождения Кульбака-Лейблера является более консервативным и требует большего размера выборки. Критерий S-достаточности, напротив, предполагает, что может быть достаточен минимальный размер выборки.}
    \label{chapter:samplesize:experiment:fig:dependence_on_available_sample_set}
\end{figure}

В данном разделе представлено расширенное эмпирическое исследование предложенных методов. Эксперименты состоят из нескольких частей.
В первой мы проверяем сходимости, полученные в ходе теоретического анализа.
Далее мы оцениваем размеры выборок для различных наборов данных, используя разные подходы.
Наконец, мы изучаем зависимость достаточного размера выборки от объема доступных данных.

Здесь мы исследуем, наблюдаются ли на практике полученные теоретические сходимости.
А именно, сначала мы рассматриваем поведение минимального собственного значения матрицы~$\mathbf{X}_k^\top \mathbf{X}_k$ при увеличении размера выборки.
Затем мы исследуем сходимость предложенных функций~$KL(k)$ и~$S(k)$.
Наконец, изучается зависимость достаточного размера выборки от пороговых параметров.
Эксперимент проводится на двух наборах данных: синтетическая регрессия и Liver Disorders.

Синтетические данные генерируются из модели линейной регрессии.
Количество объектов составляет 500, количество признаков - 10.
Для генерации синтетического набора данных регрессии мы выполнили выборку исходных признаков, параметров модели и шумовых остатков из стандартного нормального распределения.
Априорное распределение параметров также было задано как стандартное нормальное, как для синтетической регрессии, так и для набора данных Liver Disorders, который содержит 345 объектов и 5 признаков.
Мы предобработали входные признаки, используя стандартный масштабатор (Standard Scaler).

Один объект последовательно удалялся из заданной выборки до тех пор, пока количество объектов в подвыборке не становилось равным количеству признаков.
Для каждого размера выборки~$k$ мы вычисляли минимальное собственное значение матрицы~$\mathbf{X}_k^\top \mathbf{X}_k$.
Также вычислялись значения~$KL(k)$ и~$S(k)$. Этот процесс повторялся~$B=100$ раз.

На Рис.~\ref{chapter:samplesize:experiment:fig:eigvals} показано асимптотическое поведение минимального собственного значения матрицы~$\mathbf{X}_k^\top \mathbf{X}_k$. Мы видим, что когда размер выборки стремится к бесконечности, минимальное собственное значение также стремится к бесконечности.
При этом, как и требуется для Теоремы~\ref{chapter:samplesize:theorem-kiselev-posterior-similiarity-linear-regression}, график лежит выше~$\sqrt{k}$.

На Рис.~\ref{chapter:samplesize:experiment:fig:synthetic-regression} мы можем наблюдать полученные зависимости между доступным размером выборки~$k$ и предложенными функциями~$KL(k)$ и~$S(k)$ для синтетического набора данных регрессии.
В то же время, на Рис.~\ref{chapter:samplesize:experiment:fig:liver-disorders} мы видим аналогичные графики для набора данных Liver Disorders.
Можно заметить, что в обоих случаях значение~$KL(k)$ приближается к нулю с увеличением размера выборки, а~$S(k)$ стремится к единице.
Эти эмпирические результаты подтверждают полученные ранее теоретические выводы.

В определениях KL-достаточности и S-достаточности присутствует гиперпараметр~$\varepsilon$, который соответствует порогу для достаточного размера выборки~$m^*$.
Чтобы изучить зависимость между ними, мы представляем Рис.~\ref{chapter:samplesize:experiment:fig:sufficient-vs-threshold}, который показывает, какие размеры выборки можно выбрать для обеспечения определенного уровня достоверности.

Для сравнения наших предложенных методов с базовыми мы использовали следующую схему эксперимента. Модель машинного обучения - линейная регрессия.
Мы выбрали 4 набора данных с задачей регрессии из открытых источников: Boston, Diabetes, Forestfires и Servo.
Их описательная статистика представлена в Таблице~\ref{chapter:samplesize:experiment:table:descr}.
Мы применили к ним 9 различных базовых методов оценки размера выборки: Тест множителей Лагранжа, Тест отношения правдоподобий, Тест Вальда, Кросс-валидация, Бутстрэп, Критерий средней апостериорной дисперсии (APVC), Критерий среднего покрытия (ACC), Критерий средней длины (ALC) и Функция полезности.
Они были подробно проанализированы в~\cite{motrenko2022numerical613055643}.
Использовались значения параметров этих методов по умолчанию.

Что касается наших методов, мы немного изменили определения достаточности, переведя их в термины относительного изменения.
А именно, мы считаем размер выборки достаточным, если функция~$KL(k)$ имеет относительное отклонение от своего значения на всей выборке не более чем~$\varepsilon$.
Аналогично с функцией~$S(k)$.
Мы зафиксировали~$\varepsilon=0.05$ и получили результирующие размеры выборок.
Это значение было выбрано потому, что другие методы, особенно статистические, используют~$0.05$ как значение ошибки I рода.

Результаты в Таблице~\ref{chapter:samplesize:experiment:table:results} указывают на то, что критерий на основе расхождения Кульбака-Лейблера является более консервативным и требует большего размера выборки, в то время как критерий S-достаточности предполагает, что минимального размера выборки может быть достаточно.
Мы полагаем, что это типичный результат для функции схожести s-score, которая была разработана для сравнения различных моделей машинного обучения, особенно в случаях с неинформативными распределениями.
Если распределения имеют высокую дисперсию, функция близости приближается к единице, что приводит к тому, что критерий считает достаточным даже небольшой размер выборки.

Данная часть включает наиболее комплексный анализ различных методов определения размера выборки.
Мы анализируем, как достаточный размер выборки зависит от объема доступного набора данных. В частности, мы увеличиваем объем доступной выборки и вычисляем достаточный размер на основе различных методов.
Таким образом, мы получаем Рис.~\ref{chapter:samplesize:experiment:fig:dependence_on_available_sample_set}, который позволяет нам сравнить вышеупомянутые методы с точки зрения их консервативности.

Можно видеть, что S-достаточный размер выборки часто является минимальным.
Мы уже обсуждали причину этого в предыдущем подразделе.
Также, KL-достаточный размер выборки, как правило, требует почти полной выборки.
По нашему мнению, это связано с тем, что расхождение Кульбака-Лейблера чрезвычайно чувствительно к изменениям математического ожидания и дисперсии сравниваемых распределений.
Таким образом, стабилизация расстояния между ними происходит довольно поздно.

\section{Заключение по главе}

В данной главе была рассмотрена задача оценки достаточного объема выборки для задач машинного обучения.
Были предложены различные методы решения данной задачи.

Предложен метод, основанный на анализе функции правдоподобия, включает методы D-достаточности и M-достаточности.
Метод D-достаточности использует дисперсию функции правдоподобия на бутстрэп-подвыборках, в то время как метод M-достаточности анализирует разность математических ожиданий функции правдоподобия при последовательном добавлении объектов в выборку.
Для линейной регрессионной модели была строго доказана корректность определения M-достаточного объема выборки при выполнении определенных условий на параметры модели.

Также, предложен метод, основанный на анализе близости апостериорных распределений параметров модели на похожих подвыборках.
В рамках этого подхода были предложены критерии KL-достаточности и S-достаточности, использующие расхождение Кульбака-Лейблера и функцию схожести s-score соответственно.
Для нормального апостериорного распределения удалось получить аналитические выражения для этих мер близости, что значительно упростило теоретический анализ.

Экспериментальные исследования на синтетических и реальных данных подтвердили эффективность предложенных методов.
Показано, что функции~$D(k), M(k), KL(k)$ стремятся к нулю, а функция~$S(k)$~--- к единице с ростом объема выборки.
Сравнительный анализ выявил особенности различных критериев: KL-дивергенция дает более консервативные оценки, требующие больших объемов данных, в то время как s-score часто указывает на достаточность минимального размера выборки.

Практические рекомендации включают использование относительных отклонений от значений на полной выборке с порогами~$0.05-0.1$.
Основное ограничение методов связано с вычислительной сложностью обращения ковариационных матриц для моделей с большим количеством параметров.

Перспективы дальнейших исследований включают расширение теоретического обоснования методов на более сложные модели, в том числе нейронные сети, а также преодоление ограничения о нормальности апостериорного распределения.
Полученные результаты создают основу для разработки практических инструментов оценки достаточного объема данных в прикладных задачах машинного обучения.