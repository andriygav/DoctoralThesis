В настоящей главе рассматриваются методы снижения сложности параметрических моделей глубокого обучения. Предполагается, что число параметров нейросети можно существенно снизить без значимой потери качества и значимого повышения дисперсии функции ошибки.

Предлагаются методы снижения сложности моделей на основе ковариационной матрицы градиентов функции ошибки по параметрам модели. Разработанные методы опираются на теоретический аппарат, введенный в главах~\ref{chapter:complexity} и~\ref{chapter:gesian}, и обеспечивают практические инструменты для уменьшения сложности моделей при сохранении их качества.

\section{Удаления параметров моделей глубокого обучения}

В настоящем разделе рассматривается метод удаления параметров моделей глубокого обучения на основе анализа ковариационной матрицы градиентов функции ошибки. Предложенный подход позволяет упорядочить параметры по их важности и последовательно удалять наименее значимые параметры без существенной потери качества модели.

Рассмотрим выборку:
\[
    \label{ch-5:eq:st:1}
    \begin{aligned}
    \mathfrak{D} = \bigr\{\bigr(\textbf{x}_i, y_i\bigr)\bigr\}_{i=1}^{m}, \quad \textbf{x}_{i} \in \mathbb{X} = \mathbb{R}^{n}, \quad y_i \in \mathbb{Y},
    \end{aligned}
\]
где $n$~--- размерность признакового пространства, $m$~--- число объектов в выборке. Пространство ответов $\mathbb{Y} = \mathbb{R}$ в случае задачи регрессии и  $\mathbb{Y} = \{1,\ldots, R\}$ в случае задачи классификации, где $R$~--- число классов.

Определим семейство моделей параметрических функций с наперед заданной структурой:
\[
    \label{ch-5:eq:st:2}
    \begin{aligned}
    \mathfrak{F} &= \bigr\{f\bigr(\textbf{w}\bigr):\mathbb{X} \to \mathbb{Y} | \textbf{w} \in \mathbb{R}^{p}\bigr\}, \\ 
    \mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr) &= \textbf{W}_1\bm{\sigma}\bigr(\textbf{W}_2\bm{\sigma}\bigr(\ldots\bm{\sigma}\bigr(\textbf{W}_r\textbf{x}\bigr)\ldots\bigr)\bigr),\\
    f_{\text{\text{cl}}}\bigr(\textbf{w}, \textbf{x}\bigr) &= \arg \max_{j \in \bigr\{1,\ldots, R\bigr\}} \text{softmax}\bigr(\mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr)\bigr)_{j}, \\ 
    f_{\text{reg}}\bigr(\textbf{w}, \textbf{x}\bigr) & = \mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr), 
    \end{aligned}
\]
где $p$~--- размерность пространства параметров, $r$~--- число слоев нейросети, $\textbf{w} = \text{vec}[\textbf{W}_1, \textbf{W}_2, \ldots, \textbf{W}_r]$, а $\bm{\sigma}$~--- функция активации. В случае задачи регрессии структура модели имеет вид $f_{\text{\text{reg}}}$, а в случае классификации имеет вид $f_{\text{\text{cl}}}$.
Определим функцию потерь:
\[
    \label{ch-5:eq:st:3}
    \begin{aligned}
    \mathcal{L}\bigr(\textbf{w}, \mathfrak{D}\bigr) &= \frac{1}{m}\sum_{i=1}^{m}l\bigr(\textbf{x}_{i}, y_i, \textbf{w}\bigr),\\
    l_{\text{\text{reg}}}\bigr(\textbf{x}, y, \textbf{w}\bigr) &= \bigr(y - f\bigr(\textbf{w}, \textbf{x}\bigr)\bigr)^{2},\\
    l_{\text{\text{cl}}}\bigr(\textbf{x}, y, \textbf{w}\bigr) &= -\sum_{j=1}^{R}\bigr([y = j]\ln\text{softmax}_j\bigr(\mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr)\bigr)\bigr),
    \end{aligned}
\]
где $l_{\text{\text{reg}}}$~--- это функция ошибки на одном элементе для задачи регрессии, $l_{\text{\text{cl}}}$~--- для задачи классификации.
Оптимальный вектор параметров $\hat{\textbf{w}}$ получается минимизацией функции потерь:
\[
    \label{ch-5:eq:st:0:1}
    \begin{aligned}
    \hat{\textbf{w}} = \arg \min_{\textbf{w}\in\mathbb{R}^{p}} \mathcal{L}\bigr(\textbf{w}, \mathfrak{D}\bigr).
    \end{aligned}
\]

Для поиска оптимальных параметров модели используется градиентный метод оптимизации:
\[
    \label{ch-5:eq:st:4}
    \begin{aligned}
    \textbf{w}_{t} = \textbf{w}_{t-1} + \Delta\textbf{w}\bigr(\textbf{g}_{S,t}, \textbf{w}_{t-1}, \textbf{w}_{t-2}, \ldots\bigr), \quad \textbf{g}_{S,t}=\frac{\partial \mathcal{L}\bigr(\textbf{w}_{t}, \textbf{X}_{S}, \textbf{Y}_{S}\bigr)}{\partial \textbf{w}},
    \end{aligned}
\]
где $t$~--- номер итерации, $\textbf{g}_{S,t}$~--- значение градиента на подвыборке размера $S$, $\Delta\textbf{w}$~--- приращение вектора параметров.
 
 
Порядок на множестве параметров модели задается при помощи ковариационной матрицы $\textbf{C}$ градиентов функции ошибки $\mathcal{L}$ по параметрам модели $\textbf{w}$. Для вычисления ковариационной матрицы $\textbf{C}$ используется итерационная формула~\cite{Chunyan2016}, вычисляемая на каждой итерации \eqref{ch-5:eq:st:4} градиентного метода оптимизации параметров:
\[
    \label{ch-5:eq:st:5}
    \begin{aligned}
    \textbf{C}_t = \bigr(1-\kappa_t\bigr)\textbf{C}_{t-1}+\kappa_t\bigr(\textbf{g}_{1,t}-\textbf{g}_{S,t}\bigr)\bigr(\textbf{g}_{1,t}-\textbf{g}_{S,t}\bigr)^{\mathsf{T}},
    \end{aligned}
\]
 где $t$~--- номер итерации, $\textbf{g}_{S,t}$~--- значение градиента на подвыборке размера $S$, $\textbf{g}_{1,t}$~--- значение градиента на первом элементе подвыборки, $\kappa_t=\frac{1}{t}$~--- параметр сглаживания, $\textbf{C}_0$ инициализируются из равномерного распределения.
 
Пусть известно $t_0$~--- число итераций, после которого все параметры находятся в некоторой локальной окрестности минимума, тогда, как показано в работе~\cite{Chunyan2016}, матрица $\textbf{C}_{t_0}$ аппроксимирует истинную ковариационную матрицу $\textbf{C}$. Ковариационная матрица $\textbf{C}_{t_0}$ используется для упорядочения параметров модели $\textbf{w}_{t_0}$. 
 
Пусть $\mathcal{I}$~---  упорядоченный вектор индексов $[1, 2, \ldots, p]$. Обозначим $\mathcal{I}_{\textbf{w}_{t_0}}$ вектор индексов, порядок которого задан при помощи ковариационной матрицы $\textbf{C}_{t_0}$. 
 
Например, если ковариационная матрица $\textbf{C}_{t_0}$  имеет вид
$$
    \begin{bmatrix}
    0{,}3& 0 & 0\\
    0& 0{,}2 & 0\\
    0& 0 & 0{,}25\\
    \end{bmatrix},
$$
 то вектор индексов $\mathcal{I}_{\textbf{w}_{t_0}} = [3,1,2]$.
 
Для фиксации параметров $\textbf{w}_{t_0}$ при помощи вектора индексов $\mathcal{I}_{\textbf{w}_{t_0}}$ используется бинарный вектор $\bm{\alpha}\bigr(\zeta\bigr)$:
\[
    \label{ch-5:eq:st:6}
    \begin{aligned}
    \alpha_i\bigr(\zeta\bigr) = \begin{cases}
       1, &\text{если }\mathcal{I}_{\textbf{w}_{t_0}}[j] \leq \zeta;\\
       0 &\text{иначе},
     \end{cases}
    \end{aligned}
\]
 где $\zeta$~--- число фиксирующих параметров.
 
 Учитывая \eqref{ch-5:eq:st:6}, уравнение \eqref{ch-5:eq:st:4} приводится к виду
\[
    \label{ch-5:eq:st:7}
    \begin{aligned}
    \textbf{w}_{t} = \textbf{w}_{t-1} + \bm{\alpha}\bigr(\zeta\bigr)\cdot\Delta\textbf{w}\bigr(\textbf{g}_{S,t}, \textbf{w}_{t-1}, \textbf{w}_{t-2}, \ldots\bigr),
    \end{aligned}
\]
где $t$~--- номер итерации, $\textbf{g}_{S,t}$~--- значение градиента на подвыборке размера $S$, $\Delta\textbf{w}$~--- приращение вектора параметров. После умножения на бинарный вектор $\bm\alpha$ часть параметров не оптимизируется, что приводит к фиксации параметров.

Предлагается метод, основанный на модификации метода Белсли. Пусть $\textbf{w}$~--- вектор параметров, доставляющий минимум функционалу потерь $\mathcal{L}$ на множестве $\mathbb{W_\mathcal{A}}$, а $\textbf{A}_\text{ps}$ --- соответствующая ему ковариационная матрица.

Выполним сингулярное разложение матрицы
\[
\textbf{A}_\text{ps} = \textbf{U}{\bf\Lambda}\textbf{V}^\mathsf{T}.
\]
Индекс обусловленности $\eta_{j}$ определим как отношение максимального элемента к $j$-му элементу матрицы ${\bf\Lambda}$. Для нахождения мультиколлинеарных признаков требуется найти индекс $\xi$ вида:
\[
\xi = \arg\max_{j\in \mathcal{A}}{\eta_j}.
\]

\begin{figure}[h!t]\center
    \subfloat[Матрица ковариации]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Cov}}
    \subfloat[Дисперсионные доли]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/BelslyImage}}
    \caption{Иллюстрация метода Белсли для анализа мультиколлинеарности параметров на синтетических данных. Слева показана матрица ковариации параметров, справа --- дисперсионные доли, демонстрирующие вклад каждого признака в дисперсию параметров модели.}
    \label{CovBel}
\end{figure}

\begin{table}[h!t]
    \begin{center}
    \caption{Индексы обусловленности $\eta$ и дисперсионные доли $q_j$ для синтетических данных, демонстрирующие работу метода Белсли. Максимальный индекс обусловленности $\eta_6 = 1.2\cdot 10^{16}$ соответствует максимальным дисперсионным долям признаков с индексами 1 и 4, которые являются линейно зависимыми.}
    \begin{tabular}{|c|cccccc|}
    \hline
    $\eta$ & $q_1$& $q_2$& $q_3$& $q_4$& $q_5$& $q_6$\\
    \hline
    $1.0$ &  $2\cdot 10^{-17}$ &  $4\cdot 10^{-17}$ &  $1\cdot 10^{-16}$ &  $2\cdot 10^{-17}$ &  $6\cdot 10^{-17}$&  $3\cdot 10^{-4}$ \\
    \hline
    $1.5$ &  $5\cdot 10^{-17}$ &  $9\cdot 10^{-17}$ &  $2\cdot 10^{-16}$ &  $5\cdot 10^{-17}$ &  $3\cdot 10^{-20}$ &  $3\cdot 10^{-2}$ \\
    \hline
    $3.3$ &  $9\cdot 10^{-18}$ &  $1\cdot 10^{-17}$ &  $2\cdot 10^{-17}$ &  $9\cdot 10^{-18}$ &  $2\cdot 10^{-19}$ &  $9\cdot 10^{-1}$ \\
    \hline
    $2\cdot 10^{15}$ &  $1\cdot 10^{-2}$ &  $1\cdot 10^{-1}$ &  $8\cdot 10^{-1}$ &  $2\cdot 10^{-3}$ &  $9\cdot 10^{-2}$ &  $1\cdot 10^{17}$ \\ 
    \hline
    $8\cdot 10^{15}$ &  $6\cdot 10^{-2}$ &  $8\cdot 10^{-1}$ &  $9\cdot 10^{-2}$ &  $8\cdot 10^{-2}$ &  $9\cdot 10^{-1}$ & $ 2\cdot 10^{17} $\\
    \hline
    $1\cdot 10^{16}$ &  $\bf9\cdot 10^{-1}$ &  $1\cdot 10^{-2}$& $ 4\cdot 10^{-2}$&  $\bf9\cdot 10^{-1}$ &  $1\cdot 10^{-3}$ & $ 5\cdot 10^{-21}$ \\
    \hline
    \end{tabular}
    \label{CovBelTable}
    \end{center}
\end{table}

Дисперсионный долевой коэффициент $q_{ij}$ определим как вклад $j$-го признака в дисперсию $i$-го элемента вектора параметра $\textbf{w}$:

\[
q_{ij} = \frac{u^2_{ij}/\lambda_{jj}}{\sum^n_{j=1}{u^2_{ij}/\lambda_{jj}}}.
\]

Большие значения дисперсионных долей указывают на наличие зависимости между параметрами. Находим долевые коэффициенты, вносящие максимальный вклад в дисперсию параметра $w_\xi$:

\[
\zeta = \arg\max_{j\in \mathcal{A}}{q_{\xi j}}.
\]
Параметр с индексом $\zeta$ определяется как наименее релевантный параметр нейросети.

Проиллюстрируем принцип работы метода Белсли на примере. Гипотеза порождения данных: 
\[
    \textbf{w} = \begin{bmatrix}
    \text{sin}(x)\\
    \text{cos}(x)\\
    \text{2+cos}(x)\\
    \text{2+sin}(x)\\
    \text{cos}(x) + \text{sin}(x)\\
    x
    \end{bmatrix}
\]
с матрицей ковариации на рис.~\ref{CovBel}.a, где $x \in [0.0, 0.02, \ldots, 20.0]$.

В таблице~\ref{CovBelTable} приведены индексы обусловленности и соответствующие им дисперсионные доли, изображенные на рис.~\ref{CovBel}.b. Согласно этим данным, максимальный индекс обусловленности $\eta_6 = 1.2\cdot 10^{16}$. Ему соответствуют максимальные дисперсионные доли признаков с индексами 1 и 4, которые, согласно построению выборки, являются линейно зависимыми.


\section{Дистилляция моделей глубокого обучения на многодоменных данных}

В настоящем разделе рассматривается метод дистилляции моделей глубокого обучения на многодоменных данных. В отличие от классической дистилляции, где модель учителя и модель ученика обучаются на данных из одного домена, предлагаемый метод позволяет передавать знания между моделями, обученными на данных из различных доменов, связанных инъективным отображением.

\begin{figure}[h!t]\center
    {\includegraphics[width=0.4 \textwidth]{thesis/figures/chapter-4/multidomain-distillation/Distillation_only}}
    \caption{Схема базовой дистилляции моделей глубокого обучения, где модель учителя обучается на большом наборе данных из генеральной совокупности $A$, а затем ее выходы используются для обучения модели ученика на меньшем наборе данных из того же домена.}
    \label{dist_only}
\end{figure}

\begin{figure}[h!t]\center
    {\includegraphics[width=0.4 \textwidth]{thesis/figures/chapter-4/multidomain-distillation/Distillation_DA}}
    \caption{Схема дистилляции моделей глубокого обучения с доменной адаптацией, где модель учителя обучается на данных из домена $A$, а модель ученика --- на данных из домена $B$, связанных инъективным отображением $\varphi$.}
    \label{dist_da}
\end{figure}

\begin{definition}
Генеральная совокупность объектов~$B$ называется близкой к генеральной совокупности~$A$, если существует инъективное отображение $\varphi: A \rightarrow B$.
\end{definition}

Предлагается использовать, помимо меток учителя на одном из доменов, связь между доменами при обучении модели ученика.
В этом случае в качестве доменов должны выступать близкие генеральные совокупности.

На рис.~\ref{dist_only} показан процесс обучения модели ученика в базовой постановке задачи дистилляции. Модель учителя обучается на большом наборе данных из генеральной совокупности~$A$, затем ее выходы используются для обучения модели ученика на меньшем наборе данных из того же домена.
На рис.~\ref{dist_da} представлен предложенный метод, задействующий выходы модели учителя, обученной на другом домене, и связь между доменами.

Рассмотрим базовую постановку задачи дистилляции. Задан набор данных
$$\mathfrak{D}=\{(\mathbf{x_i}, \mathbf{y_i})\}_{i=1}^n,
\quad \mathbf{x_i} \in \mathbb{X},
\quad \mathbf{y_i} \in \{1,...,R\},$$
где $R$~--- количество классов в задаче классификации.

Предполагается, что задана обученная модель с большим количеством параметров --- модель учителя. Требуется обучить модель ученика с меньшим количеством параметров, учитывая ответы учителя. Модель учителя $\mathbf{f}$ и модель ученика $\mathbf{g}$ принадлежат параметрическому семейству функций:
$$\mathfrak{F}=\{\mathbf{f}|\mathbf{f}=\text{softmax}(\mathbf{v(x)} /T), \mathbf{v}:\mathbb{R}^{n}\rightarrow \mathbb{R}^{R}\},$$
где $\mathbf{v}$~--- дифференцируемая параметрическая функция заданной структуры, $T$~--- параметр температуры, используемый для смягчения распределения вероятностей.

Функция потерь $\mathcal{L}$, учитывающая модель учителя $\mathbf{f}$ при выборе модели ученика $\mathbf{g}$, имеет вид:
\[
\begin{aligned}
     \mathcal{L}(\mathbf{w,X,Y,f})=&-\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{R}y_{i }^{r}\log{g^{r}(x_{i})}\bigr|_{T=1}\\
     &-\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{R}f^{r}(x_{i})\bigr|_{T=T_{0} }\log{g^{r}(x_{i})}\bigr|_{T=T_{0}},
\end{aligned}
\]
где $\cdot\bigr|_{T=t}$ означает, что параметр температуры $T$ в предыдущей функции равен $t$. Первое слагаемое в функции потерь соответствует стандартной задаче классификации с температурой $T=1$, второе слагаемое --- дистилляции с температурой $T=T_0$, где $T_0 > 1$ позволяет получить более мягкое распределение вероятностей от модели учителя.

Задача оптимизации формулируется следующим образом:
$$\hat{\mathbf{w}} = \arg\min_{\mathbf{w} \in \mathbb{W}} \mathcal{L}(\mathbf{w,X,Y,f}).$$

Перейдем к постановке задачи дистилляции для многодоменной выборки. Даны две выборки:
$$\mathfrak{D}_{\text{s}}=\{(\mathbf{x_i},\mathbf{y_i})\}_{i=1}^n,
\quad \mathbf{x_i} \in \mathbb{X}_{\text{s}},
\quad \mathbf{y_i} \in \mathbb{Y}$$
$$\mathfrak{D}_{\text{t}}=\{(\mathbf{x_i},\mathbf{y_i})\}_{i=1}^m, \quad \mathbf {x_i} \in \mathbb{X}_{\text{t}},
\quad \mathbf{y_i} \in \mathbb{Y},$$
где $\mathfrak{D}_{\text{s}}, \mathfrak{D}_{\text{t}}$~--- исходный и целевой наборы данных. В базовой постановке задачи дистилляции предполагается, что
$\mathfrak{D}_{\text{t}} \subset \mathfrak{D}_{\text{s}},
\mathbb{X}_{\text{t}}=\mathbb{X}_{\text{s}}$. Предполагается, что количество объектов в наборах данных не совпадает:
$$n \gg m$$

Пусть задана модель учителя на выборке большей мощности:
$$\mathbf{f}: \mathbb{X}_{\text{s}} \rightarrow \mathbb{Y}^{\prime},$$
где $\mathbf{f}$~--- модель учителя, $\mathbb{Y}^{\prime}$~--- пространство вероятностей классов.

Связь между исходной и целевой выборками задается:
$$\varphi: \mathbb{X}_{\text{t}} \rightarrow \mathbb{X}_{\text{s}},$$
где $\varphi$~--- инъективное отображение. Требуется получить модель ученика для малоресурсной выборки:
$$\mathbf{g}: \mathbb{X}_{\text{t}} \rightarrow \mathbb{Y}^{\prime},$$
где $\mathbf{g}$~--- модель ученика.

В работе рассматривается функция потерь, учитывающая метки учителя и связь между доменами:
\begin{align}
    &\mathcal{L}(\mathbf{w,X,Y,f,\varphi})=\\&
    -\lambda\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{ R}\mathbb{I}[y_{i}=r]\log{g^{r}(\mathbf{x}_{i},\mathbf{w})} \\&-(1-\lambda)\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{R}(f\circ \varphi)^{r}(\mathbf{x }_{i})\log{g^{r}(\mathbf{x}_{i},\mathbf{w})},
\end{align}
где $\lambda$~--- метапараметр, задающий вес дистилляции, $\mathbb{I}$~--- индикаторная функция.

Задача оптимизации для мультидоменной дистилляции формулируется следующим образом:
$$\hat{\mathbf{w}} = \arg\min_{\mathbf{w} \in \mathbb{W}} \mathcal{L}(\mathbf{w,X,Y,f,\varphi} ).$$
Функция потерь объединяет информацию от истинных меток и от модели учителя, примененной к преобразованным данным другого домена.


\section{Анти-Дистилляция моделей глубокого обучения}

В настоящем разделе описывается постановка задачи анти-дистилляции для задачи классификации. В отличие от классической дистилляции, где знания передаются от более сложной модели к более простой, анти-дистилляция решает обратную задачу: передачу знаний от простой модели к более сложной для работы с более сложными наборами данных. Аналогичный подход может быть применен для произвольных задач машинного обучения.

Даны два набора данных
\begin{align}
    \mathfrak{D}_1 = \{(\mathbf{x}_i, y_i)\}_{i=1}^{m_1},~\mathbf{x}_i \in \mathbb{R}^{n},~y_i \in C_1 = \{1, \dots, c_1\}, \\
    \mathfrak{D}_2 =  \{(\mathbf{x}_i, y_i)\}_{i=1}^{m_2},~\mathbf{x}_i \in \mathbb{R}^{n},~y_i \in C_2 = \{1, \dots, c_2\},
\end{align}
где $m_1$ и $m_2$~--- количество объектов в $\mathfrak{D}_1$ и $\mathfrak{D}_2$ соответственно, $n$~--- размерность входного пространства. $C_1$ и $C_2$~--- множества меток классов $1, \dots, c_1, \dots, c_2$.

Предполагается, что объекты $\mathbf{x}_i$ порождены из генеральной совокупности, общей для обоих наборов данных $\mathfrak{D}_1, \mathfrak{D}_2$, и имеют схожие свойства для этих наборов.
Предполагается, что набор данных $\mathfrak{D}_2$ является более сложным для классификации и требует более сложной модели классификации.

Рассмотрим модель учителя $\mathbf{g}_\text{tr}$, обученную на первом наборе данных $\mathfrak{D}_1$:
\[
    \mathbf{g}_\text{tr}: \mathbb{R}^{n} \rightarrow \Delta^{c_1},\quad \mathbf{g}_\text{tr}(\mathbf{x}) = \mathbf{g}(\mathbf{x}, \hat{\mathbf{u}}),
\] 
где $\Delta^c$~--- множество $c$-мерных вероятностных векторов,

Параметры модели учителя $\mathbf{g}_\text{tr}$ определяются следующим образом:
$$\hat{\mathbf{u}} =  \underset{\mathbf{u}}{\arg\min}~\mathcal{L}_\text{ce}(\mathbf{u}, \mathfrak{D}_1) =\underset{\mathbf{u}}{\arg\min}~\sum\limits_{i=1}^{m_1} l \left(y_i,~g(\mathbf{x}_i, \mathbf{u})\right),$$
здесь $l$~--- перекрестная энтропия:
$$l(y, \hat{y}) = -\sum\limits_{k=1}^{c} [y = k] \log{\hat{y}_k},~y \in C,~\hat{y} \in \Delta^c.$$

Задача состоит в построении модели ученика
\[
    \mathbf{f}_\text{st}: \mathbb{R}^{n} \rightarrow \Delta^{c_2},\quad \mathbf{f}_\text{st}(\mathbf{x}) = \mathbf{f}(\mathbf{x}, \hat{\mathbf{w}}),
\],
минимизирующей перекрестную энтропию на валидационной части второго набора данных $\mathfrak{D}_2$
\[
    \hat{\mathbf{w}} =  \underset{\mathbf{w}}{\arg\min}~\mathcal{L}_\text{ce}(\mathbf{w}, \mathfrak{D}^\text{val}_2),
\],
где $\mathfrak{D}_2 = \mathfrak{D}^\text{train}_2 \sqcup \mathfrak{D}^{\text{val}}_2$ и $\hat{\mathbf{w}}$~--- оптимальные параметры модели.

Поскольку валидационная функция потерь не оптимизируется напрямую, используются градиентные методы оптимизации на обучающей части $\mathfrak{D}^\text{train}_2$ набора данных $\mathfrak{D}_2$.
Чтобы уменьшить переобучение и использовать больше информации о данных, используется информация от модели учителя $\mathbf{g}_{\text{tr}}$.
Предполагается, что наборы данных $\mathfrak{D}_1$ и $\mathfrak{D}_2$ имеют общие свойства.

Функция 
\[
    \boldsymbol{\varphi}: \mathbb{R}^{N_\text{tr}} \rightarrow \mathbb{R}^{N_\text{st}}
\]
отображает параметры модели учителя в начальные параметры модели ученика $\mathbf{w} = \boldsymbol{\varphi}(\hat{\mathbf{u}})$.

\begin{hypothesis}
Модели ученика, инициализированные результатом применения функции $\boldsymbol{\varphi}$ к параметрам предварительно обученной модели учителя, являются более устойчивыми и достигают более высокой точности, чем модели с параметрами по умолчанию.
\end{hypothesis}

Основная проблема предложенного метода заключается в том, что модель учителя $\mathbf{g}_\text{tr}$, обученная на простом наборе данных $\mathfrak{D}_1$, может быть намного проще, чем модель ученика $\mathbf{f}_\text{st}$. Для использования большего объема информации из параметров модели учителя $\hat{\mathbf{u}}$ требуется расширить размерность пространства параметров модели учителя ${N_\text{tr}}$ до размерности ${N_\text{st}}$ пространства параметров модели ученика.

Для решения данной проблемы оптимизируется следующая составная функция потерь:
\begin{equation}\label{phi}
  \boldsymbol{\varphi}(\mathbf{u}) = \underset{\mathbf{w} \in \mathbb{R}^{N_\text{st}}}{\arg\min}~\mathcal{L}(\mathbf{w}),
\end{equation}
где 
\[
    \mathcal{L}(\mathbf{w}) = \lambda_1 \mathcal{L}_\text{ce}(\mathbf{w}, \mathfrak{D}_1) + \lambda_2 \mathcal{L}_2 (\mathbf{w}, \mathbf{u}) + \lambda_3 \mathcal{L}_3^\delta (\mathbf{w}, \mathfrak{D}_1) + \lambda_4 \mathcal{L}_4 (\mathbf{w}),
\]
\[
    \forall i \in \overline{1, 4} \; \lambda_i \ge 0
\]
Первое слагаемое $\mathcal{L}_\text{ce}(\mathbf{w}, \mathfrak{D}_1)$ представляет собой перекрестную энтропию, отвечающую за качество модели ученика на простом наборе данных $\mathfrak{D}_1$. Это слагаемое обеспечивает сохранение способности модели ученика решать исходную задачу классификации.

Второе слагаемое
\[
    \mathcal{L}_2 (\mathbf{w}, \mathbf{u}) = \|\textbf{u} - \textbf{Pr}[\textbf{w}]\|^2_2
\]
обеспечивает близость параметров модели учителя и модели ученика в соответствующих местах, где $\textbf{Pr}$ --- оператор проекции, выбирающий только первые параметры, общие для обеих моделей. В случае моделей многослойного перцептрона оператор $\textbf{Pr}$ выбирает параметры тех же нейронов для каждого слоя модели. Данное слагаемое способствует передаче знаний от модели учителя к модели ученика через близость параметров.

Третье слагаемое
\[
    \mathcal{L}_3^\delta (\mathbf{w}, \mathfrak{D}_1) = \displaystyle \sum \limits_{(\textbf{x}, y) \in \mathfrak{D}_1} \displaystyle \mathbb{E}_{\textbf{x}' \in U_\delta(\textbf{x})} \mathcal{L}_\text{ce}(\mathbf{w}, \textbf{x}', y)
\]
отвечает за устойчивость решения к шуму во входных данных, где $U_\delta(\textbf{x})$ представляет равномерное распределение в окрестности $[\delta - \textbf{x}, \delta + \textbf{x}]$. Данное слагаемое обеспечивает сглаживание функции потерь в окрестности обучающих примеров, что повышает устойчивость модели к небольшим искажениям входных данных.

Четвертое слагаемое
\[
    \mathcal{L}_4 (\mathbf{w}) = \text{tr} \left(\displaystyle \frac{\partial^2 \mathcal{L}_\text{ce}}{\partial \mathbf{w}^2}\right)
\]
выполняет регуляризацию гессиана функции потерь, что способствует нахождению решений в более плоских областях ландшафта функции потерь и повышает устойчивость модели к возмущениям параметров.

Последнее слагаемое $\mathcal{L}_4$ включает вычисление гессиана, наивное вычисление которого может быть ресурсоемким.
Для эффективного вычисления следа гессиана используется метод стохастической аппроксимации~\cite{bai1996some} с быстрым умножением гессиан-вектор~\cite{pearlmutter1994fast}. Сложность такой процедуры линейна от количества параметров модели ученика $\mathbf{f}_\text{st}$.

В интересующем нас случае анти-дистилляции подразумевается $\lambda_2 > 0$, то есть оптимизация, делающая параметры модели учителя и ученика достаточно близкими. Это обеспечивает передачу знаний от модели учителя к модели ученика через близость параметров.

Важным свойством модели является устойчивость к искажению входных данных. Для обеспечения этого свойства используются слагаемые $\mathcal{L}_3$ и $\mathcal{L}_4$. Оба этих слагаемых регулируют гессиан функции перекрестной энтропии~\cite{yao2020pyhessian,chen2020stabilizing}, способствуя нахождению решений в более плоских областях ландшафта функции потерь.

\section{Результаты вычислительных экспериментов}

В настоящем разделе представлены результаты вычислительных экспериментов для методов снижения сложности моделей, описанных в предыдущих разделах главы. Эксперименты направлены на валидацию предложенных методов и сравнение их эффективности с существующими подходами.

\subsection{Удаления параметров моделей глубокого обучения}

Для анализа свойств предложенного алгоритма и сравнения его с существующими подходами проведен вычислительный эксперимент, в котором параметры нейросети удалялись методами, описанными в разделах 3.1---3.3, и методом Белсли.

В качестве данных использовались три выборки. Выборки Wine~\cite{Wine} и Boston Housing~\cite{boston1978} представляют собой реальные данные, широко используемые в задачах машинного обучения. Синтетические данные сгенерированы таким образом, чтобы параметры сети были мультиколлинеарными, что позволяет проверить эффективность метода Белсли в условиях, для которых он был разработан. Генерация синтетических данных состояла из двух этапов. 
На первом этапе генерировался вектор параметров $\mathbf{w}_{\text{synthetic}}$:
\[
    \mathbf{w}_{\text{synthetic}}  \sim \mathcal{N}(\textbf{m}_{\text{synthetic}}, \textbf{A}_{\text{synthetic}}),
\]
где 
$\textbf{m}_{\text{synthetic}} = \begin{bmatrix}
1.0\\
0.0025\\
\ldots\\
0.0025
\end{bmatrix}$,
$\textbf{A}_{\text{synthetic}} = \begin{bmatrix}
1.0& 10^{-3}& \ldots& 10^{-3}& 10^{-3}\\
10^{-3}& 1.0& \ldots& 0.95& 0.95\\
\ldots&\ldots&\ldots&\ldots&\ldots\\
10^{-3}& 0.95& \ldots& 0.95& 1.0
\end{bmatrix}$.

На втором этапе генерировалась выборка $\mathfrak{D}_{\text{synthetic}}$:
\[
    \mathfrak{D}_{\text{synthetic}} = \{(\textbf{x}_i,y_i)| \textbf{x}_i \sim  \mathcal{N}(\textbf{1}, \textbf{I}), y_i = x_{i0}, i = 1 \ldots 10000\}.
\]
В приведенном выше векторе параметров $\mathbf{w}_{\text{synthetic}}$ для выборки $\mathfrak{D}_{\text{synthetic}}$ наиболее релевантным является первый параметр, а все остальные параметры нерелевантны. Матрица ковариации выбрана таким образом, чтобы все нерелевантные параметры были зависимыми величинами, что приводит к максимальной эффективности метода Белсли.

\begin{table}[h!t]

\begin{center}
\caption{Характеристики выборок, использованных для анализа метода задания порядка параметров методом Белсли. Включает реальные данные (Wine для классификации, Boston Housing для регрессии) и синтетические данные с мультиколлинеарными параметрами.}
\begin{tabular}{|c|c|c|c|}
\hline
	Выборка &Тип задачи& Размер выборки& Число признаков\\
	\hline
	
	\multicolumn{1}{|l|}{Wine}
	&
	\multicolumn{1}{|l|}{класификация}
	 & 178 & 13\\
	\hline
	
	\multicolumn{1}{|l|}{Boston Housing}
	&
	\multicolumn{1}{|l|}{регресия}
	& 506 & 13\\
	\hline
	
	\multicolumn{1}{|l|}{Synthetic data}
	&
	\multicolumn{1}{|l|}{регресия}
	& 10000 & 100\\
\hline

\end{tabular}
\end{center}
\end{table}


Для всех алгоритмов тренировочная и тестовая выборки составили $80\%$ и $20\%$ соответственно. Критерием качества прореживания служит процент параметров нейросети, удаление которого не влечет значимой потери качества прогноза. Дополнительным критерием качества служит устойчивость нейросети к зашумленности данных. 

Качеством прогноза $R_{\text{cl}}$ модели для задачи классификации является точность прогноза модели:
\[
R_{\text{cl}} = \frac{\sum_{(\textbf{x},y)\in \mathfrak{D}} [f(\textbf{x}, \textbf{w}) = y]}{\left|\mathfrak{D}\right|},
\]
где $[\cdot]$~--- индикаторная функция.

Качеством прогноза $R_{\text{rg}}$ модели для задачи регрессии является среднеквадратическое отклонение результата модели от точного:
\[
R_{\text{rg}} = \frac{\sum_{(\textbf{x},y)\in \mathfrak{D}} \left(f(\textbf{x}, \textbf{w}) - y\right)^2}{\left|\mathfrak{D}\right|}.
\]

Для эксперимента на выборке Wine~\cite{Wine} рассматривается нейронная сеть с 13 нейронами на входе, 13 нейронами в скрытом слое и 3 нейронами на выходе.

\begin{figure}[h!t]\center
    \includegraphics[width=0.8\textwidth]{thesis/figures/chapter-4/belsli/Wine/All.pdf}\\
    \caption{Зависимость точности классификации $R_{\text{cl}}$ от процента удаленных параметров для различных методов прореживания на выборке Wine. Метод оптимального прореживания, вариационный метод и метод Белсли позволяют удалить $\approx80\%$ параметров без существенной потери качества.}
    \label{WineAll}
\end{figure}

\begin{figure}[h!t]\center
    \subfloat[Произвольное удаление параметров]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Wine/RandomNoise3D.pdf}}
    \subfloat[Оптимальное прореживание]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Wine/OBDNoise3D.pdf}}\\
    \subfloat[Вариационный метод]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Wine/VariationalNoise3D.pdf}}
    \caption{Поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для различных методов прореживания на выборке Wine. Метод Белсли демонстрирует наименьший уровень шума, что видно по более низкому положению соответствующей поверхности.}
    \label{WineNoise}
\end{figure}

На рис.~\ref{WineAll} показана зависимость точности прогноза $R_{\text{cl}}$ от процента удаленных параметров для различных методов прореживания. Результаты демонстрируют, что метод оптимального прореживания, вариационный метод и метод Белсли позволяют удалить до $\approx80\%$ параметров без существенной потери качества. При удалении $\approx90\%$ параметров качество всех методов начинает снижаться, что указывает на достижение предела эффективности прореживания для данной архитектуры. 

На рис.~\ref{WineNoise} показаны поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для разных методов прореживания. Анализ поверхностей показывает, что метод Белсли демонстрирует наименьший уровень шума в выходных данных по сравнению с другими методами, на что указывает более низкое положение соответствующей поверхности. Это свидетельствует о лучшей устойчивости модели, полученной методом Белсли, к зашумленности входных данных.

Для эксперимента на выборке Boston Housing~\cite{boston1978} рассматривается нейронная сеть с 13 нейронами на входе, 39 нейронами в скрытом слое и одним нейроном на выходе.

\begin{figure}[h!t]\center
    \includegraphics[width=0.8\textwidth]{thesis/figures/chapter-4/belsli/Boston/All.pdf}\\
    \caption{Зависимость среднеквадратического отклонения прогноза $\mathsf{R}_{\text{rg}}$ от процента удаленных параметров для различных методов прореживания на выборке Boston Housing. Метод Белсли является наиболее эффективным, позволяя удалить больше параметров без потери качества.}
    \label{BostonAll}
\end{figure}

\begin{figure}[h!t]\center
    \subfloat[Произвольное удаление параметров]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Boston/RandomNoise3D.pdf}}
    \subfloat[Оптимальное прореживание]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Boston/OBDNoise3D.pdf}}\\
    \subfloat[Вариационный метод]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Boston/VariationalNoise3D.pdf}}
    \caption{Поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для различных методов прореживания на выборке Boston Housing. Все методы демонстрируют одинаковый уровень шума, так как соответствующие поверхности находятся на одном уровне.}
    \label{BostonNoise}
\end{figure}

На рис.~\ref{BostonAll} показана зависимость среднеквадратического отклонения прогноза $\mathsf{R}_{\text{rg}}$ от точного ответа от процента удаленных параметров для различных методов. График демонстрирует, что метод Белсли является более эффективным, чем другие методы, поскольку позволяет удалить больше параметров нейросети без потери качества.

На рис.~\ref{BostonNoise} показаны поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для разных методов прореживания. Анализ показывает, что уровень шума всех методов примерно одинаковый, поскольку соответствующие поверхности находятся на близком уровне. Это указывает на то, что для данной задачи регрессии устойчивость к шуму во входных данных не зависит существенно от выбранного метода прореживания.


Для эксперимента на синтетических данных рассматривается нейронная сеть с 100 нейронами на входе и одним нейроном на выходе.

\begin{figure}[h!t]\center
    \includegraphics[width=0.8\textwidth]{thesis/figures/chapter-4/belsli/Data1/All.pdf}\\
    \caption{Зависимость среднеквадратического отклонения прогноза от процента удаленных параметров для различных методов прореживания на синтетической выборке с мультиколлинеарными параметрами. Метод Белсли является наиболее эффективным, поскольку качество прогноза повышается при удалении шумовых параметров.}
    \label{Data1All}
\end{figure}

\begin{figure}[h!t]\center
    \subfloat[Произвольное удаление параметров]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Data1/RandomNoise3D.pdf}}
    \subfloat[Оптимальное прореживание]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Data1/OBDNoise3D.pdf}}\\
    \subfloat[Вариационный метод]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Data1/VariationalNoise3D.pdf}}
    \caption{Поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для различных методов прореживания на синтетической выборке. Метод Белсли демонстрирует наименьший уровень шума, что видно по более низкому положению соответствующей поверхности.}
    \label{Data1Noise}
\end{figure}

На рис.~\ref{Data1All} показана зависимость среднеквадратического отклонения прогноза от точного ответа от процента удаленных параметров для различных методов. График демонстрирует, что удаление параметров методом Белсли является более эффективным, чем другие методы прореживания, поскольку качество прогноза нейросети повышается при удалении шумовых параметров.

На рис.~\ref{Data1Noise} показаны поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для разных методов прореживания. Анализ показывает, что метод Белсли демонстрирует наименьший уровень шума, поскольку соответствующая поверхность находится ниже других поверхностей. Это подтверждает эффективность метода Белсли для работы с мультиколлинеарными параметрами, что соответствует его теоретическому обоснованию.

\subsection{Дистилляция моделей глубокого обучения на многодоменных данных}

В настоящем подразделе представлены результаты вычислительных экспериментов для метода мультидоменной дистилляции, описанного в разделе~\ref{chapter:pruning}. Цель вычислительного эксперимента~--- сравнить производительность моделей учителя и ученика на реальных наборах данных с использованием отображения $\varphi$ и без него для задач компьютерного зрения и обработки естественного языка. Для анализа качества дистилляции используется интегральный критерий качества~\cite{grabovoi2022probabilistic609999418}.

Эксперименты проводятся на двух типах данных:
\begin{itemize}
    \item Подмножество ImageNet~\cite{imagenet} --- набор изображений для задачи классификации на 10 классов. Набор данных состоит из обучающей и тестовой частей, причем обучающая часть разделена на мультиресурсную и малоресурсную части, что позволяет моделировать сценарий передачи знаний от модели, обученной на большом наборе данных, к модели, обучаемой на малом наборе данных.
    \item OPUS-100~\cite{opus100} --- англоцентричный набор данных для машинного перевода, в котором все обучающие пары включают английский язык на стороне источника или цели. Используются языковые пары fr-en и de-en, что позволяет проверить эффективность мультидоменной дистилляции при работе с различными языковыми парами.
\end{itemize}

\paragraph{Конфигурация алгоритма многодоменной дистилляции для задачи компьютерного зрения.}


\begin{table}[h!t]
\begin{center}
\caption{Архитектура модели учителя для эксперимента по мультидоменной дистилляции в задаче компьютерного зрения. Модель представляет собой сверточную нейронную сеть с пятью сверточными слоями и четырьмя полносвязными слоями, общее число параметров составляет 4 455 984.}
\label{table_1}
\begin{tabular}{|c|c|c|}
\hline
	Слой & Размер входного вектора & Количество параметров\\
	\hline
	\multicolumn{1}{|l|}{Входной слой}
	& (3, 200, 200) & 0 \\
	\hline
	\multicolumn{1}{|l|}{CONV1 (ядро=5)}
	& (24, 196, 196) & 1800 \\
	\hline
	\multicolumn{1}{|l|}{POOL1}
	& (24, 98, 98) & 0 \\
	\hline
	\multicolumn{1}{|l|}{CONV2 (ядро = 5)}
	& (48, 94, 94) & 28800 \\
	\hline
	\multicolumn{1}{|l|}{POOL2}
	& (48, 47, 47) & 0 \\
	\hline
	\multicolumn{1}{|l|}{CONV3 (ядро = 8)}
	& (96, 40, 40) & 294912 \\
	\hline
	\multicolumn{1}{|l|}{POOL3}
	& (96, 20, 20) & 0 \\
	\hline
    \multicolumn{1}{|l|}{CONV4 (ядро = 5)}
	& (192, 16, 16) & 460800 \\
	\hline
	\multicolumn{1}{|l|}{POOL4}
	& (192, 8, 8) & 0 \\
    \hline
	\multicolumn{1}{|l|}{CONV5 (ядро = 7)}
	& (384, 2, 2) & 3612672 \\
	\hline
	\multicolumn{1}{|l|}{POOL5}
	& (384, 1, 1) & 0 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (384) & 0 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (120) & 46080 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (84) & 10080 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (10) & 840 \\
	\hline
	\multicolumn{1}{|l|}{}
	& & $\sum=4455984$ \\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[h!t]
\begin{center}
\caption{Архитектура модели ученика для эксперимента по мультидоменной дистилляции в задаче компьютерного зрения. Модель имеет упрощенную структуру по сравнению с моделью учителя, с двумя сверточными слоями и тремя полносвязными слоями, общее число параметров составляет 12 755 640.}
\label{table_1_1}
\begin{tabular}{|c|c|c|}
\hline
	Слой & Размер входного вектора & Количество параметров\\
	\hline
	\multicolumn{1}{|l|}{Входный слой}
	& (3, 200, 200) & 0 \\
	\hline
	\multicolumn{1}{|l|}{CONV1 (ядро=5)}
	& (24, 196, 196) & 1800 \\
	\hline
	\multicolumn{1}{|l|}{POOL1}
	& (24, 98, 98) & 0 \\
	\hline
	\multicolumn{1}{|l|}{CONV2 (ядро = 5)}
	& (48, 94, 94) & 28800 \\
	\hline
	\multicolumn{1}{|l|}{POOL2}
	& (48, 47, 47) & 0 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (106032) & 0 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (120) & 12723840 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (10) & 1200 \\
	\hline
	\multicolumn{1}{|l|}{}
	& & $\sum=12755640$ \\
\hline
\end{tabular}
\end{center}
\end{table}


Структуры модели учителя $\mathbf{f}$ и модели ученика $\mathbf{g}$ описаны в таблице~\ref{table_1} и таблице~\ref{table_1_1}. Функция активации после каждого скрытого слоя~--- ReLu. 
Для решения задачи оптимизации используется метод градиентной оптимизации Adam~\cite{adam2015}.

\begin{table}[h!t]
\begin{center}
\caption{Характеристики подмножеств набора данных ImageNet, использованных в эксперименте по мультидоменной дистилляции. ImageNet-Big содержит 1 281 167 изображений для обучения и 50 000 для валидации, ImageNet-Small --- 64 058 и 2 500 соответственно, оба набора содержат 200 классов.}
\label{table_2}
\begin{tabular}{|c|c|c|}
\hline
	Набор данных & Описание & Размер набора\\
	\hline
	\multicolumn{1}{|l|}{ImageNet-Train}
	& Обучающая часть& 9469\\
	\hline
	\multicolumn{1}{|l|}{ImageNet-Big}
	& Мультиресурсная часть& 8469\\
	\hline
	\multicolumn{1}{|l|}{ImageNet-Small}
	& Малоресурсная часть& 1000\\
	\hline
	\multicolumn{1}{|l|}{ImageNet-Test}
	& Тестовая часть& 3925\\
\hline
\end{tabular}
\end{center}
\end{table}

В таблице~\ref{table_2} описаны наборы данных для вычислительного эксперимента по компьютерному зрению. Каждый из наборов данных состоит из обучающей и тестовой части, при этом обучающая часть разделена на мультиресурсную и малоресурсную части. Обучающая часть содержит 9 469 объектов, мультиресурсная часть содержит 8 469 объектов, малоресурсная часть содержит 1 000 объектов, а тестовая часть содержит 3 925 объектов.

Цель эксперимента~--- сравнить производительность модели ученика, обучающейся без учителя, с учителем и с учителем на другом домене с использованием адаптации домена.
Экспериментальная процедура включает три этапа. На первом этапе обучается модель ученика на малоресурсной части и происходит ее тестирование на тестовой части для получения базовой оценки качества. На втором этапе используются метки модели учителя, обученной на мультиресурсной части, для обучения модели ученика. На третьем этапе происходит обучение модели учителя на изображениях из мультиресурсной части и используется модель учителя и отображение для обучения модели ученика. Предобученная модель CycleGAN~\cite{CycleGAN} используется в качестве отображения $\varphi$ для доменной адаптации.

\begin{figure}[htpb]\center
    {\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/multidomain-distillation/Mapping}}
    \caption{Пример применения инъективного отображения $\varphi$ для доменной адаптации между ImageNet-Small и ImageNet-Big. Демонстрирует визуальное преобразование объекта из исходного домена в целевой домен для использования в мультидоменной дистилляции.}
    \label{mapping}
\end{figure}

На рис.~\ref{mapping} показано одно из изображений в наборе данных ImageNet~\cite{imagenet} и то же изображение после преобразования с помощью модели CycleGAN~\cite{CycleGAN}. Результаты усредняются по 5 запускам для вычисления среднего значения и дисперсии метрик.

\paragraph{Конфигурация алгоритма многодоменной дистилляции для задачи обработки естественного языка.}

Набор данных OPUS-100 был разделен следующим образом: обучающая часть для учителя состояла из 5 000 немецко-английских предложений, обучающая часть для модели ученика --- из 2 000 французско-английских предложений, тестовая часть --- из 500 французско-английских предложений. Такое разделение позволяет проверить эффективность мультидоменной дистилляции при работе с различными языковыми парами.

\begin{table}[h!t]
\begin{center}
\caption{Характеристики подмножеств набора данных OPUS-100, использованных в эксперименте по мультидоменной дистилляции для задачи машинного перевода. Показаны размеры обучающих и валидационных выборок для языковых пар fr-en и de-en.}
\label{table_3}
\begin{tabular}{|c|c|c|c|}
\hline
	Набор данных & Описание & Язык & Размер набора\\
	\hline
	\multicolumn{1}{|l|}{Teacher-Train}
	& Обучающая часть модели учителя & de-en & 5000\\
	\hline
	\multicolumn{1}{|l|}{Student-Train}
	& Обучающая часть модели ученика& fr-en & 2000\\
	\hline
	\multicolumn{1}{|l|}{Student-Test}
	& Тестовая часть & fr-en & 500\\
\hline
\end{tabular}
\end{center}
\end{table}

В таблице~\ref{table_3} описаны наборы данных для вычислительного эксперимента по обработке естественного языка.

Использовались модель ученика $\mathbf{g}$ и модель учителя $\mathbf{f}$ в качестве трансформерной модели на основе статьи~\cite{attention} и метод градиентной оптимизации Adam~\cite{adam2015} для решения задачи оптимизации. Модель NLLB~\cite{nllb} использовалась в качестве отображения $\varphi$, переводящего французские предложения в немецкие.

Аналогично эксперименту по компьютерному зрению, сравнивается производительность модели ученика без учителя, с учителем и с учителем и адаптацией домена. Результаты усреднялись по 5 запускам для вычисления среднего значения и дисперсии метрик, что обеспечивает статистическую надежность полученных оценок.
\begin{figure}[htpb]
    \centerline{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/multidomain-distillation/Dist_acc.png}}
    \caption{Зависимость точности аппроксимации от числа эпох обучения на тестовой выборке ImageNet для различных методов дистилляции. Результаты усреднены по 5 запускам и демонстрируют преимущество методов с использованием учителя и доменной адаптации.}
    \label{cv_acc}
\end{figure}

\begin{figure}[htpb]
    \centerline{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/multidomain-distillation/Dist_loss.png}}
    \caption{Зависимость ошибки перекрестной энтропии между истинными и предсказанными метками от числа эпох обучения на тестовой выборке ImageNet для различных методов дистилляции. Результаты усреднены по 5 запускам и показывают снижение ошибки при использовании методов дистилляции.}
    \label{cv_loss}
    \end{figure}

На рис.~\ref{cv_acc} и рис.~\ref{cv_loss} представлены результаты обучения моделей на наборе данных ImageNet. Анализ графиков показывает, что модели, обученные с использованием учителя, достигают лучшего качества и точности по сравнению с моделью, обученной без учителя. Модель ученика, обученная с использованием меток учителя на том же домене, достигает наивысшей точности и наименьших потерь, что ожидаемо, поскольку отсутствует необходимость в адаптации домена. Модель ученика, обученная с использованием меток учителя и адаптации домена, демонстрирует существенное улучшение качества аппроксимации по сравнению с моделью без использования учителя, что подтверждает эффективность предложенного метода мультидоменной дистилляции.

Таким образом, экспериментально показано, что дистилляция с использованием адаптации домена приводит к более эффективным нейронным сетям с меньшим количеством параметров, сохраняя при этом высокое качество аппроксимации.


\begin{table}[h!t]
\begin{center}
\caption{Сравнение качества моделей для задачи компьютерного зрения на наборе данных ImageNet. Показаны валидационная точность, потери и интегральный критерий для различных комбинаций учителя и ученика с использованием доменной адаптации и без нее, результаты усреднены по нескольким запускам.}
\label{table_4}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
	Ученика & Учитель & Отображение $\varphi$ & Точность & \begin{tabular}[c]{@{}c@{}}Потери перекрестной\\ энтропии\end{tabular} & \begin{tabular}[c]{@{}c@{}}Интегральный\\ критерий\end{tabular}\\
	\hline
	\multicolumn{1}{|l|}{ImageNet-Small}
	& --- & --- & $0{,}34 \pm 0{,}01$ & $4{,}41 \pm 1{,}10$ & $53{,}89 \pm 14{,}99$ \\
    \hline

    \multicolumn{1}{|l|}{ImageNet-Small}
	& ImageNet-Big & StyleTransfer & $0{,}37 \pm 0{,}01$ & $\textbf{2{,}01} \pm \textbf{0{,}03}$ & $28{,}30 \pm 0{,}79$ \\
\hline

    
    \hline
    \multicolumn{1}{|l|}{ImageNet-Small}
	& ImageNet-Big & --- & $\textbf{0{,}44} \pm \textbf{0{,}01}$ & $2{,}03 \pm 0{,}02$ & $\textbf{28{,}08} \pm \textbf{1{,}22}$ \\
    \hline
	
\end{tabular}
}
\end{center}
\end{table}

В таблице~\ref{table_4} представлены количественные результаты эксперимента по компьютерному зрению: валидационная точность, потери и интегральный критерий для моделей, обученных с дистилляцией и адаптацией домена и без них.


\begin{figure}[h!t]\center
    {\includegraphics[width=0.5 \textwidth]{thesis/figures/chapter-4/multidomain-distillation/NLP_loss.png}}
    \caption{Зависимость ошибки перекрестной энтропии от числа эпох обучения на тестовом наборе данных OPUS-100 для задачи машинного перевода. Результаты усреднены по 3 запускам и демонстрируют преимущество моделей, обученных с использованием учителя.}
    \label{nlp_loss}
\end{figure}

На рис.~\ref{nlp_loss} представлена зависимость ошибки перекрестной энтропии от числа эпох обучения для задачи машинного перевода. Анализ показывает, что модели, обученные с использованием учителя, достигают лучшего качества, демонстрируя более быстрое снижение ошибки и более низкие значения потерь по сравнению с моделью, обученной без учителя.

В таблице~\ref{table_5} представлены количественные результаты сравнения моделей ученика, полученных с использованием дистилляции и без нее. Результаты подтверждают эффективность мультидоменной дистилляции для задачи машинного перевода.

\begin{table}[h!t]
\begin{center}
\caption{Сравнение качества моделей для задачи машинного перевода на наборе данных OPUS-100. Показаны потери перекрестной энтропии и метрика BLEU для модели ученика, обученной с использованием учителя и доменной адаптации (NLLB) и без них.}
\label{table_5}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
	Ученик & Учитель & Отображение $\varphi$ & Потери перекрестной энтропии & \begin{tabular}[c]{@{}c@{}}BLEU\end{tabular} \\
\hline
	\multicolumn{1}{|l|}{Student-Train}
	& --- & ---& $5{,}367 \pm 0{,}015$ & $0{,}0282$ \\
    \hline
	\multicolumn{1}{|l|}{Student-Train}
	& Teacher-Train & NLLB & $\textbf{5{,}233} \pm \textbf{0{,}007}$ & $\textbf{0{,}0572}$\\
\hline
\end{tabular}
}
\end{center}
\end{table}



\subsection{Анти-Дистилляция моделей глубокого обучения}

В настоящем подразделе представлены результаты вычислительных экспериментов для метода анти-дистилляции, описанного в разделе~\ref{chapter:pruning}. Цель вычислительного эксперимента~--- сравнить производительность моделей в зависимости от инициализации параметров. 

Производится сравнение различных подходов к инициализации:
\begin{enumerate}
    \item Xavier~--- заполнение всех параметров модели $U[-\frac1{\sqrt{n}}, \frac1{\sqrt{n}}]$, где $n$~--- количество нейронов входного слоя \cite{glorot2010understanding}, т.е. инициализация параметров модели по умолчанию.
    \item Zero pad~--- заполнение расширенных параметров нулями.
    \item Uniform pad~--- заполнение расширенных параметров равномерно распределенными случайными величинами $U[-\frac1{\sqrt{n}}, \frac1{\sqrt{n}}]$, где $n$~--- количество нейронов входного слоя.
    \item Transfer learning~--- взятие предобученной модели и изменение только классификационного слоя для новой задачи классификации. Сначала модель обучалась с замороженными параметрами на всех слоях, кроме классификационного. После 3 эпох обучения все параметры размораживались. Начиная с четвертой эпохи, оптимизировались все параметры нейронной сети.
    \item Net2Net~--- инкрементальный алгоритм расширения пространства параметров модели\cite{net2net}.
    \item With Data Noise~--- получение инициализации модели ученика путем решения задачи оптимизации \ref{phi} с $\lambda_1, \lambda_3 = 1$ и $\lambda_2, \lambda_4 = 0$.
    \item Anti-Distillation, $\lambda_4 = 0$~--- инициализация методом Анти-Дистилляции с оптимизацией гиперпараметров $\lambda_1, \lambda_2, \lambda_3$ с помощью байесовской оптимизации ($\lambda_4 = 0$) \cite{akiba2019optuna}.
    \item Anti-Distillation~--- оптимизация всех $\lambda_i$.
\end{enumerate}

Критериями качества являются: точность на валидационном наборе, искаженном атакой FSGM~\cite{goodfellow2014explaining}, и точность на валидационном наборе при условии, что параметры модели искажены шумом: ${\mathbf{w}_\varepsilon} = \mathbf{w} + \varepsilon \boldsymbol{\xi}$, где $\boldsymbol{\xi} \sim \mathcal{N} (\mathbf{0}, \mathbf{I})$.


В качестве набора данных используется Fashion-MNIST~\cite{fashionmnist} --- набор данных изображений статей Zalando, состоящий из обучающего набора из 60 000 примеров и тестового набора из 10 000 примеров. Каждый пример представляет собой полутоновое изображение размером $28 \times 28$ пикселей, связанное с меткой из 10 классов.

Экспериментальная процедура включает следующие этапы: обучение модели учителя, увеличение ее сложности и сравнение различных способов инициализации параметров модели.

Рассматривается модель полносвязной сети. Модель учителя имеет следующие размеры скрытых слоев: $[128, 64, 32]$. Модель ученика имеет $[256, 128, 64]$ нейронов в скрытых слоях.

Модель учителя обучалась в течение 30 эпох с начальной скоростью обучения $10^{-2}$, уменьшавшейся до $10^{-3}$ после 10 эпох.
Модели ученика сравнивались при обучении в течение 10 эпох со скоростью обучения $10^{-3}$.
Оптимизация проводится с использованием алгоритма оптимизации Adam~\cite{adam2015}. Методы инициализации сравниваются по следующим критериям: точность предсказаний на валидационной выборке, значение функции потерь перекрестной энтропии и дисперсия предсказаний. Дополнительно исследуется устойчивость методов к зашумленным входным данным, для чего указанные критерии качества анализируются в зависимости от процента искаженных изображений.

\begin{figure}[!t]
\centering
  \includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/anti-distillation/file}
 \caption{Сравнение валидационной точности для различных методов инициализации параметров модели на наборе данных Fashion-MNIST. Модели, использующие Анти-Дистилляцию, демонстрируют меньшую дисперсию и более высокую точность по сравнению с моделями с различной инициализацией.}
  \label{fig:1}
\end{figure}
\begin{figure}[!t]
\centering
  \includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/anti-distillation/fsgm}
 \caption{Зависимость валидационной точности от уровня адверсарного шума в данных для различных методов инициализации на наборе данных Fashion-MNIST. Анти-Дистилляция является наиболее устойчивым методом, демонстрируя наивысшую точность при высоких уровнях шума.}
  \label{fig:2}
\end{figure}
\begin{figure}[!t]
\centering
  \includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/anti-distillation/noise}
 \caption{Зависимость валидационной точности от параметра интенсивности нормального шума $\varepsilon$ в параметрах модели для различных методов инициализации на наборе данных Fashion-MNIST. Метод Анти-Дистилляции без регуляризации гессиана ($\lambda_4=0$) является наиболее устойчивым, сохраняя наивысшую точность при максимальном уровне шума.}
  \label{fig:3}
\end{figure}

Набор данных $\mathfrak{D}_2$ состоит из Fashion-MNIST, а $\mathfrak{D}_1 = \{(\textbf{x}, y) \;|\; (\textbf{x}, y) \in \mathfrak{D}_2, y \in C_1\}$, где $C_1 \subset C_2$, $C_1 = \{0, \dots 4\}$, $C_2 = \{0, \dots 9\}$. Таким образом, набор данных $\mathfrak{D}_1$ представляет собой подмножество $\mathfrak{D}_2$, содержащее только первые пять классов, что позволяет моделировать сценарий перехода от простой задачи к более сложной. 


На рис.~\ref{fig:1} представлено сравнение валидационной точности для различных методов инициализации параметров модели. Анализ результатов показывает, что модели, использующие анти-дистилляцию, в среднем имеют меньшую дисперсию и более высокую точность, чем модели с различной инициализацией параметров. Обучение модели с нуля оказалось наименее эффективным решением. Предложенный метод анти-дистилляции обеспечивает лучшие результаты с меньшим количеством итераций для сходимости, что указывает на эффективность использования информации от предобученной модели учителя.

В представленных экспериментах не учитывалось количество итераций, необходимых для расширения модели учителя, которое также требует процедуры оптимизации. Предполагается, что во многих реальных случаях этим временем можно пренебречь, поскольку предложенный метод позволяет расширить модель учителя один раз, используя только базовый набор данных $\mathfrak{D}_1$, для последующего использования в множественных задачах обучения модели ученика~\cite{sun2019meta}.

На рис.~\ref{fig:2} представлена зависимость валидационной точности от уровня адверсарного шума в данных. Анализ показывает, что Анти-Дистилляция является наиболее устойчивым к адверсарным атакам методом инициализации параметров модели, поскольку демонстрирует наивысшую валидационную точность с большим отрывом при высоких уровнях шума.

На рис.~\ref{fig:3} представлена зависимость валидационной точности от уровня нормального шума в параметрах модели. Анализ показывает, что метод Анти-Дистилляции без регуляризации гессиана ($\lambda_4=0$) является наиболее устойчивым к нормальному шуму в параметрах модели, поскольку сохраняет наивысшую точность при максимальном рассматриваемом уровне шума.

\begin{table}[!h]
\caption{\label{acc_tab} Сравнение точности на валидационном наборе Fashion-MNIST для различных методов инициализации параметров модели. Показаны базовая точность, точность при адверсарной атаке FSGM и точность при нормальном шуме в параметрах, результаты усреднены по нескольким запускам.}

 \begin{tabular}{|c|c|c|c|}
        \hline
        Метод инициализации & Точность &  Атака FSGM & 
Шум в параметрах \\
        \hline
        Xavier & 0.68 $\pm$ 0.08 & 0.42 $\pm$ 0.04 & 0.58 $\pm$ 0.06\\
    \hline
Zero Pad & \textbf{0.86 $\pm$ 0.02} & 0.50 $\pm$ 0.01 & 0.71 $\pm$ 0.03\\
\hline
Uniform Pad & 0.85 $\pm$ 0.04 & 0.52 $\pm$ 0.03 & \textbf{0.73 $\pm$ 0.03}\\
\hline
Transfer Learning & 0.74 $\pm$ 0.09 & 0.50 $\pm$ 0.06 & 0.53 $\pm$ 0.05\\
\hline
Net2Net & 0.85 $\pm$ 0.04 & 0.51 $\pm$ 0.02 & 0.70 $\pm$ 0.03\\
\hline
With Data Noise & 0.81 $\pm$ 0.07 & 0.51 $\pm$ 0.03 & 0.70 $\pm$ 0.05\\
\hline
Anti-Distillation, $\lambda_4$=0 & \textbf{0.86 $\pm$ 0.05} & 0.53 $\pm$ 0.03 & \textbf{0.73 $\pm$ 0.04}\\
\hline
Anti-Distillation & \textbf{0.86 $\pm$ 0.05} & \textbf{0.57 $\pm$ 0.03} & 0.67 $\pm$ 0.03\\
\hline
\end{tabular}

\end{table}

В таблице~\ref{acc_tab} представлены количественные результаты сравнения методов инициализации: валидационная точность после последней эпохи обучения, точность при наивысшем уровне шума изображения от адверсарной атаки FSGM и точность при наивысшем уровне шума в параметрах модели.

\section{Заключение по главе}

В настоящей главе рассмотрены методы снижения сложности параметрических моделей глубокого обучения, опирающиеся на теоретический аппарат, введенный в главах~\ref{chapter:complexity} и~\ref{chapter:gesian}. Предложены три класса методов: удаление параметров на основе анализа ковариационной матрицы градиентов, мультидоменная дистилляция и анти-дистилляция. Все методы направлены на практическое применение теории сложности моделей для уменьшения числа параметров при сохранении качества и повышении устойчивости моделей.

В разделе об удалении параметров предложен метод, основанный на анализе ковариационной матрицы $\textbf{C}$ градиентов функции ошибки по параметрам модели, вычисляемой итеративно в процессе оптимизации. Метод позволяет упорядочить параметры по их важности на основе диагональных элементов ковариационной матрицы и последовательно удалять наименее значимые параметры без существенной потери качества модели. Дополнительно разработан метод, основанный на модификации метода Белсли, использующий сингулярное разложение ковариационной матрицы и анализ дисперсионных долей для выявления мультиколлинеарных параметров. Экспериментальные результаты на наборах данных Wine, Boston Housing и синтетических данных с мультиколлинеарными параметрами продемонстрировали эффективность предложенных подходов. Метод оптимального прореживания, вариационный метод и метод Белсли позволяют удалить до $\approx80\%$ параметров без существенной потери качества, при этом метод Белсли показал наилучшие результаты по устойчивости к шуму во входных данных.

Мультидоменная дистилляция решает задачу передачи знаний между моделями, обученными на данных из различных доменов, связанных инъективным отображением $\varphi$. Предложена функция потерь, учитывающая как метки учителя, так и связь между доменами через композицию $(f \circ \varphi)$, что позволяет эффективно использовать информацию от модели учителя, обученной на другом домене. Эксперименты в области компьютерного зрения на наборе данных ImageNet подтвердили улучшение качества аппроксимации модели ученика при использовании доменной адаптации, при этом обучение в рамках одного домена показало лучшие результаты, однако адаптация домена обеспечила значительное улучшение по сравнению с базовыми подходами. Эксперименты в области обработки естественного языка на наборе данных OPUS-100 для задачи машинного перевода продемонстрировали снижение ошибки перекрестной энтропии и увеличение метрики BLEU при использовании мультидоменной дистилляции.

Анти-дистилляция решает обратную задачу по сравнению с классической дистилляцией: передачу знаний от простой модели к более сложной для работы с более сложными наборами данных. Предложена составная функция потерь, включающая четыре компонента: перекрестную энтропию на простых данных, регуляризацию близости параметров учителя и ученика, регуляризацию устойчивости к шуму во входных данных и регуляризацию гессиана функции потерь. Для эффективного вычисления следа гессиана использован метод стохастической аппроксимации с быстрым умножением гессиан-вектор, обеспечивающий линейную сложность от числа параметров. Эксперименты на наборе данных Fashion-MNIST продемонстрировали, что модели, инициализированные методом анти-дистилляции, достигают валидационной точности $0{,}86 \pm 0{,}05$, что превосходит результаты методов Xavier ($0{,}68 \pm 0{,}08$), Transfer Learning ($0{,}74 \pm 0{,}09$) и других базовых подходов. Кроме того, анти-дистилляция показала наивысшую устойчивость к адверсарным атакам FSGM (точность $0{,}57 \pm 0{,}03$ при высоких уровнях шума) и к нормальному шуму в параметрах модели (точность $0{,}73 \pm 0{,}04$ при максимальном уровне шума для варианта без регуляризации гессиана).

Рассмотренные методы открывают перспективы для практического применения в условиях ограниченных данных и необходимости адаптации моделей к новым доменам. Разработанные подходы обеспечивают эффективное снижение сложности моделей при сохранении их качества и повышении устойчивости, что особенно важно для развертывания моделей в ресурсно-ограниченных средах. Дальнейшие исследования будут направлены на интеграцию байесовских методов дистилляции, учитывающих распределения параметров, на применение разработанных подходов к другим архитектурам нейронных сетей и наборам данных, а также на теоретический анализ связи между предложенными методами и ландшафтной мерой сложности, введенной в главе~\ref{chapter:complexity}.