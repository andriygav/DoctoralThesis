В данной главе рассмотрим методы снижения сложности параметрических моделей глубокого обучения.
Предполагается, что число параметров нейросети можно существенно снизить без значимой потери качества и значимого повышения дисперсии функции ошибки.
Предлагаются методы снижения сложности моделей на основе ковариационной матрицы градиентов функции ошибки по параметрам модели.

\section{Удаления параметров моделей глубокого обучения}

Задана выборка:
\[
    \label{ch-5:eq:st:1}
    \begin{aligned}
    \mathfrak{D} = \bigr\{\bigr(\textbf{x}_i, y_i\bigr)\bigr\}_{i=1}^{m}, \quad \textbf{x}_{i} \in \mathbb{X} = \mathbb{R}^{n}, \quad y_i \in \mathbb{Y},
    \end{aligned}
\]
где $n$~--- размерность признакового пространства, $m$~--- число объектов в выборке. Пространство ответов $\mathbb{Y} = \mathbb{R}$ в случае задачи регрессии и  $\mathbb{Y} = \{1,\ldots, R\}$ в случае задачи классификации, где $R$~--- число классов.

Задано семейство моделей параметрических функций с наперед заданной структурой:
\[
    \label{ch-5:eq:st:2}
    \begin{aligned}
    \mathfrak{F} &= \bigr\{f\bigr(\textbf{w}\bigr):\mathbb{X} \to \mathbb{Y} | \textbf{w} \in \mathbb{R}^{p}\bigr\}, \\ 
    \mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr) &= \textbf{W}_1\bm{\sigma}\bigr(\textbf{W}_2\bm{\sigma}\bigr(\ldots\bm{\sigma}\bigr(\textbf{W}_r\textbf{x}\bigr)\ldots\bigr)\bigr),\\
    f_{\text{\text{cl}}}\bigr(\textbf{w}, \textbf{x}\bigr) &= \arg \max_{j \in \bigr\{1,\ldots, R\bigr\}} \text{softmax}\bigr(\mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr)\bigr)_{j}, \\ 
    f_{\text{reg}}\bigr(\textbf{w}, \textbf{x}\bigr) & = \mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr), 
    \end{aligned}
\]
где $p$~--- размерность пространства параметров, $r$~--- число слоев нейросети, $\textbf{w} = \text{vec}[\textbf{W}_1, \textbf{W}_2, \ldots, \textbf{W}_r]$, а $\bm{\sigma}$~--- функция активации. В случае задачи регрессии структура модели имеет вид $f_{\text{\text{reg}}}$, а в случае классификации имеет вид $f_{\text{\text{cl}}}$.
Задана функция потерь:
\[
    \label{ch-5:eq:st:3}
    \begin{aligned}
    \mathcal{L}\bigr(\textbf{w}, \mathfrak{D}\bigr) &= \frac{1}{m}\sum_{i=1}^{m}l\bigr(\textbf{x}_{i}, y_i, \textbf{w}\bigr),\\
    l_{\text{\text{reg}}}\bigr(\textbf{x}, y, \textbf{w}\bigr) &= \bigr(y - f\bigr(\textbf{w}, \textbf{x}\bigr)\bigr)^{2},\\
    l_{\text{\text{cl}}}\bigr(\textbf{x}, y, \textbf{w}\bigr) &= -\sum_{j=1}^{R}\bigr([y = j]\ln\text{softmax}_j\bigr(\mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr)\bigr)\bigr),
    \end{aligned}
\]
где $l_{\text{\text{reg}}}$~--- это функция ошибки на одном элементе для задачи регрессии, $l_{\text{\text{cl}}}$~--- для задачи классификации.
Оптимальный вектор параметров $\hat{\textbf{w}}$ получим минимизацией функции потерь:
\[
    \label{ch-5:eq:st:0:1}
    \begin{aligned}
    \hat{\textbf{w}} = \arg \min_{\textbf{w}\in\mathbb{R}^{p}} \mathcal{L}\bigr(\textbf{w}, \mathfrak{D}\bigr).
    \end{aligned}
\]

Для поиска оптимальных параметров модели используется градиентный метод оптимизации:
\[
    \label{ch-5:eq:st:4}
    \begin{aligned}
    \textbf{w}_{t} = \textbf{w}_{t-1} + \Delta\textbf{w}\bigr(\textbf{g}_{S,t}, \textbf{w}_{t-1}, \textbf{w}_{t-2}, \ldots\bigr), \quad \textbf{g}_{S,t}=\frac{\partial \mathcal{L}\bigr(\textbf{w}_{t}, \textbf{X}_{S}, \textbf{Y}_{S}\bigr)}{\partial \textbf{w}},
    \end{aligned}
\]
где $t$~--- номер итерации, $\textbf{g}_{S,t}$~--- значение градиента на подвыборке размера $S$, $\Delta\textbf{w}$~--- приращение вектора параметров.
 
 
Порядок на множестве параметров модели задается при помощи ковариационной матрицы $\textbf{C}$ градиентов функции ошибки $\mathcal{L}$ по параметрам модели $\textbf{w}$. Для вычисления ковариационной матрицы $\textbf{C}$ используется итерационная формула~\cite{Chunyan2016}, которая вычисляется на каждой итерации \eqref{ch-5:eq:st:4} градиентного метода оптимизации параметров:
\[
    \label{ch-5:eq:st:5}
    \begin{aligned}
    \textbf{C}_t = \bigr(1-\kappa_t\bigr)\textbf{C}_{t-1}+\kappa_t\bigr(\textbf{g}_{1,t}-\textbf{g}_{S,t}\bigr)\bigr(\textbf{g}_{1,t}-\textbf{g}_{S,t}\bigr)^{\mathsf{T}},
    \end{aligned}
\]
 где $t$~--- номер итерации, $\textbf{g}_{S,t}$~--- значение градиента на подвыборке размера $S$, $\textbf{g}_{1,t}$~--- значение градиента на первом элементе подвыборки, $\kappa_t=\frac{1}{t}$~--- параметр сглаживания, $\textbf{C}_0$ инициализируются из равномерного распределения.
 
Пусть известно $t_0$~--- число итераций, после которого все параметры находятся в некоторой локальной окрестности минимума, тогда, как показано в работе~\cite{Chunyan2016}, матрица $\textbf{C}_{t_0}$ аппроксимирует истинную ковариационную матрицу $\textbf{C}$. Ковариационная матрица $\textbf{C}_{t_0}$ используется для упорядочения параметров модели $\textbf{w}_{t_0}$. 
 
Пусть $\mathcal{I}$~---  упорядоченный вектор индексов $[1, 2, \ldots, p]$. Обозначим $\mathcal{I}_{\textbf{w}_{t_0}}$ вектор индексов, порядок которого задан при помощи ковариационной матрицы $\textbf{C}_{t_0}$. 
 
Например, если ковариационная матрица $\textbf{C}_{t_0}$  имеет вид
$$
    \begin{bmatrix}
    0{,}3& 0 & 0\\
    0& 0{,}2 & 0\\
    0& 0 & 0{,}25\\
    \end{bmatrix},
$$
 то вектор индексов $\mathcal{I}_{\textbf{w}_{t_0}} = [3,1,2]$.
 
\paragraph{Фиксация параметров модели в процессе обучения.}
Для фиксации параметров $\textbf{w}_{t_0}$ при помощи вектора индексов $\mathcal{I}_{\textbf{w}_{t_0}}$ используется бинарный вектор $\bm{\alpha}\bigr(\zeta\bigr)$:
\[
    \label{ch-5:eq:st:6}
    \begin{aligned}
    \alpha_i\bigr(\zeta\bigr) = \begin{cases}
       1, &\text{если }\mathcal{I}_{\textbf{w}_{t_0}}[j] \leq \zeta;\\
       0 &\text{иначе},
     \end{cases}
    \end{aligned}
\]
 где $\zeta$~--- число фиксирующих параметров.
 
 Учитывая \eqref{ch-5:eq:st:6}, уравнение \eqref{ch-5:eq:st:4} приводится к виду
\[
    \label{ch-5:eq:st:7}
    \begin{aligned}
    \textbf{w}_{t} = \textbf{w}_{t-1} + \bm{\alpha}\bigr(\zeta\bigr)\cdot\Delta\textbf{w}\bigr(\textbf{g}_{S,t}, \textbf{w}_{t-1}, \textbf{w}_{t-2}, \ldots\bigr),
    \end{aligned}
\]
где $t$~--- номер итерации, $\textbf{g}_{S,t}$~--- значение градиента на подвыборке размера $S$, $\Delta\textbf{w}$~--- приращение вектора параметров. После умножения на бинарный вектор $\bm\alpha$ часть параметров не оптимизируется, что приводит к фиксации параметров.


Предлагается метод основанный на модификации метода Белсли. Пусть $\textbf{w}$~--- вектор параметров доставляющий минимум функционалу потерь $\mathcal{L}$ на  множестве $\mathbb{W_\mathcal{A}}$, а $\textbf{A}_\text{ps}$ соответствующая ему ковариационная матрица.

Выполним сингулярное разложение матрицы
\[
\textbf{A}_\text{ps} = \textbf{U}{\bf\Lambda}\textbf{V}^\mathsf{T}.
\]
Индекс обусловленности $\eta_{j}$ определим как отношение максимального элемента к $j$-му элементу матрицы ${\bf\Lambda}$. Для нахождения мультиколлинеарных признаков требуется найти индекс $\xi$ вида:
\[
\xi = \arg\max_{j\in \mathcal{A}}{\eta_j}.
\]

\begin{figure}[h!t]\center
    \subfloat[Матрица ковариации]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Cov}}
    \subfloat[Дисперсионные доли]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/BelslyImage}}
    \caption{Илюстрация метода Белсли для анализа мультиколлинеарности параметров}
    \label{CovBel}
\end{figure}

\begin{table}[h!t]
    \begin{center}
    \caption{Илюстрация метода Белсли для анализа мультиколлинеарности параметров}
    \begin{tabular}{|c|cccccc|}
    \hline
    $\eta$ & $q_1$& $q_2$& $q_3$& $q_4$& $q_5$& $q_6$\\
    \hline
    $1.0$ &  $2\cdot 10^{-17}$ &  $4\cdot 10^{-17}$ &  $1\cdot 10^{-16}$ &  $2\cdot 10^{-17}$ &  $6\cdot 10^{-17}$&  $3\cdot 10^{-4}$ \\
    \hline
    $1.5$ &  $5\cdot 10^{-17}$ &  $9\cdot 10^{-17}$ &  $2\cdot 10^{-16}$ &  $5\cdot 10^{-17}$ &  $3\cdot 10^{-20}$ &  $3\cdot 10^{-2}$ \\
    \hline
    $3.3$ &  $9\cdot 10^{-18}$ &  $1\cdot 10^{-17}$ &  $2\cdot 10^{-17}$ &  $9\cdot 10^{-18}$ &  $2\cdot 10^{-19}$ &  $9\cdot 10^{-1}$ \\
    \hline
    $2\cdot 10^{15}$ &  $1\cdot 10^{-2}$ &  $1\cdot 10^{-1}$ &  $8\cdot 10^{-1}$ &  $2\cdot 10^{-3}$ &  $9\cdot 10^{-2}$ &  $1\cdot 10^{17}$ \\ 
    \hline
    $8\cdot 10^{15}$ &  $6\cdot 10^{-2}$ &  $8\cdot 10^{-1}$ &  $9\cdot 10^{-2}$ &  $8\cdot 10^{-2}$ &  $9\cdot 10^{-1}$ & $ 2\cdot 10^{17} $\\
    \hline
    $1\cdot 10^{16}$ &  $\bf9\cdot 10^{-1}$ &  $1\cdot 10^{-2}$& $ 4\cdot 10^{-2}$&  $\bf9\cdot 10^{-1}$ &  $1\cdot 10^{-3}$ & $ 5\cdot 10^{-21}$ \\
    \hline
    \end{tabular}
    \label{CovBelTable}
    \end{center}
\end{table}

Дисперсионный долевой коэффициент $q_{ij}$ определим как вклад $j$-го признака в дисперсию $i$-го элемента вектора параметра $\textbf{w}$:

\[
q_{ij} = \frac{u^2_{ij}/\lambda_{jj}}{\sum^n_{j=1}{u^2_{ij}/\lambda_{jj}}}.
\]

Большие значение дисперсионных долей указывают на наличие зависимости между параметрами. Находим долевые коэффициенты, которые вносят максимальный вклад в дисперсию параметра $w_\xi$:

\[
\zeta = \arg\max_{j\in \mathcal{A}}{q_{\xi j}}.
\]
Параметр с индексом $\zeta$ определим как наименее релевантный параметр нейросети.

Проиллюстрируем принцип работы метода Белсли на примере. Гипотеза порождения данных: 
\[
    \textbf{w} = \begin{bmatrix}
    \text{sin}(x)\\
    \text{cos}(x)\\
    \text{2+cos}(x)\\
    \text{2+sin}(x)\\
    \text{cos}(x) + \text{sin}(x)\\
    x
    \end{bmatrix}
\]
с матрицей ковариации на рис. \ref{CovBel}.a, где $x \in [0.0, 0.02, ..., 20.0]$.


В табл. \ref{CovBelTable} приведены индексы обусловленности и соответствующие им дисперсионные доли, которые также изображены на рис. \ref{CovBel}.b. Согласно этим данным, максимальный индекс обусловленности $\eta_6 = 1.2\cdot 10^{16}$. Ему соответствуют максимальные дисперсионные доли признаков с индексами 1 и 4, которые, как видно из построения выборки, являются линейно зависимые.


\section{Дистилляция моделей глубокого обучения на многодоменных данных}
\begin{figure}[h!t]\center
    {\includegraphics[width=0.4 \textwidth]{thesis/figures/chapter-4/multidomain-distillation/Distillation_only}}
    \caption{Базовая дистилляция моделей глубокого обучения}
    \label{dist_only}
\end{figure}

\begin{figure}[h!t]\center
    {\includegraphics[width=0.4 \textwidth]{thesis/figures/chapter-4/multidomain-distillation/Distillation_DA}}
    \caption{Дистилляция моделей глубокого обучения с доменной адаптацией}
    \label{dist_da}
\end{figure}

\begin{definition}
Генеральная совокупность объектов~$B$ называется близкой к генеральной совокупности~$A$, если существует инъективное отображение $\varphi: A \rightarrow B$.
\end{definition}

Предлагается использовать, помимо меток учителя на одном из доменов, связь между доменами при обучении модели ученика.
В этом случае в качестве доменов должны выступать близкие генеральные совокупности.

На Рис.~\ref{dist_only} показан процесс обучения модели ученика в базовой постановке задачи дистилляции. Модель учителя обучается на большом наборе данных из генеральной совокупности~$A$, затем ее выходы используются для обучения студенческой модели на меньшем наборе данных из того же домена.
На Рис.~\ref{dist_da} представлен предложенный метод, который задействует выходы модели учителя, обученной на другом домене, и связь между доменами.

Базовая постановка задачи дистилляции. Задан набор данных
$$\mathfrak{D}=\{(\mathbf{x_i}, \mathbf{y_i})\}_{i=1}^n,
\quad \mathbf{x_i} \in \mathbb{X},
\quad \mathbf{y_i} \in \{1,...,R\},$$
где $R$ - количество классов в задаче классификации.

Предполагается, что задана обученная модель с большим количеством параметров --- модель учителя. И требуется обучить студенческую модель с меньшим количеством параметров, учитывая ответы учителя. Модель учителя $\mathbf{f}$ и студенческая модель $\mathbf{g}$ принадлежат параметрическому семейству функций: $$\mathfrak{F}=\{\mathbf{f}|\mathbf{f}=\text{softmax}(\mathbf{v(x)} /T), \mathbf{v}:\mathbb{R}^{n}\rightarrow \mathbb{R}^{R}\}.$$

где $\mathbf{v}$ - дифференцируемая параметрическая функция заданной структуры, $T$ - параметр температуры.

Функция потерь $\mathcal{L}$, учитывающая модель учителя $\mathbf{f}$ при выборе студенческой модели $\mathbf{g}$, имеет вид:
\[
\begin{aligned}
     \mathcal{L}(\mathbf{w,X,Y,f})=&-\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{R}y_{i }^{r}\log{g^{r}(x_{i})}\bigr|_{T=1}\\
     &-\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{R}f^{r}(x_{i})\bigr|_{T=T_{0} }\log{g^{r}(x_{i})}\bigr|_{T=T_{0}},
\end{aligned}
\]
где $\cdot\bigr|_{T=t}$ означает, что параметр температуры $T$ в предыдущей функции равен $t$.

Получаем задачу оптимизации:
$$\hat{\mathbf{w}} = \arg\min_{\mathbf{w} \in \mathbb{W}} \mathcal{L}(\mathbf{w,X,Y,f}).$$

Постановка задачи дистилляции для многодоменной выборки. Даны две выборки:
$$\mathfrak{D}_{\text{s}}=\{(\mathbf{x_i},\mathbf{y_i})\}_{i=1}^n,
\quad \mathbf{x_i} \in \mathbb{X}_{\text{s}},
\quad \mathbf{y_i} \in \mathbb{Y}$$
$$\mathfrak{D}_{\text{t}}=\{(\mathbf{x_i},\mathbf{y_i})\}_{i=1}^m, \quad \mathbf {x_i} \in \mathbb{X}_{\text{t}},
\quad \mathbf{y_i} \in \mathbb{Y},$$
где $\mathfrak{D}_{\text{s}}, \mathfrak{D}_{\text{t}}$~--- исходный и целевой наборы данных. В базовой постановке задачи дистилляции предполагается, что
$\mathfrak{D}_{\text{t}} \subset \mathfrak{D}_{\text{s}},
\mathbb{X}_{\text{t}}=\mathbb{X}_{\text{s}}$. Предполагается, что количество объектов в наборах данных не совпадает:
$$n \gg m$$

Пусть задана модель учителя на выборке большей мощности:
$$\mathbf{f}: \mathbb{X}_{\text{s}} \rightarrow \mathbb{Y}^{\prime},$$
где $\mathbf{f}$ - модель учителя, $\mathbb{Y}^{\prime}$ - пространство вероятностей классов.

Связь между исходной и целевой выборками задается:
$$\varphi: \mathbb{X}_{\text{t}} \rightarrow \mathbb{X}_{\text{s}},$$
где $\varphi$ - инъективное отображение. Требуется получить студенческую модель для малоресурсной выборки:
$$\mathbf{g}: \mathbb{X}_{\text{t}} \rightarrow \mathbb{Y}^{\prime},$$
где $\mathbf{g}$ - студенческая модель.

В работе рассматривается функция потерь, учитывающая метки учителя и связь между доменами:
\begin{align}
    &\mathcal{L}(\mathbf{w,X,Y,f,\varphi})=\\&
    -\lambda\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{ R}\mathbb{I}[y_{i}=r]\log{g^{r}(\mathbf{x}_{i},\mathbf{w})} \\&-(1-\lambda)\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{R}(f\circ \varphi)^{r}(\mathbf{x }_{i})\log{g^{r}(\mathbf{x}_{i},\mathbf{w})},
\end{align}
где $\lambda$ - метапараметр, задающий вес дистилляции, $\mathbb{I}$ - индикаторная функция.

Получаем задачу оптимизации:
$$\hat{\mathbf{w}} = \arg\min_{\mathbf{w} \in \mathbb{W}} \mathcal{L}(\mathbf{w,X,Y,f,\varphi} ).$$


\section{Анти-Дистилляция моделей глубокого обучения}
В этом разделе описывается постановка задачи анти-дистилляции для задачи классификации.
Отметим, что аналогичный подход может быть применен для произвольных задач.

Даны два набора данных
\begin{align}
    \mathfrak{D}_1 = \{(\mathbf{x}_i, y_i)\}_{i=1}^{m_1},~\mathbf{x}_i \in \mathbb{R}^{n},~y_i \in C_1 = \{1, \dots, c_1\}, \\
    \mathfrak{D}_2 =  \{(\mathbf{x}_i, y_i)\}_{i=1}^{m_2},~\mathbf{x}_i \in \mathbb{R}^{n},~y_i \in C_2 = \{1, \dots, c_2\},
\end{align}
где $m_1$ и $m_2$ - количество объектов в $\mathfrak{D}_1$ и $\mathfrak{D}_2$ соответственно, $n$ - размерность входного пространства. $C_1$ и $C_2$ - множества меток классов $1, \dots, c_1, \dots, c_2$.

Предполагается, что объекты $\mathbf{x}_i$ порождены из генеральной совокупности, общей для обоих наборов данных $\mathfrak{D}_1, \mathfrak{D}_2$, и имеют схожие свойства для этих наборов.
Также предположим, что набор данных $\mathfrak{D}_2$ является более сложным для классификации и требует более сложной модели классификации.

Задана модель учителя $\mathbf{g}_\text{tr}$, обученная на первом наборе данных $\mathfrak{D}_1$:
\[
    \mathbf{g}_\text{tr}: \mathbb{R}^{n} \rightarrow \Delta^{c_1},\quad \mathbf{g}_\text{tr}(\mathbf{x}) = \mathbf{g}(\mathbf{x}, \hat{\mathbf{u}}),
\] 
где $\Delta^c$ - множество $c$-мерных вероятностных векторов,

Параметры модели учителя $\mathbf{g}_\text{tr}$ определяются следующим образом:
$$\hat{\mathbf{u}} =  \underset{\mathbf{u}}{\arg\min}~\mathcal{L}_\text{ce}(\mathbf{u}, \mathfrak{D}_1) =\underset{\mathbf{u}}{\arg\min}~\sum\limits_{i=1}^{m_1} l \left(y_i,~g(\mathbf{x}_i, \mathbf{u})\right),$$
здесь $l$ - перекрестная энтропия:
$$l(y, \hat{y}) = -\sum\limits_{k=1}^{c} [y = k] \log{\hat{y}_k},~y \in C,~\hat{y} \in \Delta^c.$$

Наша задача - построить студенческую модель
\[
    \mathbf{f}_\text{st}: \mathbb{R}^{n} \rightarrow \Delta^{c_2},\quad \mathbf{f}_\text{st}(\mathbf{x}) = \mathbf{f}(\mathbf{x}, \hat{\mathbf{w}}),
\],
которая минимизирует перекрестную энтропию на валидационной части второго набора данных $\mathfrak{D}_2$
\[
    \hat{\mathbf{w}} =  \underset{\mathbf{w}}{\arg\min}~\mathcal{L}_\text{ce}(\mathbf{w}, \mathfrak{D}^\text{val}_2),
\],
где $\mathfrak{D}_2 = \mathfrak{D}^\text{train}_2 \sqcup \mathfrak{D}^{\text{val}}_2$ и $\hat{\mathbf{w}}$~--- оптимальные параметры модели.

Поскольку валидационная функция потерь не оптимизируется напрямую, общепринятой практикой является использование градиентных методов оптимизации на обучающей части $\mathfrak{D}^\text{train}_2$ набора данных $\mathfrak{D}_2$.
Чтобы уменьшить переобучение и использовать больше информации о данных, используется информация от модели учителя $\mathbf{g}_{\text{tr}}$.
Заметим, что используется предположение, что наборы данных $\mathfrak{D}_1$ и $\mathfrak{D}_2$ имеют общие свойства.

Функция 
\[
    \boldsymbol{\varphi}: \mathbb{R}^{N_\text{tr}} \rightarrow \mathbb{R}^{N_\text{st}}
\]
отображает параметры модели учителя в начальные параметры модели ученика $\mathbf{w} = \boldsymbol{\varphi}(\hat{\mathbf{u}})$.

\begin{hypothesis}
Модели ученика, инициализированные результатом применения функции $\boldsymbol{\varphi}$ к параметрам предварительно обученной модели учителя, являются более устойчивыми и достигают более высокой точности, чем модели с параметрами по умолчанию.
\end{hypothesis}

Основная проблема предложенного метода заключается в том, что модель учителя $\mathbf{g}_\text{tr}$, обученная на простом наборе данных $\mathfrak{D}_1$, может быть намного проще, чем студенческая модель $\mathbf{f}_\text{st}$. Чтобы использовать больше информации из параметров модели учителя $\hat{\mathbf{u}}$, нам нужно расширить размерность пространства параметров модели учителя ${N_\text{tr}}$ до размерности ${N_\text{st}}$ пространства параметров студенческой модели.

Чтобы справиться с этим, оптимизируем следующую составную функцию потерь:
\begin{equation}\label{phi}
  \boldsymbol{\varphi}(\mathbf{u}) = \underset{\mathbf{w} \in \mathbb{R}^{N_\text{st}}}{\arg\min}~\mathcal{L}(\mathbf{w}),
\end{equation}
где 
\[
    \mathcal{L}(\mathbf{w}) = \lambda_1 \mathcal{L}_\text{ce}(\mathbf{w}, \mathfrak{D}_1) + \lambda_2 \mathcal{L}_2 (\mathbf{w}, \mathbf{u}) + \lambda_3 \mathcal{L}_3^\delta (\mathbf{w}, \mathfrak{D}_1) + \lambda_4 \mathcal{L}_4 (\mathbf{w}),
\]
\[
    \forall i \in \overline{1, 4} \; \lambda_i \ge 0
\]
Здесь $\mathcal{L}_\text{ce}(\mathbf{w}, \mathfrak{D}_1)$ - перекрестная энтропия, отвечающая за качество студенческой модели на $\mathfrak{D}_1$.

Второе слагаемое
\[
    \mathcal{L}_2 (\mathbf{w}, \mathbf{u}) = \|\textbf{u} - \textbf{Pr}[\textbf{w}]\|^2_2
\]
обеспечивает малую разницу между параметрами модели учителя и студенческой модели в соответствующих местах, где \textbf{Pr} берет только первые параметры, общие для обеих моделей (в случае моделей многослойного перцептрона, \textbf{Pr} берет параметры тех же нейронов для каждого слоя модели).

Компонента
\[
    \mathcal{L}_3^\delta (\mathbf{w}, \mathfrak{D}_1) = \displaystyle \sum \limits_{(\textbf{x}, y) \in \mathfrak{D}_1} \displaystyle \mathbb{E}_{\textbf{x}' \in U_\delta(\textbf{x})} \mathcal{L}_\text{ce}(\mathbf{w}, \textbf{x}', y)
\]
отвечает за устойчивость решения к шуму во входных данных, где $U_\delta(\textbf{x})$ представляет равномерное распределение в $[\delta - \textbf{x}; \delta + \textbf{x}].$

Последнее слагаемое
\[
    \mathcal{L}_4 (\mathbf{w}) = \text{tr} \left(\displaystyle \frac{\partial^2 \mathcal{L}_\text{ce}}{\partial \mathbf{w}^2}\right)
\]
выполняет регуляризацию гессиана, что также повышает устойчивость модели.

Заметим, что последнее слагаемое $\mathcal{L}_4$ включает вычисление гессиана, наивное вычисление которого может быть ресурсоемким.
Поэтому, используется метод стохастической аппроксимации~\cite{bai1996some} следа гессиана с быстрым умножением гессиан-вектор~\cite{pearlmutter1994fast}. Сложность такой процедуры линейна от количества параметров модели $\mathbf{f}_\text{st}$.

В интересующем нас случае, Анти-Дистилляции, подразумевается $\lambda_2 >0$, т.е. оптимизация, которая делает параметры модели учителя и ученика достаточно близкими.
Заметим, что важна устойчивость модели к искажению входных данных.
Для этого свойства используем слагаемые $\mathcal{L}_3$ и $\mathcal{L}_4$. Оба этих слагаемых регулируют гессиан функции перекрестной энтропии~\cite{yao2020pyhessian,chen2020stabilizing}.

\section{Результаты вычислительных экспериментов}

\subsection{Удаления параметров моделей глубокого обучения}
Для анализа свойств предложенного алгоритма и сравнения его с существующими проведен вычислительный эксперимент в котором параметры нейросети удалялись методами, которые описаны в разделах 3.1---3.3 и методом Белсли.

В качестве данных использовались три выборки. Выборки Wine~\cite{Wine} и Boston Housing~\cite{boston1978} ~--- это реальные данные. Синтетические данные сгенерированы таким образом чтобы параметры сети мультиколинеарными. Генерация данных состояла из двух этапов. 
На первом этапе генерировался вектор параметров $\mathbf{w}_{\text{synthetic}}$:
\[
    \mathbf{w}_{\text{synthetic}}  \sim \mathcal{N}(\textbf{m}_{\text{synthetic}}, \textbf{A}_{\text{synthetic}}),
\]
где 
$\textbf{m}_{\text{synthetic}} = \begin{bmatrix}
1.0\\
0.0025\\
\ldots\\
0.0025
\end{bmatrix}$,
$\textbf{A}_{\text{synthetic}} = \begin{bmatrix}
1.0& 10^{-3}& \ldots& 10^{-3}& 10^{-3}\\
10^{-3}& 1.0& \ldots& 0.95& 0.95\\
\ldots&\ldots&\ldots&\ldots&\ldots\\
10^{-3}& 0.95& \ldots& 0.95& 1.0
\end{bmatrix}$.

На втором этапе генерировалась выборка $\mathfrak{D}_{\text{synthetic}}$:
\[
    \mathfrak{D}_{\text{synthetic}} = \{(\textbf{x}_i,y_i)| \textbf{x}_i \sim  \mathcal{N}(\textbf{1}, \textbf{I}), y_i = x_{i0}, i = 1 \ldots 10000\}.
\]
В приведенном выше векторе параметров $\mathbf{w}_{\text{synthetic}}$ для выборки $\mathfrak{D}_{\text{synthetic}}$, наиболее релевантным является первый параметр, а все остальные параметры являются нерелевантными. Матрица ковариации выбрана таким образом, чтобы все нерелевантные параметры являлись зависимыми величинами, что приводит к максимальной эффективности метода Белсли.

\begin{table}[h!t]

\begin{center}
\caption{Описание выборок для анализа метода задания порядка методом Белсли}
\begin{tabular}{|c|c|c|c|}
\hline
	Выборка &Тип задачи& Размер выборки& Число признаков\\
	\hline
	
	\multicolumn{1}{|l|}{Wine}
	&
	\multicolumn{1}{|l|}{класификация}
	 & 178 & 13\\
	\hline
	
	\multicolumn{1}{|l|}{Boston Housing}
	&
	\multicolumn{1}{|l|}{регресия}
	& 506 & 13\\
	\hline
	
	\multicolumn{1}{|l|}{Synthetic data}
	&
	\multicolumn{1}{|l|}{регресия}
	& 10000 & 100\\
\hline

\end{tabular}
\end{center}
\end{table}


Для алгоритмов тренировочная и тестовая выборки составили $80\%$ и $20\%$ соответсвенно. Критерием качества прореживания служит процент параметров нейросети, удаление которого не влечет значимой потери качества прогноза. Также критерием качества служит устойчивость нейросети к зашумленности данных. 

Качеством прогноза $R_{\text{cl}}$ модели для задачи классификации является точность прогноза модели:
\[
R_{\text{cl}} = \frac{\sum_{(\textbf{x},y)\in \mathfrak{D}} [f(\textbf{x}, \textbf{w}) = y]}{\left|\mathfrak{D}\right|},
\]

Качеством прогноза $R_{\text{rg}} $ модели для задачи регрессии является среднеквадратическое отклонение результата модели от точного:

\[
R_{\text{rg}} = \frac{\sum_{(\textbf{x},y)\in \mathfrak{D}} \left(f(\textbf{x}, \textbf{w}) - y\right)^2}{\left|\mathfrak{D}\right|},
\]

\paragraph{Wine.} Рассмотрим нейроную сеть с 13 нейронами на входе, 13 нейронами в скрытом слое и 3 нейронами на выходе.

\begin{figure}[h!t]\center
    \includegraphics[width=0.8\textwidth]{thesis/figures/chapter-4/belsli/Wine/All.pdf}\\
    \caption{Качество прогноза при удаление параметров на выборке Wine}
    \label{WineAll}
\end{figure}

\begin{figure}[h!t]\center
    \subfloat[Произвольное удаление параметров]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Wine/RandomNoise3D.pdf}}
    \subfloat[Оптимальное прореживание]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Wine/OBDNoise3D.pdf}}\\
    \subfloat[Вариационный метод]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Wine/VariationalNoise3D.pdf}}
    \caption{Влияние шума в начальных данных на шум выхода нейросети на выборке Wine}
    \label{WineNoise}
\end{figure}

На рис. \ref{WineAll} показано как меняется точность прогноза $R_{\text{cl}}$ при удалении параметров указанными методами. Из графика видно, что метод оптимального прореживания, вариационный метод и метод Белсли позволяют удалить $\approx80\%$ параметров и качество всех этих методов падает при удалении $\approx90\%$ параметров нейросети. 

На рис. \ref{WineNoise} показаны поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для разных методов прореживания. На графиках показано, что при удалении параметров нейросети методом Белсли шум меньше, чем при удалении параметров другими методами, на это указывает то что поверхность которая соответствует методу Белсли ниже других поверхностей.

\paragraph{Boston Housing.} Рассмотрим нейроную сеть с 13 нейронами на входе, 39 нейронами в скрытом слое и одним нейроном на выходе.

\begin{figure}[h!t]\center
    \includegraphics[width=0.8\textwidth]{thesis/figures/chapter-4/belsli/Boston/All.pdf}\\
    \caption{Качество прогноза при удаление параметров на выборке Boston}
    \label{BostonAll}
\end{figure}

\begin{figure}[h!t]\center
    \subfloat[Произвольное удаление параметров]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Boston/RandomNoise3D.pdf}}
    \subfloat[Оптимальное прореживание]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Boston/OBDNoise3D.pdf}}\\
    \subfloat[Вариационный метод]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Boston/VariationalNoise3D.pdf}}
    \caption{Влияние шума в начальных данных на шум выхода нейросети на выборке Boston}
    \label{BostonNoise}
\end{figure}

На рис. \ref{BostonAll} показано как меняется среднеквадратическое отклонение прогноза $\mathsf{R}_{\text{rg}}$ от точного ответа  при удалении параметров указанными методами. График показывает, что метод Белсли является более эффективным, чем другие методы, так как позволяет удалить больше параметров нейросети без потери качества.

На рис. \ref{BostonNoise} показаны поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для разных методов прореживания. График показывает, что уровень шума всех методов одинаковый, так как поверхности всех методов находятся на одном уровне.


\paragraph{Синтетические данные.} Рассмотрим нейроную сеть с 100 нейронами на входе и одним нейроном на выходе.

\begin{figure}[h!t]\center
    \includegraphics[width=0.8\textwidth]{thesis/figures/chapter-4/belsli/Data1/All.pdf}\\
    \caption{Качество прогноза при удаление параметров на синтетической выборке}
    \label{Data1All}
\end{figure}

\begin{figure}[h!t]\center
    \subfloat[Произвольное удаление параметров]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Data1/RandomNoise3D.pdf}}
    \subfloat[Оптимальное прореживание]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Data1/OBDNoise3D.pdf}}\\
    \subfloat[Вариационный метод]{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/belsli/Data1/VariationalNoise3D.pdf}}
    \caption{Влияние шума в начальных данных на шум выхода нейросети на синтетической выборке}
    \label{Data1Noise}
\end{figure}

На рис. \ref{Data1All} показано как меняется среднеквадратическое отклонение прогноза от $\mathsf{R}_{\text{rg}}$ точного ответа при удалении параметров указанными методами. График показывает, что удаление параметров методом Белсли являеться более эффективным чем другие методы прореживания, так как качество прогноза нейросети повышается при удалении шумовых параметров.

На рис. \ref{Data1Noise} показаны поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для разных методов прореживания. На графиках показано, что при удалении параметров нейросети методом Белсли шум меньше, чем при удалении параметров другими методами, так как поверхность которая соответствует методу Белсли ниже других поверхностей.

\subsection{Дистилляция моделей глубокого обучения на многодоменных данных}
Цель вычислительного эксперимента~--- сравнить производительность моделей учителя и ученика на реальных наборах данных с использованием отображения $\varphi$ и без него для задач компьютерного зрения и обработки естественного языка. Для анализа качества дистилляции предложен интегральный критерий качества~\cite{grabovoi2022probabilistic609999418}.
\begin{itemize}
    \item Используется подмножество ImageNet~--- набор изображений, для которого необходимо решить задачу классификации на 10 классов~\cite{imagenet}. Набор данных состоит из обучающей и тестовой частей, причем обучающая часть разделена на мультиресурсную и малоресурсную части.
    \item OPUS-100 является англоцентричным, то есть все обучающие пары включают английский язык на стороне источника или цели~\cite{opus100}. Используются наборы данных fr-en и de-en.
\end{itemize}

Конфигурация алгоритма многодоменной дистилляции для задачи компьютерного зрения


\begin{table}[h!t]
\begin{center}
\caption{Структура учителя}
\label{table_1}
\begin{tabular}{|c|c|c|}
\hline
	Слой & Размер входного вектора & Количество параметров\\
	\hline
	\multicolumn{1}{|l|}{Входной слой}
	& (3, 200, 200) & 0 \\
	\hline
	\multicolumn{1}{|l|}{CONV1 (размер ядра=5)}
	& (24, 196, 196) & 1800 \\
	\hline
	\multicolumn{1}{|l|}{POOL1}
	& (24, 98, 98) & 0 \\
	\hline
	\multicolumn{1}{|l|}{CONV2 (размер ядра = 5)}
	& (48, 94, 94) & 28800 \\
	\hline
	\multicolumn{1}{|l|}{POOL2}
	& (48, 47, 47) & 0 \\
	\hline
	\multicolumn{1}{|l|}{CONV3 (размер ядра = 8)}
	& (96, 40, 40) & 294912 \\
	\hline
	\multicolumn{1}{|l|}{POOL3}
	& (96, 20, 20) & 0 \\
	\hline
    \multicolumn{1}{|l|}{CONV4 (размер ядра = 5)}
	& (192, 16, 16) & 460800 \\
	\hline
	\multicolumn{1}{|l|}{POOL4}
	& (192, 8, 8) & 0 \\
    \hline
	\multicolumn{1}{|l|}{CONV5 (размер ядра = 7)}
	& (384, 2, 2) & 3612672 \\
	\hline
	\multicolumn{1}{|l|}{POOL5}
	& (384, 1, 1) & 0 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (384) & 0 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (120) & 46080 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (84) & 10080 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (10) & 840 \\
	\hline
	\multicolumn{1}{|l|}{}
	& & $\sum=4455984$ \\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[h!t]
\begin{center}
\caption{Структура модели ученика}
\label{table_1_1}
\begin{tabular}{|c|c|c|}
\hline
	Слой & Размер входного вектора & Количество параметров\\
	\hline
	\multicolumn{1}{|l|}{Входный слой}
	& (3, 200, 200) & 0 \\
	\hline
	\multicolumn{1}{|l|}{CONV1 (размер ядра=5)}
	& (24, 196, 196) & 1800 \\
	\hline
	\multicolumn{1}{|l|}{POOL1}
	& (24, 98, 98) & 0 \\
	\hline
	\multicolumn{1}{|l|}{CONV2 (размер ядра = 5)}
	& (48, 94, 94) & 28800 \\
	\hline
	\multicolumn{1}{|l|}{POOL2}
	& (48, 47, 47) & 0 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (106032) & 0 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (120) & 12723840 \\
	\hline
	\multicolumn{1}{|l|}{Полносвязный слой}
	& (10) & 1200 \\
	\hline
	\multicolumn{1}{|l|}{}
	& & $\sum=12755640$ \\
\hline
\end{tabular}
\end{center}
\end{table}


Структуры модели учителя $\mathbf{f}$ и модели ученика $\mathbf{g}$ описаны в Таблице~\ref{table_1} и Таблице~\ref{table_1_1}. Функция активации после каждого скрытого слоя~--- ReLu. 
Используется метод градиентной оптимизации Adam \cite{adam2015} для решения задачи оптимизации.

\begin{table}[h!t]
\begin{center}
\caption{Набор данных ImageNet}
\label{table_2}
\begin{tabular}{|c|c|c|}
\hline
	Набор данных & Описание & Размер набора\\
	\hline
	\multicolumn{1}{|l|}{ImageNet-Train}
	& Обучающая часть& 9469\\
	\hline
	\multicolumn{1}{|l|}{ImageNet-Big}
	& Мультиресурсная часть& 8469\\
	\hline
	\multicolumn{1}{|l|}{ImageNet-Small}
	& Малоресурсная часть& 1000\\
	\hline
	\multicolumn{1}{|l|}{ImageNet-Test}
	& Тестовая часть& 3925\\
\hline
\end{tabular}
\end{center}
\end{table}

В Таблице~\ref{table_2} описаны наборы данных для вычислительного эксперимента по компьютерному зрению. Каждый из наборов данных состоит из обучающей и тестовой части, при этом обучающая часть разделена на мультиресурсную и малоресурсную части. Обучающая часть содержит 9 469 объектов, мультиресурсная часть содержит 8 469 объектов, малоресурсная часть содержит 1 000 объектов, а тестовая часть содержит 3 925 объектов.

Цель эксперимента~--- сравнить производительность студенческой модели, обучающейся без учителя, с учителем и с учителем на другом домене с использованием адаптации домена.
Сначала обучается модель учченика на малоресурсной части и происходит ее тестирование на тестовой части, затем используются метки модели учителя, обученной на мультиресурсной части, для обучения модели ученика.
Наконец, происходит обучение модели учителя на изображениях из мультиресурсной части и используется модель учителя и отображение для обучения модели ученика.
Предобученная модель CycleGAN~\cite{CycleGAN} используется в качестве отображения $\varphi$.

\begin{figure}[htpb]\center
    {\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/multidomain-distillation/Mapping}}
    \caption{Сравнение примера объекта до и после преобразования.}
    \label{mapping}
\end{figure}

Рис.~\ref{mapping} показывает одно из изображений в наборе данных ImageNet~\cite{imagenet} и то же изображение после преобразования с помощью модели CycleGAN~\cite{CycleGAN}. Результаты усредняются по 5 запускам и для вычисления среднего значение и дисперсии метрик.

Конфигурация алгоритма многодоменной дистилляции для задачи обработки естественного языка.
Набор данных OPUS100 был разделен на обучающую часть для учителя, состоящую из немецко-английских предложений, и обучающую часть и тестовую часть для модели ученика, состоящую из французско-английских предложений.
Обучающая часть учителя содержала 5 000 предложений, обучающая часть модели ученика содержала 2 000 предложений, а тестовая часть содержала 500 предложений.

\begin{table}[h!t]
\begin{center}
\caption{Набор данных OPUS100}
\label{table_3}
\begin{tabular}{|c|c|c|c|}
\hline
	Набор данных & Описание & Язык & Размер набора\\
	\hline
	\multicolumn{1}{|l|}{Teacher-Train}
	& Обучающая часть модели учителя & de-en & 5000\\
	\hline
	\multicolumn{1}{|l|}{Student-Train}
	& Обучающая часть модели ученика& fr-en & 2000\\
	\hline
	\multicolumn{1}{|l|}{Student-Test}
	& Тестовая часть & fr-en & 500\\
\hline
\end{tabular}
\end{center}
\end{table}

Таблица~\ref{table_3} описывает наборы данных для вычислительного эксперимента по обработке естественного языка.

Использовалась модель ученика модель~\textbf{g} и модель учителя~\textbf{f} в качестве трансформерной модели на основе статьи~\cite{attention} и метод градиентной оптимизации Adam \cite{adam2015} для решения задачи оптимизации. Модель NLLB\cite{nllb} использовалась в качестве отображения $\varphi$.
Эта модель переводила французские предложения в немецкие.

Аналогично эксперименту по компьютерному зрению, сравнивается производительности модели ученика без учителя, с учителем и с учителем и адаптацией домена.
Результаты усреднялись по 5 запускам для вычисления среднего значение и дисперсии метрик.
\begin{figure}[htpb]
    \centerline{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/multidomain-distillation/Dist_acc.png}}
    \caption{Точность аппроксимации на тестовой выборке. Все результаты усреднены по 5 запускам.}
    \label{cv_acc}
\end{figure}

\begin{figure}[htpb]
    \centerline{\includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/multidomain-distillation/Dist_loss.png}}
    \caption{Ошибка перекрестной энтропии между истинными и предсказанными студенческими метками на тестовой выборке. Все результаты усреднены по 5 запускам.}
    \label{cv_loss}
    \end{figure}

Как видно из Рис.~\ref{cv_acc} и Рис.~\ref{cv_loss}, модели, обученные с использованием учителя, достигают лучшего качества и точности. Можно заметить, что студенческая модель, обученная с использованием меток учителя на том же домене (зеленые линии), достигает наивысшей точности и наименьших потерь. Студенческая модель, обученная с использованием меток учителя и адаптации домена (красные линии), показывает лучшее качество аппроксимации, чем модель без использования учителя.

Таким образом, экспериментально показано, что дистилляция с использованием адаптации домена приводит к более эффективным нейронным сетям с меньшим количеством параметров.


\begin{table}[h!t]
\begin{center}
\caption{Качество моделей для компьютерного зрения}
\label{table_4}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
	Ученика & Учитель & Отображение $\varphi$ & Точность & \begin{tabular}[c]{@{}c@{}}Потери перекрестной\\ энтропии\end{tabular} & \begin{tabular}[c]{@{}c@{}}Интегральный\\ критерий\end{tabular}\\
	\hline
	\multicolumn{1}{|l|}{ImageNet-Small}
	& --- & --- & $0{,}34 \pm 0{,}01$ & $4{,}41 \pm 1{,}10$ & $53{,}89 \pm 14{,}99$ \\
    \hline

    \multicolumn{1}{|l|}{ImageNet-Small}
	& ImageNet-Big & StyleTransfer & $0{,}37 \pm 0{,}01$ & $\textbf{2{,}01} \pm \textbf{0{,}03}$ & $28{,}30 \pm 0{,}79$ \\
\hline

    
    \hline
    \multicolumn{1}{|l|}{ImageNet-Small}
	& ImageNet-Big & --- & $\textbf{0{,}44} \pm \textbf{0{,}01}$ & $2{,}03 \pm 0{,}02$ & $\textbf{28{,}08} \pm \textbf{1{,}22}$ \\
    \hline
	
\end{tabular}
}
\end{center}
\end{table}

Результаты также представлены в табличной форме. Таблица~\ref{table_4} содержит данные о валидационной точности, потерях и интегральном критерии моделей, обученных с дистилляцией и адаптацией домена и без них, в эксперименте по компьютерному зрению.


\begin{figure}[h!t]\center
    {\includegraphics[width=0.5 \textwidth]{thesis/figures/chapter-4/multidomain-distillation/NLP_loss.png}}
    \caption{Ошибка перекрестной энтропии на тестовом наборе данных. Все результаты усреднены по 3 запускам.}
    \label{nlp_loss}
\end{figure}

Как видно из Рис.~\ref{nlp_loss}, модели, обученные с использованием учителя, достигают лучшего качества.

Таблица~\ref{table_5} показывает результаты сравнения студенческих моделей, полученных с использованием дистилляции и без.

\begin{table}[h!t]
\begin{center}
\caption{Качество моделей для NLP}
\label{table_5}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
	Ученик & Учитель & Отображение $\varphi$ & Потери перекрестной энтропии & \begin{tabular}[c]{@{}c@{}}BLEU\end{tabular} \\
\hline
	\multicolumn{1}{|l|}{Student-Train}
	& --- & ---& $5{,}367 \pm 0{,}015$ & $0{,}0282$ \\
    \hline
	\multicolumn{1}{|l|}{Student-Train}
	& Teacher-Train & NLLB & $\textbf{5{,}233} \pm \textbf{0{,}007}$ & $\textbf{0{,}0572}$\\
\hline
\end{tabular}
}
\end{center}
\end{table}



\subsection{Анти-Дистилляция моделей глубокого обучения}
Цель вычислительного эксперимента~--- сравнить производительность моделей в зависимости от инициализации параметров. 

Производится сравнение различных подходов к инициализации:
\begin{enumerate}
    \item Xavier~--- заполнение всех параметров модели $U[-\frac1{\sqrt{n}}, \frac1{\sqrt{n}}]$, где $n$~--- количество нейронов входного слоя \cite{glorot2010understanding}, т.е. инициализация параметров модели по умолчанию.
    \item Zero pad~--- заполнение расширенных параметров нулями.
    \item Uniform pad~--- заполнение расширенных параметров равномерно распределенными случайными величинами $U[-\frac1{\sqrt{n}}, \frac1{\sqrt{n}}]$, где $n$~--- количество нейронов входного слоя.
    \item Transfer learning~--- взятие предобученной модели и изменение только классификационного слоя для новой задачи классификации. Сначала модель обучалась с замороженными параметрами на всех слоях, кроме классификационного. После 3 эпох обучения все параметры размораживались. Начиная с четвертой эпохи, оптимизировались все параметры нейронной сети.
    \item Net2Net~--- инкрементальный алгоритм расширения пространства параметров модели\cite{net2net}.
    \item With Data Noise~--- получение инициализации студенческой модели путем решения задачи оптимизации \ref{phi} с $\lambda_1, \lambda_3 = 1$ и $\lambda_2, \lambda_4 = 0$.
    \item Anti-Distillation, $\lambda_4 = 0$~--- инициализация методом Анти-Дистилляции с оптимизацией гиперпараметров $\lambda_1, \lambda_2, \lambda_3$ с помощью байесовской оптимизации ($\lambda_4 = 0$) \cite{akiba2019optuna}.
    \item Anti-Distillation~--- оптимизация всех $\lambda_i$.
\end{enumerate}

Критериями качества являются: точность на валидационном наборе, точность на валидационном наборе, искаженном атакой FSGM \cite{goodfellow2014explaining}, точность на валидационном наборе при условии, что параметры модели искажены шумом: ${\mathbf{w}_\varepsilon} = \mathbf{w} + \varepsilon \boldsymbol{\xi}$, где $\boldsymbol{\xi} \sim \mathcal{N} (\mathbf{0}, \mathbf{I})$.


Fashion-MNIST~--- это набор данных изображений статей Zalando, состоящий из обучающего набора из 60 000 примеров и тестового набора из 10 000 примеров. Каждый пример представляет собой полутоновое изображение 28x28, связанное с меткой из 10 классов \cite{fashionmnist}.

Проведение экспериментов осуществляется следующим образом: обучается модель учителя, увеличивается ее сложность и сравниваем различные способы инициализации параметров модели.
Рассматривается модель полносвязной сети. Модель учителя имеет следующие размеры скрытых слоев: $[128, 64, 32]$. Модели ученика имеет $[256, 128, 64]$ нейронов в скрытых слоях.

Модель учителя обучался в течение 30 эпох с начальной скоростью обучения 1e-2, которая затем уменьшалась до 1e-3 после 10 эпох.
Модель ученика сравнивались при обучении в течение 10 эпох со скоростью обучения 1e-3.
Оптимизация проводится с использованием алгоритма оптимизации Adam \cite{adam2015}.
Методы инициализации сравниваются, измеряя точность предсказаний, значение функции потерь перекрестной энтропии на валидационной выборке и дисперсию предсказаний.
Также исследуется случай зашумленных входных данных, рассматривая указанные критерии качества в зависимости от процента искаженных изображений.

\begin{figure}[!t]
\centering
  \includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/anti-distillation/file}
 \caption{Сравнение валидационной точности для различных методов инициализации}
  \label{fig:1}
\end{figure}
\begin{figure}[!t]
\centering
  \includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/anti-distillation/fsgm}
 \caption{Зависимость валидационной точности от адверсарного шума в данных}
  \label{fig:2}
\end{figure}
\begin{figure}[!t]
\centering
  \includegraphics[width=0.5\textwidth]{thesis/figures/chapter-4/anti-distillation/noise}
 \caption{Зависимость валидационной точности от параметра интенсивности шума $\varepsilon$}
  \label{fig:3}
\end{figure}

Набор данных $\mathfrak{D}_2$ состоит из Fasion-MNIST, а $\mathfrak{D}_1 = \{(\textbf{x}, y) \;|\; (\textbf{x}, y) \in \mathfrak{D}_2, y \in C_1\}$, $C_1 \subset C_2$, $C_1 = \{0, \dots 4\}, C_2 = \{0, \dots 9\}$. 


Как видно на Рисунке \ref{fig:1}, модели, использующие Анти-Дистилляцию, в среднем имеют меньшую дисперсию и более высокую точность, чем модели с различной инициализацией параметров.
Обучение модели с нуля оказалось не лучшим решением.
Предложенный метод дает нам лучшие результаты с меньшим количеством итераций для сходимости.
Отметим, что не учитывалось количество итераций, необходимых для расширения модели учителя, которое также требует процедуры оптимизации.
Предполагается, что во многих реальных случаях этим временем можно пренебречь, поскольку предложенный метод позволяет нам расширить модель учителя один раз, используя только базовый набор данных $\mathfrak{D}_1$, для последующего использования в множественных задачах обучения модели ученика~\cite{sun2019meta}.

Рис.~\ref{fig:2} показывает, что Анти-Дистилляция является наиболее устойчивым к адверсарным атакам методом инициализации параметров модели, поскольку она имеет наивысшую валидационную точность с большим отрывом при высоких уровнях шума.

На Рисунке \ref{fig:3} видно, что метод Анти-Дистилляции без регуляризации гессиана ($\lambda_4=0$) является наиболее устойчивым к нормальному шуму в параметрах модели, поскольку сохраняет наивысшую точность при максимальном рассматриваемом уровне шума.

\begin{table}[!h]
\caption{\label{acc_tab} Точность на валидационном наборе.}

 \begin{tabular}{|c|c|c|c|}
        \hline
        Метод инициализации & Точность &  Атака FSGM & 
Шум в параметрах \\
        \hline
        Xavier & 0.68 $\pm$ 0.08 & 0.42 $\pm$ 0.04 & 0.58 $\pm$ 0.06\\
    \hline
Zero Pad & \textbf{0.86 $\pm$ 0.02} & 0.50 $\pm$ 0.01 & 0.71 $\pm$ 0.03\\
\hline
Uniform Pad & 0.85 $\pm$ 0.04 & 0.52 $\pm$ 0.03 & \textbf{0.73 $\pm$ 0.03}\\
\hline
Transfer Learning & 0.74 $\pm$ 0.09 & 0.50 $\pm$ 0.06 & 0.53 $\pm$ 0.05\\
\hline
Net2Net & 0.85 $\pm$ 0.04 & 0.51 $\pm$ 0.02 & 0.70 $\pm$ 0.03\\
\hline
With Data Noise & 0.81 $\pm$ 0.07 & 0.51 $\pm$ 0.03 & 0.70 $\pm$ 0.05\\
\hline
Anti-Distillation, $\lambda_4$=0 & \textbf{0.86 $\pm$ 0.05} & 0.53 $\pm$ 0.03 & \textbf{0.73 $\pm$ 0.04}\\
\hline
Anti-Distillation & \textbf{0.86 $\pm$ 0.05} & \textbf{0.57 $\pm$ 0.03} & 0.67 $\pm$ 0.03\\
\hline
\end{tabular}

\end{table}

Результаты также представлены в табличной форме. Таблица \ref{acc_tab} содержит данные о валидационной точности для различных методов инициализации после последней эпохи обучения, значения валидационной точности при наивысшем уровне шума изображения от адверсарной атаки и информацию о значениях точности для наивысшего уровня шума в параметрах модели.

\section{Заключение по главе}

В главе были рассмотрены методы передачи знаний между нейронными сетями в условиях различных доменов и сложности данных.

Мультидоменная дистилляция продемонстрировала эффективность передачи знаний от более сложной модели, обученной на большом наборе данных, к менее сложной модели, обучаемой на малом наборе данных того же или другого домена. Эксперименты в областях компьютерного зрения и обработки естественного языка подтвердили улучшение качества аппроксимации студенческой модели. Ожидаемо, обучение в рамках одного домена показало лучшие результаты по сравнению с использованием адаптации домена, однако последняя также обеспечила значительное улучшение по сравнению с базовыми подходами.

Антидистилляция решила задачу расширения модели для работы с более сложными наборами данных. Предложенный метод передачи знаний от простой модели к более сложной не только повысил точность на сложных данных, но и увеличил устойчивость модели к шуму во входных данных и нормальному шуму в параметрах модели. Эксперименты на наборе данных Fashion-MNIST подтвердила эффективность подхода.

Оба метода открывают перспективы для практического применения в условиях ограниченных данных и необходимости адаптации моделей к новым доменам. Дальнейшие исследования будут направлены на интеграцию байесовских методов дистилляции, учитывающих распределения параметров, а также на применение разработанных подходов к другим архитектурам нейронных сетей и наборам данных.
