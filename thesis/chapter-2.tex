В современных задачах оптимизации, к которым редуцируется процесс обучения моделей глубокого обучения, фундаментальную роль играет анализ свойств целевой функции потерь $\mathcal{L}(\boldsymbol{\theta})$, заданной на многомерном пространстве параметров $\boldsymbol{\theta} \in \mathbb{R}^n$.
Глубокие нейронные сети, обладающие способностью к аппроксимации сложных нелинейных зависимостей, порождают высокосложные не выпуклые функции потерь с многочисленными локальными минимумами, седловыми точками и сложным ландшафтом оптимизационной задачи.
Если градиент $\nabla \mathcal{L}(\boldsymbol{\theta})$ характеризует скорость и направление наискорейшего спуска в параметрическом пространстве, то матрица Гессе $\mathbf{H}(\mathcal{L})$~--- симметричная матрица вторых частных производных функции потерь~--- предоставляет информацию о ее локальной кривизне, описывающая геометрические свойства ландшафта.
Матрица Гессе содержит информацию о локальном поведении функции в окрестности заданной точки, позволяя не только предсказывать траекторию оптимизации, но и анализировать устойчивость найденных решений.

Формальное определение матрицы Гессе для функции $\mathcal{L}(\boldsymbol{\theta})$ от $n$ параметров задается следующим выражением:
\[
    \mathbf{H}(\mathcal{L})_{ij} = \frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j},
\]
где индексы $i, j = 1, \dots, n$ соответствуют компонентам вектора параметров $\boldsymbol{\theta}$.
Для функций, обладающих непрерывными вторыми производными, матрица Гессе симметрична в силу Теоремы Шварца-Клеро-Янга о равенстве смешанных производных.
Эта симметричность обеспечивает вещественность всех собственных значений и ортогональность соответствующих собственных векторов, что имеет фундаментальное значение для спектрального анализа в дальнейшем.

Применения матрицы Гессе в контексте глубокого обучения проявляется в решении разнообразных теоретических и практических задач.
В аспекте оптимизации, на основе матрицы Гессе строятся методы второго порядка, такие как метод Ньютона, который использует обратную матрицу Гессе~$\mathbf{H}^{-1}$ для вычисления адаптивных направлений обновления параметров, учитывающих локальную кривизну поверхности потерь.
Это свойство обеспечивает существенное ускорение сходимости в окрестности локального минимума по сравнению с методами первого порядка, основанными исключительно на градиентной информации.
Однако прямая реализация методов второго порядка сопряжена с существенными вычислительными сложностями, что стимулировало развитие квази-ньютоновских методов и методов приближенного вычисления обратной матрицы Гессе.

В контексте теоретического анализа моделей машинного обучения, матрица Гессе вносит существенный вклад в оценку сложности и обобщающей способности моделей. 
Собственные значения матрицы Гессе, вычисленные в стационарной точке, содержат информацию о геометрии ландшафта функции потерь.
Спектральный анализ матрицы Гессе позволяет количественно охарактеризовать локальную кривизну функции потерь вдоль различных направлений в параметрическом пространстве, выявить наличие седловых точек и оценить устойчивость найденного решения:
\begin{itemize}
    \item[--] Малые собственные значения соответствуют направлениям с незначительной кривизной~--- так называемым ``плоским'' регионам, где параметры могут варьироваться без существенного роста ошибки.
    Эти направления часто ассоциируются с параметрами, оказывающими незначительное влияние на выход модели, или с симметриями в архитектуре сети.
    В противоположность этому, большие собственные значения указывают на ``острые'' минимумы с выраженной кривизной.
    Эмпирические и теоретические исследования подтверждают, что плоские минимумы демонстрируют улучшенную обобщающую способность, обусловленную их пониженной чувствительностью к малым возмущениям в данных и параметрах модели.
    Этот феномен, известный как "острота" минимума, активно изучаится в современной теории глубокого обучения и оптимизации.
    \item[--] След матрицы Гессе, равный сумме ее собственных значений, является интегральной характеристикой общей кривизны функции потерь. 
    Детальный анализ спектрального состава матрицы Гессе, в частности оценка кратности собственных значений вблизи нуля, позволяет количественно оценить эффективную размерность пространства параметров, влияющих на выход модели, и идентифицировать структурную избыточность в архитектуре нейронной сети.
    Кроме того, распределение собственных значений матрицы Гессе тесно связано с устойчивостью модели к шуму, способность к интерполяции данных и обобщающая способность на тестовых выборках.
\end{itemize}

Таким образом, матрица Гессе является не только эффективным инструментов для ускорения процесса оптимизации, но и является аналитическим аппаратом для анализа внутренних свойств модели: от устойчивости найденного решения до прогнозирования его способности к обобщению на новые данные.
Однако, использование матрицы Гессе в задачах глубокого обучения с миллионами и миллиардами параметров сталкивается с вычислительными ограничениями, поскольку требования к памяти и вычислительным ресурсам для хранения и обращения плотной матрицы размерности~$n \times n$ становятся непрактичными для реальных приложений.
Это методологическое ограничение обуславливает актуальность разработки эффективных методов аппроксимации матрицы Гессе и ее ключевых спектральных характеристик, чему и посвящена настоящая глава.
Современные подходы к решению этой проблемы включают методы случайного проектирования, разложения Кронекера, диагональные и блочно-диагональные аппроксимации, а также методы, основанные на теории случайных матриц.

\section{Полносвязная нейросетевая модель глубокого обучения}

Рассмотрим формальную постановку задачи $K$--классовой классификации с использованием функции потерь кросс-энтропии.
В данной постановке входные данные представляются вектором $\mathbf{x} \in \mathbb{R}^{l}$, а выходные данные~--- вектором $\mathbf{y} \in \mathbb{R}^{K}$, имеющим структуру one-hot кодирования, где все компоненты равны нулю, за исключением позиции $y_k = 1$, соответствующей истинной метке класса для входного образца $\mathbf{x}$.
Такое представление данных является стандартным для задач классификации и позволяет естественным образом использовать категориальное распределение для моделирования неопределенности предсказаний.

Рассматривается $L$-слойная полносвязная нейронная сеть $f_{\boldsymbol{\theta}}(\cdot)$ с функцией активации ReLU, применяемой после каждого линейного преобразования.
Выбор функции активации ReLU обусловлен ее вычислительной эффективностью и свойством устранять проблему затухающих градиентов, а также ее удобство для анализа в теоретическом анализе нейросетевых архитектур.
Для функции активации ReLU, определяемой как $\sigma(\mathbf{x}) = \left[ \mathbf{x} \geqslant\mathbf{0} \right] \mathbf{x},$ где $[\cdot]$ обозначает поэлементную индикаторную функцию, выход сети представляет собой вектор логитов $\mathbf{z} \in \mathbb{R}^{K}$.
Вычисление логитов осуществляется посредством последовательного применения следующих рекуррентных соотношений:
\begin{align}
    \mathbf{z}^{(p)} &= \mathbf{W}^{(p)} \mathbf{x}^{(p)} + \mathbf{b}^{(p)}, \\
    \mathbf{x}^{(p+1)} &= \sigma(\mathbf{z}^{(p)}).
\end{align}
Здесь $\mathbf{x}^{(p)}$ и $\mathbf{z}^{(p)}$ обозначают вход и выход $p$-го слоя соответственно, при этом полагается $\mathbf{x}^{(1)} = \mathbf{x}$ и $\mathbf{z} = f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{z}^{(L)}$. 
Совокупность всех параметров модели обозначается как $\boldsymbol{\theta} = \mathrm{col}(\mathbf{w}^{(1)}, \mathbf{b}^{(1)}, \ldots, \mathbf{w}^{(L)}, \mathbf{b}^{(L)}) \in \mathbb{R}^{n}$. Для $p$-го слоя $\mathbf{w}^{(p)}$ представляет собой векторизованную матрицу весов $\mathbf{W}^{(p)}$, а $\mathbf{b}^{(p)}$~--- соответствующий вектор смещений.
Общее число параметров $n$ в таких моделях может достигать миллионов и даже миллиардов, что и создает вычислительные трудности для точного вычисления матрицы Гессе.

Выходы модели определяется как $\mathbf{p} = \mathrm{softmax}(\mathbf{z}) \in \mathbb{R}^{K}$, где каждая компонента вычисляется по формуле:
\[
    p_i = \mathrm{softmax}(\mathbf{z})_i = \dfrac{\exp{(z_i)}}{\sum_{j=1}^{K} \exp{(z_j)}} \in (0; 1).
\] 
Функция потерь представляет собой стандартную кросс-энтропийную функцию ошибки:
\[
    \ell(\mathbf{z}, \mathbf{y}) = \mathrm{CE}(\mathbf{p}, \mathbf{y}) = - \sum_{k=1}^{K} y_k \log p_k \in \mathbb{R}^{+}.
\]
Эта функция является выпуклой по логитам $\mathbf{z}$, но невыпуклой по параметрам сети $\boldsymbol{\theta}$ из-за сложной композиционной структуры нейронной сети.

Согласно установленным результатам в литературе \cite{sagun2018empiricalanalysishessianoverparametrized}, применение цепного правила для матриц второго порядка \cite{skorski2019chainruleshessianhigher} позволяет декомпозировать матрицу Гессе на сумму двух структурно различных компонент:
\[
    \mathbf{H}_{i}(\boldsymbol{\theta}) = \underbrace{\nabla_{\boldsymbol{\theta}} \mathbf{z}_i \dfrac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_{i}^2} \nabla_{\boldsymbol{\theta}} \mathbf{z}_i^{\text{T}} }_{\text{G-компонента}} + \underbrace{\sum\limits_{k=1}^{K} \dfrac{\partial \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial z_{ik}} \nabla^2_{\boldsymbol{\theta}} z_{ik}}_{\text{H-компонента}},
\]
где $\nabla_{\boldsymbol{\theta}} \mathbf{z}_i \in \mathbb{R}^{P \times K}$ представляет собой матрицу Якоби функции нейронной сети по параметрам, а $\dfrac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_{i}^2}$~--- матрицу Гессе функции потерь относительно выходных логитов для $i$-го наблюдения. Первое слагаемое (G-компонента) отражает влияние кривизны функции потерь, в то время как второе слагаемое (H-компонента)~--- кривизну самой нейронной сети.

Эмпирические исследования \cite{pmlr-v97-ghorbani19b,sagun2018empiricalanalysishessianoverparametrized,papyan2019spectrumdeepnethessiansscale} демонстрируют, что спектральное распределение матрицы Гессе характеризуется наличием основной массы собственных значений, сосредоточенной вблизи нуля (обусловленной H-компонентой), и выбросов, распределенных в области ненулевых значений (обусловленных G-компонентой). Это бимодальное распределение собственных значений является характерной чертой гессиинов глубоких нейронных сетей и отражает фундаментальные свойства параметрического пространства таких моделей. Вследствие данной спектральной структуры, для практического анализа наиболее релевантной является G-компонента, что обосновывает использование следующей аппроксимации:
\[
    \mathbf{H}_{i}(\boldsymbol{\theta}) \approx \nabla_{\boldsymbol{\theta}} \mathbf{z}_i \dfrac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_{i}^2} \nabla_{\boldsymbol{\theta}} \mathbf{z}_i^{\text{T}}.
\]

Дополнительное теоретическое обоснование данной аппроксимации предоставляется в рамках теории ядра нейронного касательного пространства \cite{NEURIPS2018_5a4be1fa,Lee_2020}, где предполагается линейная зависимость логитов $\mathbf{z}$ от параметров $\boldsymbol{\theta}$ в окрестности точки оптимума. Данное предположение имплицирует исчезающую кривизну логитов $\nabla^2_{\boldsymbol{\theta}} z_{ik}$, что влечет тождественное обращение в ноль H-компоненты.

На основе работ \cite{wu2022dissecting}, предлагающих аналитическую аппроксимацию G-компоненты для полносвязных нейронных сетей, принимается следующая параметризация: $\mathbf{H}_{i}(\boldsymbol{\theta}) \approx \mathbf{F}_i^{\text{T}} \mathbf{A}_i \mathbf{F}_i$.
Эта факторизация позволяет эффективно вычислять приближения гессиана без явного построения полной матрицы, используя разложения в матрицы меньшей размерности.
Введем систему обозначений (для упрощения записи индекс $i$ опущен):

\begin{itemize}
    \item Матричное представление функции активации ReLU:
    \[
        \mathbf{D}^{(p)} = \mathrm{diag}([\mathbf{z}^{(p)} \geqslant \mathbf{0}]),
    \]
    Эта диагональная матрица кодирует паттерн активации нейронов на $p$-м слое и играет основную роль в определении функциональной структуры сети.
    
    \item Матрица прямого распространения от $p$-го слоя к выходу:
    \[
        \mathbf{G}^{(p)} = \dfrac{\partial \mathbf{z}}{\partial \mathbf{z}^{(p)}} = \mathbf{W}^{(L)} \mathbf{D}^{(L-1)} \mathbf{W}^{(L-1)} \mathbf{D}^{(L-2)} \cdot \ldots \cdot \mathbf{D}^{(p)}, 
    \]
    Эта матрица описывает, как изменения в активациях на $p$-м слое через последующие слои к выходу сети.
    
    \item Блочная матрица всех производных логитов по параметрам:
    \[
        \mathbf{F}^{\text{T}} =
        \begin{pmatrix}
            (\mathbf{G}^{(1)})^{\text{T}} \otimes \mathbf{x}^{(1)} \\
            (\mathbf{G}^{(1)})^{\text{T}} \\ 
            \vdots \\
            (\mathbf{G}^{(L)})^{\text{T}} \otimes \mathbf{x}^{(L)} \\
            (\mathbf{G}^{(L)})^{\text{T}} \\ 
        \end{pmatrix}, 
    \]
    где $\otimes$ обозначает произведение Кронекера. Эта блочная структура естественным образом отражает слоистую архитектуру сети и позволяет эффективное вычисление.
    
    \item Гессиан функции потерь относительно логитов, имеющий структуру ковариационной матрицы \cite{singla2019understanding}:
    \[
        \mathbf{A} = \nabla^2_\mathbf{z} \ell(\mathbf{z}, \mathbf{y}) = \mathrm{diag}(\mathbf{p}) - \mathbf{p} \mathbf{p}^{\text{T}}.
    \]
    Эта матрица является положительно полуопределенной и вырожденной, что отражает инвариантность функции softmax к сдвигам в пространстве логитов.
\end{itemize}

На основе предложенной параметризации получена верхняя оценка спектральной нормы матрицы Гессе в полносвязной нейронной сети, формулируемая в Теореме~\eqref{theorem:hess-kiselev-theorem}.


\subsection{Спектральная оценка матрицы Гессе}

Теорема~\ref{theorem:hess-kiselev-theorem} устанавливает верхнюю оценку спектральной нормы матрицы Гессе для полносвязной нейронной сети с функцией активации ReLU. Условия Теоремы включают ограниченность спектральных норм матриц весов всех слоев и норм входных векторов, что является естественным предположением для большинства практических приложений. Полученная оценка демонстрирует экспоненциальную зависимость от глубины сети~$L$, что согласуется с известными результатами о возрастании сложности оптимизационного ландшафта с увеличением глубины нейронной сети.
\begin{theorem}\label{theorem:hess-kiselev-theorem}
    Рассмотрим~$L$-слойную полносвязную нейронную сеть с функцией активации ReLU и без членов смещения, применяемую для решения задачи классификации на~$K$ классов.
    Предположим, что выполнены условия:
    \begin{align}
        \| \mathbf{W}^{(p)} \|_2 &\leqslant M_{\mathbf{W}},\\
        \| \mathbf{x}_i \|_2 &\leqslant M_{\mathbf{x}},
    \end{align}
    для всех слоев~$p = 1, \ldots, L$ в сети и для всех объектов~$i = 1, \ldots, m.$
    Тогда для любого объекта~$i = 1, \ldots, m$ выполняется следующее неравенство:
    \begin{align}
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \dfrac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1}.
    \end{align}
\end{theorem}
\begin{proof}
    Для упрощения, опустим индекс~$i$, соответствующий конкретному объекту в выборке, поскольку оценка проводится для произвольного фиксированного объекта.
    
    Вначале воспользуемся факторизацией гессиана, полученной ранее: $\mathbf{H}(\boldsymbol{\theta}) = \mathbf{F}^\top \mathbf{A} \mathbf{F}$.
    В силу субмультипликативности спектральной нормы матрицы, получаем следующую оценку:
    \begin{align}
        \left\| \mathbf{H}(\boldsymbol{\theta}) \right\|_2 = \left\| \mathbf{F}^\top \mathbf{A} \mathbf{F} \right\|_2 \leqslant \left\| \mathbf{F}^\top \right\|_2 \cdot \left\| \mathbf{A} \right\|_2 \cdot \left\| \mathbf{F} \right\|_2.
    \end{align}
    Учитывая, что~$\left\| \mathbf{F}^\top \right\|_2 = \left\| \mathbf{F} \right\|_2$ для любой матрицы, приходим к оценке:
    \[
        \left\| \mathbf{H}(\boldsymbol{\theta}) \right\|_2 \leqslant \left\| \mathbf{A} \right\|_2 \cdot \left\| \mathbf{F} \right\|_2^2.
    \]
    
    Далее детально рассмотрим каждое слагаемое отдельно, начиная с оценки спектральной нормы матрицы $\mathbf{A}$. В силу эквивалентности матричных норм выполняется неравенство между спектральной нормой и нормой Фробениуса:
    \[
        \left\| \mathbf{A} \right\|_2 \leqslant \left\| \mathbf{A} \right\|_F,
    \]
    где $\left\| \mathbf{A} \right\|_F$~--- норма Фробениуса, определяемая как квадратный корень из суммы квадратов всех элементов матрицы. Используя свойства этой нормы для оценки указанного слагаемого. Согласно определению нормы Фробениуса получаем:
    \begin{align}
        \left\| \mathbf{A} \right\|_F^2 &= \left\| \mathrm{diag}(\mathbf{p}) - \mathbf{p} \mathbf{p}^\top \right\|_F^2 = \sum\limits_{k=1}^{K} (p_k - p_k^2)^2 + \sum\limits_{k \neq l} p_k^2 p_l^2 =\\
        &=\sum\limits_{k=1}^{K} p_k^2 (1 - p_k)^2 + \sum\limits_{k \neq l} p_k^2 p_l^2.
    \end{align}
    Поскольку все вероятности удовлетворяют следующему неравенству~$0 \leqslant p_k \leqslant 1$ для всех $k = 1, \ldots, K$ получаем:
    \[
        0 \leqslant p_k^2 \leqslant p_k \quad \text{и} \quad 0 \leqslant (1 - p_k)^2 \leqslant (1 - p_k),
    \]
    а следовательно, для первого слагаемого получаем оценку:
    \[
        \sum\limits_{k=1}^{K} p_k^2 (1 - p_k)^2 \leqslant \sum\limits_{k=1}^{K} p_k (1 - p_k) \leqslant \sum\limits_{k=1}^{K} p_k = 1,
    \]
    где последнее неравенство следует из того, что $p_k(1-p_k) \leqslant p_k$ и $\sum_{k=1}^K p_k = 1$.
    Для второго слагаемого получаем:
    \[
        \sum\limits_{k \neq l} p_k^2 p_l^2 \leqslant \sum\limits_{k \neq l} p_k p_l = \left( \sum\limits_{k=1}^{K} p_k \right)^2 - \sum\limits_{k=1}^{K} p_k^2 = 1 - \sum\limits_{k=1}^{K} p_k^2,
    \]
    поскольку $\left( \sum_{k=1}^K p_k \right)^2 = 1$, а двойная сумма $\sum_{k \neq l} p_k p_l$ равна квадрату суммы за вычетом суммы квадратов.
    Комбинируя полученные оценки, получаем:
    \[
        \left\| \mathbf{A} \right\|_F^2 \leqslant 1 + \left(1 - \sum\limits_{k=1}^{K} p_k^2\right) = 2 - \sum\limits_{k=1}^{K} p_k^2 \leqslant 2,
    \]
    где последнее неравенство следует из неотрицательности~$\sum_{k=1}^K p_k^2$.
    Таким образом, норма Фробениуса матрицы $\mathbf{A}$ ограничена сверху значением $\sqrt{2}$, и следовательно:
    \[
        \left\| \mathbf{A} \right\|_2 \leqslant \left\| \mathbf{A} \right\|_F \leqslant \sqrt{2}.
    \]

    Далее оценим норму~$\left\| \mathbf{F} \right\|_2.$ Для оценки $\left\| \mathbf{F} \right\|_2$ проанализируем спектральную норму матриц $\mathbf{G}^{(p)}$, которые определяются как:
    \[
        \mathbf{G}^{(p)} = \mathbf{W}^{(L)} \mathbf{D}^{(L-1)} \mathbf{W}^{(L-1)} \mathbf{D}^{(L-2)} \cdot \ldots \cdot \mathbf{D}^{(p)}.
    \]
    Используя свойство субмультипликативности спектральной нормы, имеем:
    \[
        \| \mathbf{G}^{(p)} \|_2 \leqslant \|\mathbf{W}^{(L)}\|_2 \cdot \|\mathbf{D}^{(L-1)}\|_2 \cdot \|\mathbf{W}^{(L-1)}\|_2 \cdot \|\mathbf{D}^{(L-2)}\|_2 \cdot \ldots \cdot \|\mathbf{D}^{(p)}\|_2.
    \]
    Причем, так как матрицы~$\mathbf{D}^{(p)}$~--- диагональные матрица с элементами~$0$ или~$1,$ так как является индикаторной матрицы оператора ReLU, то ее спектральная норма не превосходит~$1$. Таким образом, получаем:
    \[
        \|\mathbf{G}^{(p)} \|_2 \leqslant \prod_{s=p}^{L} \| \mathbf{W}^{(s)} \|_2.
    \]
    Далее, так как матрица~$\mathbf{F}$ представляет собой вертикальную конкатенацию блоков вида $(\mathbf{G}^{(p)})^\top \otimes \mathbf{x}^{(p)}$ и $(\mathbf{G}^{(p)})^\top$, то для спектральной нормы такой блочной матрицы справедливо неравенство:
    \[
        \| \mathbf{F} \|_2^2 \leqslant \sum\limits_{p=1}^{L} \left( \| (\mathbf{G}^{(p)})^\top \otimes \mathbf{x}^{(p)} \|_2^2 + \| (\mathbf{G}^{(p)})^\top \|_2^2 \right),
    \]
    где используя свойство спектральной нормы произведения Кронекера:
    \[
        \| \mathbf{A} \otimes \mathbf{B} \|_2 = \| \mathbf{A} \|_2 \cdot \| \mathbf{B} \|_2,
    \]
    а также учитывая, что $\| (\mathbf{G}^{(p)})^\top \|_2 = \| \mathbf{G}^{(p)} \|_2$, получаем:
    \begin{align}
        \| \mathbf{F} \|_2^2 &\leqslant \sum\limits_{p=1}^{L} \left( \| \mathbf{G}^{(p)} \|_2^2 \cdot \| \mathbf{x}^{(p)} \|_2^2 + \| \mathbf{G}^{(p)} \|_2^2 \right) =\\
        &=\sum\limits_{p=1}^{L} \| \mathbf{G}^{(p)} \|_2^2 \left(\| \mathbf{x}^{(p)} \|_2^2 + 1\right).
    \end{align}
    Подставляя полученную ранее оценку для~$\| \mathbf{G}^{(p)} \|_2$, получаем итоговую оценку:
    \[
        \| \mathbf{F} \|_2^2 \leqslant \sum\limits_{p=1}^{L} \left(\| \mathbf{x}^{(p)} \|_2^2 + 1\right) \prod_{s=p}^{L} \| \mathbf{W}^{(s)} \|_2^2.
    \]
    
    Собирая полученные оценки норм~$\|\mathbf{A}\|,\|\mathbf{F}\|$ получаем итоговую оценка для гессиана:
    \[
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant \sqrt{2} \sum\limits_{p=1}^{L} \left(\| \mathbf{x}_i^{(p)} \|_2^2 + 1\right) \prod_{s=p}^{L} \| \mathbf{W}^{(s)} \|_2^2.
    \]
    
    В случае, когда вектора смещений отсутствуют, т.е. $\mathbf{b}^{(p)} = \mathbf{0}$ для всех $p = 1, \ldots, L$. Получаем, что выход каждого слоя оценивается:
    \[
        \| \mathbf{x}_i^{(p)} \|_2 \leqslant \| \mathbf{x}_i \|_2 \prod_{s=1}^{p-1} \| \mathbf{W}^{(s)} \|_2,
    \]
    поскольку каждый слой осуществляет линейное преобразование с последующей нелинейностью ReLU, которая не увеличивает норму. Тогда используя данную оценку, получаем:
    \begin{align}
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 &\leqslant \sqrt{2} \sum\limits_{p=1}^{L} \left( \| \mathbf{x}_i \|_2^2 \prod_{s=1}^{p-1} \| \mathbf{W}^{(s)} \|_2^2 + 1\right) \prod_{s=p}^{L} \| \mathbf{W}^{(s)} \|_2^2 = \\
        &= \sqrt{2} \sum\limits_{p=1}^{L} \left( \| \mathbf{x}_i \|_2^2 \prod_{s=1}^{L} \| \mathbf{W}^{(s)} \|_2^2 + \prod_{s=p}^{L} \| \mathbf{W}^{(s)} \|_2^2 \right) =\\
        &= L \sqrt{2} \| \mathbf{x}_i \|_2^2 \prod_{p=1}^{L} \| \mathbf{W}^{(p)} \|_2^2 + \sqrt{2} \sum\limits_{p=1}^{L} \prod_{s=p}^{L} \| \mathbf{W}^{(s)} \|_2^2.
    \end{align}
    
    Воспользовавшись условиями Теоремы, что для всех $p$ норма матриц~$\| \mathbf{W}^{(p)} \|_2 \leqslant M_{\mathbf{W}}$ и~$\| \mathbf{x}_i \|_2 \leqslant M_{\mathbf{x}}$, то:
    \[ 
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \sum\limits_{p=1}^{L} M_{\mathbf{W}}^{2(L-p+1)}.
    \]
    Причем, заметим, что сумма в правой части представляет собой сумму геометрической прогрессии:
    \[
        \sum\limits_{p=1}^{L} M_{\mathbf{W}}^{2(L-p+1)} = \sum\limits_{k=1}^{L} M_{\mathbf{W}}^{2k} = \dfrac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1},
    \]
    а следовательно получаем итоговую оценку на матрицу Гессе:
    \[
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \dfrac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1}.
    \]
    
    Для однослойной сети~$L=1$ оценка упрощается:
    \[
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant \sqrt{2} M_{\mathbf{W}}^{2} \left( M_{\mathbf{x}}^2 + 1 \right).
    \]
\end{proof}

\begin{remark}
В Теореме~\ref{theorem:hess-kiselev-theorem} получена оценка на матрицу Гессе для однослойной сети~$L=1$ вида:
\[
    \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant \sqrt{2} M_{\mathbf{W}}^{2} \left( M_{\mathbf{x}}^2 + 1 \right).
\]
Данная оценка соответствует эмперическому представлению о том, что кривизна функции потерь пропорциональна квадрату нормы весов и квадрату нормы входных данных.
\end{remark}

Теорема~\ref{theorem:hess-kiselev-lemma} указывает на зависимость спектральной нормы матрицы Гессе от размера скрытого слоя~$h$. Полученная оценка демонстрирует, что норма гессиана растет экспоненциально как по глубине сети~$L$, так и по размеру скрытого слоя $h$, причем степень экспоненты определяется произведением $hM$. Это подчеркивает влияние ширины и глубины сети на сложность оптимизационного ландшафта. Экспоненциальный характер зависимости объясняет известную эмпирическую оценку о том, что глубокие и широкие сети обладают значительно более сложной геометрией функции потерь, что создает дополнительные сложности для методов оптимизации. Полученный результат также подчеркивает важность контроля норм параметров на протяжении всего процесса обучения для обеспечения устойчивости алгоритмов оптимизации.

\begin{theorem}\label{theorem:hess-kiselev-lemma}
    Пусть все параметры модели ограничены некоторой константой~$M > 0$, то есть для всех $i, j = 1, \ldots, h$ и для всех слоев $p = 1, \ldots, L$ выполняется условие~$|w_{ij}^{(p)}| \leqslant M,$ тогда при выполнении условий Теоремы~\ref{theorem:hess-kiselev-theorem} справедливо следующее неравенство:
    \[ 
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 (hM)^{2L} + \sqrt{2} \dfrac{(hM)^2 ((hM)^{2L} - 1)}{(hM)^2 - 1}.
    \]
    Таким образом, имеет место следующая пропорциональность для нормы матрицы Гессе:
    \[
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \propto L (hM)^{2L}.
    \]
\end{theorem}
\begin{proof}
    Для произвольной матрицы~$\mathbf{W}^{(p)} \in \mathbb{R}^{h \times h}$ справедливо неравенство между спектральной и фробениусовой нормами:
    \[
        \| \mathbf{W}^{(p)} \|_2 \leqslant \| \mathbf{W}^{(p)} \|_F = \sum_{i=1}^{h} \sum_{j=1}^{h} \left(w_{ij}^{(p)}\right)^2.
    \]
    Из условия теоремы каждый элемент матрицы ограничен~$|w_{ij}^{(p)}| \leqslant M$ для всех $i, j = 1, \ldots, h$ и всех слоев $p = 1, \ldots, L,$ а следовательно:
    \[
        \left(w_{ij}^{(p)}\right)^2 \leqslant M^2,
    \]
    далее, так как матрица имеет размер $h \times h$, общее число элементов равно $h^2$, и следовательно:
    \[
        \| \mathbf{W}^{(p)} \|_F^2 \leqslant h^2 M^2.
    \]
    
    В Теореме~\ref{theorem:hess-kiselev-theorem} была получена следующая оценка:
    \[
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \dfrac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1},
    \]
    где~$M_{\mathbf{W}}$~--- верхняя оценка спектральных норм матриц весов всех слоев, причем ранее было получено, что~$M_{\mathbf{W}} \leqslant hM,$ а следовательно получаем:
    \[
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 (hM)^{2L} + \sqrt{2} \dfrac{(hM)^2 ((hM)^{2L} - 1)}{(hM)^2 - 1}.
    \]
    
    Для анализа асимптотического поведения полученной оценки, рассмотрим ее поведение при больших значениях~$h$ и~$L$. Первое слагаемое имеет ассимптотику:
    \[
        L \sqrt{2} M_{\mathbf{x}}^2 (hM)^{2L} = \propto L h^{2L},
    \]
    поскольку~$M$ и~$M_{\mathbf{x}}$ являются константами. Второе слагаемое:
    \[
        \sqrt{2} \dfrac{(hM)^2 ((hM)^{2L} - 1)}{(hM)^2 - 1} \propto h^{2L},
    \]
    так как при~$hM > 1$ числитель растет как~$(hM)^{2L+2}$, а знаменатель как~$(hM)^2$.  Таким образом итоговая ассимптотика:
    \[
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \propto L h^{2L}.
    \]
    
    Для однослойной сети оценка упрощается, то есть подставляя~$L=1$, получаем:
    \[
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant \sqrt{2} M_{\mathbf{W}}^{2} (M_{\mathbf{x}}^2 + 1) \leqslant \sqrt{2} (hM)^2 (M_{\mathbf{x}}^2 + 1).
    \]
    Следовательно, в случае одного слоя норма гессиана растет квадратично с размером слоя:
    \[
        \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \propto h^2.
    \]
\end{proof}

Полученные результаты указывают на то, что норма матрицы Гессе является степенной функцией от размера скрытого слоя~$h$ и экспоненциальной функцией от количества слоев~$L$.
Хотя может показаться, что полученная оценка является завышенной, на самом деле это не так.
Дело в том, что если выбрать значение~$h$ большим, то ограничивающая константа~$M$, скорее всего, будет очень малой.
Благодаря этому значение под знаком степени~$2L$, вероятно, окажется меньше единицы. В дальнейшем мы используем эту верхнюю оценку для получения неравенства разности функции потерь.

\begin{remark}
    Полученная оценка имеет важное значение для теории глубокого обучения, поскольку она раскрывает компромисс между шириной и глубиной нейронной сети с точки зрения сложности оптимизации.
    Экспоненциальная зависимость от глубины сети~$L$ объясняет известные практические трудности при обучении очень глубоких сетей, в то время как степенная зависимость от ширины $h$ указывает на более управляемый рост сложности при увеличении размера слоев.
    Учет взаимосвязи между~$h$ и~$M$ позволяет более адекватно интерпретировать полученные оценки и использовать их для практического анализа сходимости методов оптимизации.
\end{remark}

\section{Матричные модели глубокого обучения}

В данном разделе рассматривается общий класс матричных моделей глубокого обучения, которые представляют собой композицию последовательных линейных преобразований и нелинейных функций активации.
Частным случаем такого представления являются сверточные нейронные сети (англ. CNN), а также другие архитектуры, где каждый слой может быть представлен в виде линейного оператора.

Пусть $f_{\boldsymbol{\theta}}(\mathbf{x})$ является суперпозицией $L+1$ слоев с активациями ReLU, что формально записывается как:
\[
    f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{T}^{(L+1)} \circ \sigma \circ \dots \circ \sigma \circ \mathbf{T}^{(1)}(\mathbf{x}).
\]

В этом представлении каждый $\mathbf{T}^{(p+1)}$ представляет собой линейный оператор или его матричное представление, а $\sigma$ обозначает функцию активации ReLU, применяемую поэлементно.
Такая композиционная структура позволяет описывать глубокие нейронные сети как последовательность преобразований, где каждый слой осуществляет линейное отображение с последующей нелинейной активацией.

Промежуточные результаты вычисления функции сети могут быть представлены в виде системы уравнений:
\begin{align}
  \begin{cases}
    \mathbf{z}^{(p+1)} &= \mathbf{T}^{(p+1)}\mathbf{x}^{(p)}, \\
    \mathbf{x}^{(p+1)} &= \sigma(\mathbf{z}^{(p+1)})
  \end{cases}
\end{align}
где выход сети определяется как $f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{z} := \mathbf{z}^{(L+1)}$, а входные данные задаются как $\mathbf{x}^{(0)} := \mathbf{x}$.
Здесь $\mathbf{z}^{(p+1)}$ представляет собой выход линейного оператора на $(p+1)$-м слое до применения активации, а $\mathbf{x}^{(p+1)}$~--- результат после применения функции активации ReLU, который является входом для следующего слоя.

Рассмотрим матрицу~$\boldsymbol{\Lambda}^{(p+1)} := \mathrm{diag}(\mathbf{x}^{(p+1)} > 0)$, зависящую от входных данных, которая кодирует паттерн активации нейронов на $(p+1)$-м слое.
Элементы этой матрицы равны 1 для нейронов с положительной активацией и 0 в противном случае, что отражает свойство функции ReLU "отсекать" отрицательные значения и представляет функцию активации в виде линейного оператора.
Используя эти диагональные матрицы, всю функцию нейронной сети можно представить в виде произведения матриц, а имменно супперпозиций линейных операторов:
\begin{equation}\label{eq::m-net:repr}
    f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)}\dots \boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)}\mathbf{x}.
\end{equation}

Данное представление удобно, так как позволяет рассматривать глубокую нейронную сеть с активациями ReLU как кусочно-линейную функцию, где нелинейность возникает исключительно за счет бинарных переключений в матрицах $\boldsymbol{\Lambda}^{(p)}$, зависящих от входных данных.

Вектор параметров модели объединяет все обучаемые параметры сети: $\boldsymbol{\theta} = \mathrm{col}(\mathbf{W}^{(L+1)}, \dots, \mathbf{W}^{(1)})$, где каждый линейный оператор $\mathbf{T}^{(p)}$ дифференцируемо параметризуется соответствующей частью вектора параметров $\mathbf{W}^{(p)}$. Для анализа модели вводится производная слоя по его параметрам:
\[
    \mathbf{Q}^{(p)} := \frac{\partial \mathbf{T}^{(p)}}{\partial \mathbf{W}^{(p)}},
\]
а затем строится блочно-диагональная матрица, объединяющая эти производные по всем слоям:
\[
    \mathbf{Q}:= \mathrm{diag}({\mathbf{Q}}^{(1)}, \dots, {\mathbf{Q}}^{(L+1)}).
\]

Матрица $\mathbf{Q}^{(p)}$ полностью описывает расположение параметров в $p$-м слое и их влияние на линейное преобразование этого слоя.

Для дальнейшего анализа вводятся дополнительные обозначения, которые описывают предшествующие и последующие преобразования относительно $p$-го слоя. Преобразования, которые последуют после $p$-го слоя:
\begin{align}
    \mathbf{G}^{(p)} &:= \mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)} \dots \mathbf{T}^{(p+1)}\boldsymbol{\Lambda}^{(p)}; \\
    \mathbf{G}^{(L+1)} &:= \mathbf{I};
\end{align}
и преобразования, которые предшествовали $p$-му слою:
\begin{align}
    \mathbf{R}^{(p)} &:= \boldsymbol{\Lambda}^{(p)}\mathbf{T}^{(p)} \dots \boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)};\;\;p = \overline{1,L}; \\ 
    \mathbf{R}^{(0)} &:= \mathbf{I}.
\end{align}
Матрица $\mathbf{G}^{(p)}$ описывает линейные преобразования от $p$-го слоя к выходу сети, в то время как $\mathbf{R}^{(p)}$ описывает линейные преобразование входа до $p$-го слоя.

Используя введенные обозначения, запишем следующие выражения:
\begin{align}
    \mathbf{z} &= \mathbf{G}^{(p)}\mathbf{z}^{(p)},\\
    \mathbf{x}^{(p)} &= \mathbf{R}^{(p)}\mathbf{x}, \\
    \mathbf{z} &= f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{G}^{(p)}\mathbf{T}^{(p)}\mathbf{R}^{(p-1)}\mathbf{x}.
\end{align}
Первое уравнение выражает выход сети через промежуточные значения на $p$-м слое, второе показывает преобразование входного сигнала до $p$-го слоя, а третье дает полное представление выхода сети через параметры $p$-го слоя и преобразования до и после него.

Объединяя матрицы $\mathbf{G}^{(p)}$ и $\mathbf{R}^{(p)}$ в единый оператор, получаем расширенную матрицу:
\begin{align}
\mathbf{F}^T := 
    \begin{pmatrix}
        \mathbf{G}^{(1)^T} \otimes \mathbf{R}^{(0)}\mathbf{x} \\
        \vdots \\
        \mathbf{G}^{(k)^T} \otimes \mathbf{R}^{(k-1)}\mathbf{x} \\
        \vdots \\
        \mathbf{G}^{(L+1)^T} \otimes \mathbf{R}^{(L)}\mathbf{x}
    \end{pmatrix}.
\end{align}
Эта блочная матрица содержит в себе всю информацию о том, как изменения параметров в различных слоях влияют на выход сети.
Каждый блок этой матрицы соответствует определенному слою и содержит информацию как о линейных преобразованиях от этого слоя к выходу ($\mathbf{G}^{(k)^T}$), так и о преобразовании входа до этого слоя ($\mathbf{R}^{(k-1)}\mathbf{x}$).

В случае использования функции потерь кросс-энтропии (CE) для задач классификации, гессиан функции потерь относительно логитов имеет структуру:
\[
    \mathbf{A} := \nabla^2_{\mathbf{z}} \ell = \mathrm{diag}(\mathbf{p}) - \mathbf{p}\mathbf{p}^T,
\]
где $\mathbf{p} := \mathrm{softmax}(\mathbf{z})$ представляет вектор вероятностей, предсказанных моделью.
Матрица $\mathbf{A}$ является ковариационной матрицей многомерного распределения и обладает свойствами положительной полуопределенности и вырожденности, что отражает инвариантность функции softmax к сдвигам в пространстве логитов.

\subsection{Матричная факторизация матрицы Гессе}

\begin{theorem}\label{theorem:m-net:hesstruct}
    Пусть функция нейронной сети $f_{\boldsymbol{\theta}}(\mathbf{x})$ представима в виде \eqref{eq::m-net:repr}, тогда матрица Гессе функции потерь относительно параметров модели может быть представлен в факторизованной форме:
    $\mathbf{H}_O(\boldsymbol{\theta}) = \mathbf{Q}^{T}\mathbf{F}^{T}\mathbf{A}\mathbf{F}\mathbf{Q}$, где $\mathbf{H}_O$ описывает G-компоненту матрицы Гессе.
\end{theorem}
\begin{proof}
Доказательство Теоремы основано на последовательном применении цепного правила матричного дифференцирования и использовании свойств произведения Кронекера.
Исходное представление выхода матричной нейросетевой модели:
\[
    \mathbf{z} = f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)}\mathbf{T}^{(L)}....\boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)}\mathbf{x}.
\]

Производные выхода сети по параметрам модели вычисляются с использованием цепного правила:
\begin{align}
 \frac{\partial\mathbf{z}}{\partial\mathbf{W}^{(p)}} = \frac{\partial\mathbf{z}}{\partial\mathbf{z}^{(p)}} \frac{\partial\mathbf{z}^{(p)}}{\partial\mathbf{T}^{(p)}} \frac{\partial\mathbf{T}^{(p)}}{\partial\mathbf{W}^{(p)}}.
\end{align}

Для вычисления $\frac{\partial\mathbf{z}^{(p)}}{\partial\mathbf{T}^{(p)}}$ используется тождество для векторизации матричных произведений: $vec(\mathbf{B}\mathbf{V}\mathbf{A}^T) = (\mathbf{A} \otimes \mathbf{B})vec(\mathbf{V})$.
Применяя это тождество с $\mathbf{A} = \mathbf{I}$ и векторизуя $\mathbf{z}^{(p)} = \mathbf{T}^{(p)}\mathbf{x}^{(p-1)}$, получаем:
\[
    vec(\mathbf{z}^{(p)}) = vec(\mathbf{T}^{(p)}\mathbf{x}^{(p-1)}) = (\mathbf{I} \otimes \mathbf{x}^{(p-1)})vec(\mathbf{T}^{(p)}).
\]

Отсюда следует, что:
\[
    \frac{\partial \mathbf{z}^{(p)}}{\partial \mathbf{T}^{(p)}} = \mathbf{I} \otimes \mathbf{x}^{(p-1)^T}.
\]

Используя выражение $\mathbf{z} = \mathbf{G}^{(p)}\mathbf{z}^{(p)}$, получаем производную выхода сети по промежуточному значению:
\[
    \frac{\partial \mathbf{z}}{\partial \mathbf{z}^{(p)}} = \mathbf{G}^{(p)}.
\]

По определению $\mathbf{Q}^{(p)}$ имеем:
\[
    \frac{\partial\mathbf{T}^{(p)}}{\partial\mathbf{W}^{(p)}} = {\mathbf{Q}}^{(p)}.
\]

Для объединения этих выражений используется свойство произведения Кронекера: если $\mathbf{A}_i \in \mathbb{R}^{m_i \times n_i}$, то $\mathbf{A}_1 \otimes \mathbf{A}_2 = (\mathbf{A}_1 \otimes \mathbf{I}_{m_2})(\mathbf{I}_{m_1} \otimes \mathbf{A}_2)$.
Применяя это свойство с $m_2 = 1$, получаем:
\[
    \mathbf{G}^{(p)}\big(\mathbf{I} \otimes \mathbf{x}^{(p-1)^T}\big) = \big(\mathbf{G}^{(p)} \otimes \mathbf{I}_1\big)\big(\mathbf{I} \otimes \mathbf{x}^{(p-1)^T}\big) = \mathbf{G}^{(p)} \otimes \mathbf{x}^{(p-1)^T}.
\]

Подставляя все компоненты в исходную формулу для производной, получаем окончательное выражение:
\[
    \frac{\partial\mathbf{z}}{\partial\mathbf{W}^{(p)}} = (\mathbf{G}^{(p)} \otimes \mathbf{I}_1)(\mathbf{I} \otimes \mathbf{x}^{(p-1)^T})\mathbf{Q}^{(p)} = (\mathbf{G}^{(p)} \otimes \mathbf{x}^{(p-1)^T})\mathbf{Q}^{(p)}.
\]

Используя результаты работ по анализу гессиана в нейронных сетях \cite{singh2023hessianperspectivenatureconvolutional}, получаем выражение для блоков матрицы Гессе:
\begin{align}
    \mathbf{H}_O^{(kl)} &= J(\boldsymbol{\theta})^T \mathbf{A} J(\boldsymbol{\theta}) = \\
    &= \mathbf{Q}^{(k)^T}(\mathbf{G}^{(k)^T} \otimes \mathbf{R}^{(k-1)}\mathbf{x})A(\mathbf{G}^{(l)} \otimes \mathbf{x}^T\mathbf{R}^{(l-1)^T})\mathbf{Q}^{(l)}.
\end{align}

Объединяя все блоки в единую матрицу, получаем итоговое выражение для матрицы Гессе:
\[
    \mathbf{H}_O = \mathbf{Q}^\mathbf{T}\mathbf{F}\mathbf{A}\mathbf{F}^\mathbf{T}\mathbf{Q}.
\]
\end{proof}

Теорема~\ref{theorem:m-net:hesstruct} устанавливает результат о структуре гессиана в матричных моделях глубокого обучения.
Предложенная факторизация позволяет эффективно анализировать и вычислять гессиан без необходимости явного построения полной матрицы вторых производных, что особенно важно для моделей с большим количеством параметров.
Структура $\mathbf{H}_O = \mathbf{Q}^\mathbf{T}\mathbf{F}\mathbf{A}\mathbf{F}^\mathbf{T}\mathbf{Q}$ подчеркивает, что гессиан может быть представлен как преобразование "внутреннего" гессиана $\mathbf{A}$ (зависящего только от логитов и функции потерь) с помощью матриц $\mathbf{F}$ и $\mathbf{Q}$, которые capture архитектурные свойства сети и параметризацию слоев соответственно.

\subsection{Оценка спектральных норм матрицы Гессе}

Теорема~\ref{theorem:m-net:hessnorm} устанавливает верхнюю оценку для нормы матрицы Гессе в терминах структурных параметров нейронной сети.
Полученная оценка демонстрирует экспоненциальную зависимость от глубины сети~$L$ и полиномиальную зависимость от норм параметров~$\mathbf{Q}^{(p)}$ и~$\mathbf{T}^{(p)}$.
Особенностью данной теоремы является учет влияния всех слоев сети через мультипликативные члены~$w_{\mathbf{T}}^{2L}$ и аддитивные члены~$(L+1)$, что качественно отражает накопление сложности при увеличении глубины архитектуры.
Оценка также подчеркивает важность контроля норм весовых матриц на протяжении всего процесса обучения для обеспечения устойчивости оптимизации.

\begin{theorem}\label{theorem:m-net:hessnorm} 
    Пусть нейронная сеть $f_{\boldsymbol{\theta}}(\mathbf{x})$ представима в виде~\eqref{eq::m-net:repr}.
    Пусть для всех слоев $p$ выполнены условия:
    \begin{align}
        \left\|\mathbf{Q}^{(p)}\right\| &\leqslant q, \\
        \left\|\mathbf{T}^{(p)}\right\|^2 &\leqslant w_{\mathbf{T}}^2.
    \end{align}
    
    Тогда справедлива оценка:
    \[
        \left\|\mathbf{H}_O\right\| \leqslant \sqrt{2}q^2\left\|\mathbf{x}\right\|^2(L+1)w_{\mathbf{T}}^{2L}.
    \]
\end{theorem}
\begin{proof}
Using the results of the previous Lemma~\ref{theorem:m-net:hesstruct}, it is enough for us to evaluate the upper bound of the expression: $\left\|\mathbf{Q}\right\|^2\left\|\mathbf{F}\right\|^2\left\|\mathbf{A}\right\|$

In the work~\cite{grabovoi2024unraveling748584228}, the norm of matrix $\mathbf{A}$ was examined, and it was proven that: 
\[
    \left\|\mathbf{A}\right\| \leqslant \sqrt{2}.
\] 
Norm of block-diagonal matrix is not greater than max of block's norm \\
\[
    \left\|\mathbf{Q}\right\|^2 \leqslant \max\limits_{i=1,\dots,L+1}\left\|\mathbf{Q}^{(i)}\right\|^2 \leqslant q^2.
\]

Norm of matrix product is less or equal then product of norms:
\[
    \left\|\mathbf{G}^{(p)}\right\|^2 \leqslant \left\|\mathbf{T}^{(p+1)}\right\|^2 \dots\left\|\mathbf{T}^{(L+1)}\right\|^2 \leqslant w_{\mathbf{T}}^{2(L - p + 1)}.
\]
\[
    \left\|\mathbf{R}^{(p-1)}\right\|^2 \leqslant \left\|\mathbf{T}^{(1)}\right\|^2 \dots \left\|\mathbf{T}^{(p-1)}\right\|^2 \leqslant w_{\mathbf{T}}^{2(p-1)}.
\]
The spectral matrix norm of the Kronecker product is equal to their ordinary product norm. Spectral norm of vertical stacked matrices is less or equal then sum of norms of it's blocks
\begin{align}
    & \left\|\mathbf{F}\right\|^2 \leqslant \sum\limits_{p=1}^{L+1}\left\|\mathbf{G}^{(p+1)\top}\otimes \mathbf{R}^{(p-1)}\mathbf{x}\right\|^2 = \\ 
    & = \sum\limits_{p=1}^{L+1}\left\|\mathbf{G}^{(p)}\right\|^2\left\|\mathbf{R}^{(p-1)}\mathbf{x}\right\|^2.
\end{align}
Substituting the obtained estimates into the $\left\|\mathbf{H}_O\right\|$ formula we get
\begin{align}
    & \left\|\mathbf{F}\right\|^2 \leqslant \left\|\mathbf{x}\right\|^2\sum\limits_{p=1}^{L+1}w_{\mathbf{T}}^{2L} \leqslant \left\|\mathbf{x}\right\|^2(L+1)w_{\mathbf{T}}^{2L}. \\
    & \left\|\mathbf{H}_O\right\| \leqslant \left\|\mathbf{Q}\right\|^2 \left\|\mathbf{F}\right\|^2\left\|\mathbf{A}\right\| \leqslant \sqrt{2}\left\|\mathbf{x}\right\|^2q^2(L+1)w_{\mathbf{T}}^{2L}.
\end{align}
\end{proof}
\begin{proof}
Согласно Теоремы~\ref{theorem:m-net:hesstruct}, матрица Гессе  представима в виде~$\mathbf{H}_O = \mathbf{Q}^\top \mathbf{F}^\top \mathbf{A} \mathbf{F} \mathbf{Q}$. 
Используя субмультипликативное свойство спектральной нормы, получаем:
\[
    \|\mathbf{H}_O\| \leqslant \|\mathbf{Q}\|^2 \|\mathbf{F}\|^2 \|\mathbf{A}\|.
\]
Таким образом, задача сводится к оценке норм~$\mathbf{A}$, $\mathbf{F}$ и $\mathbf{Q}$.

Матрица~$\mathbf{A} = \mathrm{diag}(\mathbf{p}) - \mathbf{p} \mathbf{p}^{\text{T}}$ представляет собой гессиан функции потерь относительно логитов. 
Согласно результатам работы~\cite{grabovoi2024unraveling748584228}, для кросс-энтропийной функции потерь справедлива оценка:
\[
    \|\mathbf{A}\| \leqslant \sqrt{2}.
\]

Матрица~$\mathbf{Q}$ является блочно-диагональной с блоками $\mathbf{Q}^{(1)}, \dots, \mathbf{Q}^{(L+1)}$. 
Для блочно-диагональных матриц спектральная норма не превосходит максимальной нормы ее блоков:
\[
    \|\mathbf{Q}\| \leqslant \max_{i=1,\dots,L+1} \|\mathbf{Q}^{(i)}\|.
\]
Из условия теоремы~$\|\mathbf{Q}^{(i)}\| \leqslant q$ для всех $i$, а следовательно:
\[
    \|\mathbf{Q}\|^2 \leqslant q^2.
\]

Матрица~$\mathbf{G}^{(p)}$ и матрица~$\mathbf{R}^{(p)}$ представляют собой произведения матриц $\mathbf{T}^{(i)}$ и диагональных матриц активаций $\boldsymbol{\Lambda}^{(i)}$. 
Поскольку диагональные элементы матриц $\boldsymbol{\Lambda}^{(i)}$ равны 0 или 1, их спектральная норма не превосходит 1.
Для матрицы~$\mathbf{G}^{(p)} = \mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)} \cdots \mathbf{T}^{(p+1)}\boldsymbol{\Lambda}^{(p)}$ применяем субмультипликативное свойство:  
\[
    \|\mathbf{G}^{(p)}\| \leqslant \|\mathbf{T}^{(L+1)}\| \cdot \|\boldsymbol{\Lambda}^{(L)}\| \cdots \|\mathbf{T}^{(p+1)}\| \cdot \|\boldsymbol{\Lambda}^{(p)}\| \leqslant w_{\mathbf{T}}^{L-p+1}.
\]
Аналогично для $\mathbf{R}^{(p-1)} = \boldsymbol{\Lambda}^{(p-1)}\mathbf{T}^{(p-1)} \cdots \boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)}$ получаем оценку:
\[
    \|\mathbf{R}^{(p-1)}\| \leqslant w_{\mathbf{T}}^{p-1}.
\]

Матрица~$\mathbf{F}$ представляет собой вертикальную конкатенацию блоков вида~$\mathbf{G}^{(p)\top} \otimes \mathbf{R}^{(p-1)}\mathbf{x}$. 
Для вертикально сконкатенированных матриц спектральная норма оценивается как корень из суммы квадратов норм блоков:
\[
    \|\mathbf{F}\|^2 \leqslant \sum_{p=1}^{L+1} \|\mathbf{G}^{(p)\top} \otimes \mathbf{R}^{(p-1)}\mathbf{x}\|^2,
\]
причем, используя свойство нормы произведения Кронекера~$\|\mathbf{A} \otimes \mathbf{B}\| = \|\mathbf{A}\| \cdot \|\mathbf{B}\|$, получаем:
\[
    \|\mathbf{F}\|^2 \leqslant \sum_{p=1}^{L+1} \|\mathbf{G}^{(p)}\|^2 \cdot \|\mathbf{R}^{(p-1)}\mathbf{x}\|^2.
\]
Учитывая, что~$\|\mathbf{R}^{(p-1)}\mathbf{x}\| \leqslant \|\mathbf{R}^{(p-1)}\| \cdot \|\mathbf{x}\|$, и подставляя оценки полученные ранее получаем:
\[
    \|\mathbf{F}\|^2 \leqslant \|\mathbf{x}\|^2 \sum_{p=1}^{L+1} w_{\mathbf{T}}^{2(L-p+1)} \cdot w_{\mathbf{T}}^{2(p-1)} = \|\mathbf{x}\|^2 \sum_{p=1}^{L+1} w_{\mathbf{T}}^{2L} = \|\mathbf{x}\|^2 (L+1) w_{\mathbf{T}}^{2L}.
\]

Собирая все полученные оценки получаем:
\[
    \|\mathbf{H}_O\| \leqslant \|\mathbf{Q}\|^2 \cdot \|\mathbf{F}\|^2 \cdot \|\mathbf{A}\| \leqslant q^2 \cdot \|\mathbf{x}\|^2 (L+1) w_{\mathbf{T}}^{2L} \cdot \sqrt{2}.
\]
\end{proof}

Рассмотрим частный случай модели глубокого оюучения, удовлетворяющего условию матричной факторизации для сверточной нейронной сети~(англ. CNN) с одномерной сверткой можно получить оценки на норму матрицу Гессе, как показано далее в Теореме~\ref{thm:1Dconv}. Здесь для простоты мы сохраняем обозначение $\mathbf{T}^{(p)}$, но используем его для одномерных сверток, и поясним, как они представляются в виде линейных операторов.
Известно, что сверточные сети часто могут быть представлены в виде линейных сверточных нейронных сетей (LCN).
Обычно это относится к представлению сверточных сетей с помощью матриц Теплица~\cite{kohn2022geometrylinearconvolutionalnetworks, qin2023toeplitzneuralnetworksequence}.
В данной работе мы используем обозначения для матриц Теплица из работы~\cite{singh2023hessianperspectivenatureconvolutional}.
Также в указанной работе авторы определили специальный тип матрицы~$\mathbf{Q}^{(p)}$, соответствующий структуре одномерной матрицы Теплица.
Наша одномерная сверточная сеть имеет вид~$f_{\boldsymbol{\theta}}{\mathbf{x}} = \mathbf{T}^{(L+1)} * (\sigma(\dots (\sigma(\mathbf{T}^{(1)} * \mathbf{x}))\dots)$, где операция $*$ означает свертку.

Пусть~$C_p$ обозначает количество каналов после~$p$-го слоя, а~$d_p$ - размер последовательности. Здесь~$\mathbf{x}^{(p)} \in \mathbb{R}^{C_p \times d_p}$, $\mathbf{T}^{(p)}$~--- одномерный сверточный слой с ядром~$\mathbf{W}^{(p)} \in \mathbb{R}^{C_{p-1} \times C_{p} \times k_{p}}$. 
Для упрощения дальнейших обозначений заменим $\mathbf{x}^{(p)}$ на~$vec(\mathbf{x}^{(p)}) \in \mathbb{R}^{(C_pd_p)}$. 
Получили:
\[
    \mathbf{z}^{(p+1)} = \mathbf{T}^{(p+1)}\mathbf{x}^{(p)}.
\]

Теорема~\ref{thm:1Dconv} доказывает верхнюю оценку нормы гессиана для глубокой одномерной сверточной сети.
Особенностью полученной оценки является мультипликативная зависимость от глубины сети~$L$ и полиномиально-экспоненциальная зависимость от параметров модели~--- числа каналов $C$, размера ядра $k$ и длины последовательности $d$.

\begin{theorem}\label{thm:1Dconv}
    Пусть задана сеть вида:
    \[
        f_{\boldsymbol{\theta}}x = C_{\mathbf{W}^{(L+1)}} \circ \sigma \circ \dots \circ \sigma \circ C_{\mathbf{W}^{(1)}},
    \]
    где~$C_{\mathbf{W}^{(i)}}$~--- одномерная свертка с ядром~$\mathbf{W}^{(i)}$, без дополнения (англ. padding) и с единичным шагом (англ. stride). Пусть заданы следующие верхние оценки на параметры:
    \begin{align}
        C_l &\leqslant C,\\
        k_i &\leqslant k,\\
        d_i &\leqslant d_1:=d,\\
        |\mathbf{W}^{(p)}_{i,j,k}|^2 &\leqslant w^2.
    \end{align}
    Тогда норма матрицы Гессе имеет следующую верхнюю оценку:
    \[
        \left\|\mathbf{H}_{O}\right\| \leqslant \sqrt{2}\left\|x\right\|^2d^2(L+1)(C^2w^2kd)^L.
    \]
\end{theorem}
\begin{proof}
Из Теоремы~\ref{theorem:m-net:hessnorm} следует, что требуется доказать следующие неравенства:
\begin{align}
    \left\|\mathbf{T}^{(p)}\right\|^2 &\leqslant C^2dkw^2, \\
    \left\|\mathbf{Q}^{(p)}\right\|^2 &\leqslant d^2.
\end{align}

Согласно работе~\cite{singh2023hessianperspectivenatureconvolutional}, матрица~$\mathbf{T}^{(p)}$ состоит из блоков размером $C_l \times C_{l-1}$, каждый из которых содержит $d_{l-1}$ строк с расположением элементов ядра в соответствующих позициях.
Учитывая ограничения на число каналов $C_l \leqslant C$, длину последовательности $d_{l-1} \leqslant d$, размер ядра $k_l \leqslant k$ и ограниченность элементов ядра $|\mathbf{W}^{(p)}_{i,j,k}|^2 \leqslant w^2$, получаем оценку:
\[
    \left\|\mathbf{T}^{(p)}\right\|^2 \leqslant C^2dkw^2.
\]

Согласно работе~\cite{singh2023hessianperspectivenatureconvolutional} рассмотрим матрицы:
\[
    \frac{\partial \mathbf{T}^{(l)}}{\partial \mathbf{Q}^{(l)}} =: \mathbf{Q}^{(l)} = \mathbf{I}_{C_l} \otimes
    \begin{pmatrix}
        \mathbf{I}_{C_{l-1}} \otimes (\pi_R^0\mathbf{I}_{d_{l-1} \times k_l}) \\
        \vdots \\
        \mathbf{I}_{C_{l-1}} \otimes (\pi_R^{d_{l-1} - k_l}\mathbf{I}_{d_{l-1} \times k_l})
    \end{pmatrix}.
\]
Оценим норму этой вертикально сконкатенированной матрицы:
\begin{align}
    \left\|\mathbf{Q}^{(l)}\right\| &\leqslant \sum\limits_{i=0}^{d_{l-1} - k_l}\left\|\pi_R^i\mathbf{I}_{d_{l-1} \times k_l}\right\| \leqslant \sum\limits_{i=0}^{d_{l-1} - k_l}\left\|\pi_R\right\| = \\
    &= \sum\limits_{i=0}^{d_{l-1}-k_l} 1 = d_{l-1} - k_l + 1 = d_l \leqslant d_1 = d,
\end{align}
следовательно получаем, что~$\left\|\mathbf{Q}^{(l)}\right\|^2 \leqslant d^2$.

Собирая все полученные оценки воедино:
\[
    \left\|\mathbf{H}_{O}\right\| \leqslant \|\mathbf{Q}\|^2 \|\mathbf{F}\|^2 \|\mathbf{A}\|\leqslant \sqrt{2}\left\|x\right\|^2d^2(L+1)(C^2w^2kd)^L.
\]
\end{proof}

Далее рассматриваются двумерные сверточные сети.
Аналогично, для простоты мы сохраняем обозначение~$\mathbf{T}^{(p)}$ для слоев сверточной сети. Пусть задан~$\mathbf{x} \in \mathbb{R}^{m \times n \times C}$~--- входное изображение, имеющее размеры~$(m, n)$ и~$C$ каналов.
Обозначим~$\mathbf{x}^{(l)} \in \mathbb{R}^{m_i \times n_i \times C_i}$~--- вход~$(l+1)$-го слоя, а матрицей~$\mathbf{W}^{(l)} \in \mathbb{R}^{C_{l-1} \times C_l \times k^1_l \times k^2_l}$~--- свертку с размерами~$(k^1_l, k^2_l)$, входным и выходным количеством каналов~$C_{l-1}$ и~$C_l$ соответственно.

Аналогично тому как было сделано для 1D-свертки, используем~$vec(\mathbf{x}) \in \mathbb{R}^{m_in_iC_i}$ вместо $\mathbf{x} \in \mathbb{R}^{m_i \times n_i \times C_i}$.
Исследуется операция свертки над входным тензором, в частности, в случае векторизованного входа, используем ту же структуру Теплица, что и в \cite{toep_2dconv}, но в этом случае будет удобнее использовать конкретную матрицу~$\mathbf{T}^{(p)}$, строка которой состоит из элементов~$\mathbf{W}^{(p)}_{*, c_2, *, *}$~для $c_2$-го канала.
То есть каждая строка матрицы~$\mathbf{T}^{(p)}$ реализует ``применение'' ядра к определенной позиции и определенному каналу.
Обозначим матрицей~$\mathbf{T}^{(p)}_i$ матрицу, которая соответствует~$c_2 = c_2(i)$-му каналу~$\mathbf{W}$.

Далее Теорема~\ref{thm:2Dconv} устанавливает оценку нормы гессиана для глубоких двумерных сверточных сетей.
Особенностью полученной оценки является экспоненциальная зависимость от глубины сети~$L$ и полиномиальная зависимость от основных параметров архитектуры: числа каналов~$C$, размера ядра~$k$ и пространственных размеров $m \times n$.

\begin{theorem}\label{thm:2Dconv}
    Пусть задана сеть вида
    \[
        f_{\boldsymbol{\theta}}\mathbf{x} = C_{\mathbf{W}^{(L+1)}} \circ \dots \circ C_{\mathbf{W}^{(1)}},
    \]
    где $C_{\mathbf{W}^{(l)}}$~--- двумерная свертка с ядром $\mathbf{W}^{(i)}$, без дополнения (англ. padding) и с единичным шагом (англ. stride). Пусть заданы следующие верхние оценки на параметры:
    \begin{align}
        C_l &\leqslant C,\\
        k_i &\leqslant k,\\
        m_i &\leqslant m_1:=m, \\
        n_i &\leqslant n_1:=n,\\
        |\mathbf{W}^{(p)}_{i,j,k}|^2 &\leqslant w^2.
    \end{align}
    Тогда норма матрицы Гессе имеет следующую верхнюю оценку:
    \[
        \left\|\mathbf{H}_{O}\right\| \leqslant \sqrt{2}\left\|\mathbf{x}\right\|^2q^2(L+1)(C^2k^2w^2mn)^{L},
    \] 
    где~$q^2 = C^2k^2mn$.
\end{theorem}
\begin{proof}
Для двумерных сверток в матрице~$\mathbf{T}^{(p)}$ выполняется равенство:
\[
    \left\|\mathbf{T}^{(p)}_{i, *}\right\|^2 = \sum\limits_{c, k, l}^{C_{p-1}, k_p^1, k_p^2}|\mathbf{W}^{(p)}_{c, c_2, k, l}|^2,
\]
а следовательно:
\begin{equation}\label{eq:estim_toep_2d}
    \left\|\mathbf{T}^{(p)}\right\|_F^2 = \sum\limits_{c_1, i, k, l}^{C_{p-1}, C_pn_pm_p, k_p^1, k_p^2}\big(\mathbf{W}^{(p)}_{c_1, c_2(i), k, l}\big)^2,
\end{equation}
где предполагается соответствие между выходным каналом~$c_2$ и строкой~$\mathbf{T}^{(p)}$ $i$.

По аналогии с доказательством~\ref{thm:1Dconv}, используя~\ref{theorem:m-net:hessnorm} необходимо доказать два неравенства:
\begin{align}
    \left\|\mathbf{T}^{(p)}\right\| &\leqslant C^2k^2w^2mn,\\
    \left\|\mathbf{Q}^{(p)}\right\| &\leqslant C^2k^2mn.
\end{align}

Сначала оценим норму $\mathbf{T}^{(p)}$, используя~\eqref{eq:estim_toep_2d} получем:
\[
    \left\|\mathbf{T}^{(p)}\right\|^2 \leqslant \left\|\mathbf{T}^{(p)}\right\|_F^2 \leqslant \sum\limits_{i} Ck^2w^2 \leqslant C^2k^2w^2mn.
\]

Далее оценим норму производной слоя по параметрам:
\[
    \left\|\mathbf{Q}^{(p)}\right\| = \left\|\frac{\partial \mathbf{T}^{(p)}}{\partial \mathbf{W}^{(p)}}\right\|.
\]
Как упоминалось ранее, строка $\mathbf{T}^{(p)}$~--- является $vec_r(\mathbf{W}^{(p)}_{*, i, *, *})$, расположенная в правильном порядке.
Тогда норма строки~$\frac{\partial \mathbf{T}^{(p)}_{(i, j)}}{\partial \mathbf{W}^{(p)}_{c_1, c_2, k_1, k_2}} \neq 0 \iff$ индексы выбраны таким образом, что $T^{(p)}_i$ соответствует $c_2$, и в то же время $\mathbf{T}^{(p)}_{i, j}$ соответствует $c_1, k_1, k_2$.
Это соответствие зависит от конкретной матрицы~$\mathbf{T}^{(p)}$, но очевидно, что один~$i$ соответствует только одному~$c_2$, поскольку каждая строка участвует в формировании только одного элемента одного канала.
Поскольку только~$\mathbf{W}^{(p)}_{*, c_2, *, *}$ участвует в формировании одной строки~$\mathbf{T}^{(p)}_{i, *}$, мы можем зафиксировать~$i$ и соответствующий~$c_2$, а также одновременно мы знаем, что для каждых~$c_1, k_1, k_2$ существует только один столбец~$j$ такой, что~$\mathbf{T}^{(p)}_{i, j} = \mathbf{W}^{(p)}_{c_1, c_2, k_1, k_2}$, а следовательно получаем:
\begin{align}
    \sum\limits_{j,c_1, k_1, k_2}\bigg(\frac{\partial \mathbf{T}^{(p)}_{i, j}}{\partial\mathbf{W}^{(p)}_{c_1, c_2, k_1, k_2}}\bigg)^2 &= \sum\limits_{c_1, k_1, k_2}\sum\limits_{j}\bigg(\frac{\partial \mathbf{T}^{(p)}_{i, j}}{\partial\mathbf{W}^{(p)}_{c_1, c_2, k_1, k_2}}\bigg)^2 = \\
    & = \sum\limits_{c_1, k_1, k_2}1 = C_{p-1}k_p^1k_p^2 \leqslant Ck^2.
\end{align}
Далее используем свойство нормы Фробениуса как верхней оценки спектральной нормы:
\begin{align}
    \left\|\mathbf{Q}\right\|^2 &\leqslant \left\|\mathbf{Q}\right\|^2_F = \sum\limits_{i, j, c_1, c_2, k_1, k_2}\bigg(\frac{\partial \mathbf{T}^{(p)}_{i,j}}{\partial \mathbf{W}^{(p)}_{c_1, c_2, k_1, k_2}}\bigg)^2 = \\
    & = \sum\limits_{i, c_2}\sum\limits_{j, c_1, k_1, k_2}\bigg(\frac{\partial \mathbf{T}^{(p)}_{i,j}}{\partial \mathbf{W}^{(p)}_{c_1, c_2, k_1, k_2}}\bigg)^2 \leqslant \\
    & \leqslant \sum\limits_{i}Ck^2 = CmnCk^2 \leqslant C^2k^2mn.
\end{align}
\end{proof}

\begin{remark}
    Полученные оценки в Теоремах~\ref{thm:1Dconv} и~\ref{thm:2Dconv} имеют недостаток связанный с тем, что они не учитывают уменьшение размеров после сверточных операций и зависят только от верхних границ параметров.
\end{remark}

Рассмотренные выше теоремы~\ref{thm:1Dconv} и~\ref{thm:2Dconv} оперируют с сетями, которые состоят исключительно из сверточных слоев, что редко встречается на практике.
Далее в Теоремах~\ref{theorem:maxpool},~\ref{theorem:avgpool},~\ref{theorem:fc-head} рассматриваются случаи добавления других распространенных слоев в сверточных сетях.

Также требует оценки норма слоя~\ref{theorem:maxpool}.
Теорема~\ref{theorem:maxpool} описывает связь между параметрами сети с операцией макс-пулинга и сложностью оптимизационного ландшафта через норму матрицы Гессе.
Полученная оценка показывает, что введение слоя макс-пулинга существенно модифицирует зависимость сложности модели от ее параметров.

\begin{theorem}\label{theorem:maxpool}
    Пусть задана сеть вида: 
    \[
        f_{\boldsymbol{\theta}}{\mathbf{x}} = \mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)}\dots \boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)}\mathbf{x},
    \]
    содержащий слой MaxPool2D в позиции~$\boldsymbol{\Lambda}^{(l)}$ с ядром~$k_{\mathrm{pool}} \times k_{\mathrm{pool}}$ вместо активации ReLU.
    Тогда имеет следующую верхнюю оценку нормы матрицы Гессе:
    \[
        \left\|\mathbf{H}_O\right\| \leqslant \sqrt{2}\left\|\mathbf{x}\right\|^2q^2\big(\frac{1}{k_{\mathrm{pool}}^2}\big)^{L-l+2}(L+1)(k^2C^2w^2mn)^{L},
    \]
    где~$q^2 = mnC^2k^2$.
\end{theorem}
\begin{proof}
Введем обозначение $\mathbf{M}^{(l)}$ для слоя 2D-Max-Pool. 
Аналогично сверточным слоям, мы можем описать каждую строку $\mathbf{M}^{(l)}.$
Отметим некоторые свойства $\mathbf{M}$, которые будут использоваться: во-первых, строка~$\mathbf{M}_{i*}$ соответствует определенному окну пулинга и, во-вторых, столбец~$\mathbf{M}_{*j}$ соответствует элементам, которые умножаются на $j$-й элемент входа.

Поскольку каждое окно покрывает только один элемент и два различных окна не пересекаются, то в каждой строке имеется только один ненулевой элемент, а следовательно
\[
    \left\|\mathbf{M}^{(l)}\right\| = \sqrt{\lambda_{\max}({\mathbf{M}^{(l)\top} \mathbf{M}^{(l)}})} = 1,
\] 
так как~$(\mathbf{M}^{(l)}_{*, i}, \mathbf{M}^{(l)}_{*, j}) \neq 0 \iff (\mathbf{M}^{(l)}_{*, i}, \mathbf{M}^{(l)}_{*, j}) = 1 \iff i = j,$ а~$i$-й элемент является максимальным в соответствующем окне.

Для простоты предположим, что $\mathbf{M}^{(l)}$ уменьшает оба размера в ${k_{\mathrm{pool}}}$ раз. Аналогично доказательству теоремы~\ref{thm:2Dconv} оцениваем компоненты $\mathbf{G}^{(p)}$ и $\mathbf{R}^{(p-1)}$, однако с учетом нового слоя:
\begin{align}
    \left\|\mathbf{G}^{(p)}\right\|\left\|\mathbf{R}^{(p-1)}\right\| &\leqslant \frac{\prod\limits_{i=1}^{L+1}\left\|T^{(i)}\right\|}{\left\|T^{(p)}\right\|} \leqslant \\
    &\leqslant (C^2k^2w^2mn)^{2L}\bigg(\frac{1}{k_{\mathrm{pool}}^2}\bigg)^{L - l + 2 - I\{p-1 \leqslant l\}} \leqslant \\
    &\leqslant (C^2k^2w^2mn)^{2L}\bigg(\frac{1}{k_{\mathrm{pool}}^2}\bigg)^{L - l + 2},
\end{align}
а следовательно используя данные оценки получаем
\begin{align}
    \left\|\mathbf{F}\right\|^2 &\leqslant \left\|\mathbf{x}\right\|^2(L+1)(k^2C^2w^2mn)^{(L)}\big(\frac{1}{k_{\mathrm{pool}}^2}\big)^{L - l + 2}, \\
    \left\|\mathbf{H}_O\right\| &\leqslant \sqrt{2}\left\|\mathbf{x}^2\right\|q^2\big(\frac{1}{k_{\mathrm{pool}}^2}\big)^{L-l+2}(L+1)(k^2C^2w^2mn)^{L},
\end{align}
где $q^2 = mnC^2k^2.$
\end{proof}

\begin{remark}
    Как и в предыдущих случаях, норма гессиана растет экспоненциально с глубиной сети~$L$, что согласуется с предыдущими оценками, но появление множителя $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$ указывает на снижающий эффект операции пулинга на сложность оптимизации.
    Этот фактор отражает уменьшение размерности признакового описания после операции пулинга, что приводит к сокращению эффективной размерности параметрического пространства.
    Видно, что степень $L-l+2$ показывает, что влияние пулинга распространяется на все последующие слои.
    Чем раньше расположен слой пулинга (меньше $l$), тем сильнее его редуцирующее воздействие на норму гессиана.
    Следовательно получаем, что операция пулинга частично компенсирует экспоненциальный рост сложности с увеличением глубины сети, однако этот эффект зависит от размера ядра пулинга~$k_{\mathrm{pool}}$.
\end{remark}

Следующая теорема~\ref{theorem:avgpool} устанавливает оценку нормы матрицы Гессе для сверточной сети, содержащей слой усредняющего пулинга.

\begin{theorem}\label{theorem:avgpool}
    Пусть задана сеть вида:
    \[
        f_{\boldsymbol{\theta}}{\mathbf{x}} = \mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)}\dots \boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)}\mathbf{x},
    \]
    содержащая слой AvgPool2D в позиции~$\boldsymbol{\Lambda}^{(l)}$ вместо активации ReLU с ядром размера~$k_{\mathrm{pool}} \times k_{\mathrm{pool}}$.
    Тогда оценка нормы матрицы Гессе имеет следующую верхнюю оценку:
    \[
        \left\|\mathbf{H}_O\right\| \leqslant \sqrt{2}\left\|\mathbf{x}\right\|^2q^2\big(\frac{1}{k_{\mathrm{pool}}^2}\big)^{L-l+2}(L+1)(k^2C^2w^2mn)^{L},
    \]
где $q^2 = mnC^2k^2$.
\end{theorem}
\begin{proof}
    Введем обозначение~$\mathbf{A}^{(l)}$ для слоя 2D-Avg-Pool. 
    Заметим, что
    \[
        (\mathbf{A}_{*, i}, \mathbf{A}_{*,j}) = \frac{1}{k_{\mathrm{pool}}^4}I\{\text{i, j соответствуют одному и тому же окну}\}.
    \] 
    Для обоснования этого рассмотрим формулу:
    \[
        (\mathbf{A}_{*j}, \mathbf{A}_{*i}) = \sum\limits_{k}\mathbf{A}_{ki}\mathbf{A}_{kj} = \sum\limits_{k:\mathbf{A}_{ki} \neq 0, \mathbf{A}_{kj} \neq 0}\frac{1}{k_{\mathrm{pool}}^4}.
    \]
    После этого, применяя элементарные преобразования над строками и столбцами, мы приводим матрицу $\mathbf{A}^{(p)\top}\mathbf{A}^{(p)}$ к блочно-диагональной форме, где блоки соответствуют индексам в одном окне усредняющего пулинга.
    Каждый блок матрицы $\mathbf{A}^{(p)\top}\mathbf{A}^{(p)}$ имеет вид $\mathbf{B}_i = \frac{1}{k_{\mathrm{pool}}^2}\mathbf{11}^\top$, где $\mathbf{1} = \mathbf{1}_{k_{\mathrm{pool}}^2} \in \mathbb{R}^{k_{\mathbf{A}}^2}$~--- вектор из единиц, и его норма $\left\|\mathbf{B}_i\right\| = \frac{1}{k_{\mathrm{pool}}^2}\left\|\mathbf{11}^\top\right\| = \frac{1}{k_{\mathrm{pool}}}$.
    Норма блочно-диагональной матрицы равна максимуму норм блоков:
    \[
        \left\|\mathbf{A}^{(p)}\right\| = \max\limits_{i}\left\|\mathbf{B}_{i}\right\| = \frac{1}{k_{\mathrm{pool}}}.
    \]
    Учитывая, что $\left\|\mathbf{A}^{(p)}\right\| \leqslant 1$, мы можем полностью повторить вычисления доказательства Теоремы~\ref{theorem:maxpool} и получить тот же результат.
\end{proof}

\begin{remark}
    Полученная оценка в рамках Теоремы~\ref{theorem:avgpool} демонстрирует, что операция усредняющего пулинга оказывает аналогичное макс-пулингу влияние на сложность оптимизационного ландшафта, уменьшая норму гессиана за счет множителя $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$.
    Это подтверждает гипотезу о том, что операции пулинга любого типа способствуют снижению сложности модели и могут рассматриваться как эффективный механизм регуляризации в глубоких сверточных нейронных сетях.
\end{remark}

Теорема~\ref{theorem:fc-head} устанавливает оценку нормы матрицы Гессе для гибридной архитектуры, сочетающей сверточные слои и полносвязный слой.
Полученная оценка указывает на мультипликативный вклад обеих частей сети, причем сверточная часть вносит множитель~$(k^2C^2w^2mn)^L$, а полносвязная~--- $(h^2\tilde{w}^2)^P$.

\begin{theorem}\label{theorem:fc-head}
Пусть задана сеть с~$P$ полносвязными слоями следующего вида:
\begin{align}
    f_{\boldsymbol{\theta}}{\mathbf{x}} &= \mathbf{T}^{(L+P+1)}\boldsymbol{\Lambda}^{(L+P)} \dots \\
    &\quad\dots \boldsymbol{\Lambda}^{(L+1)}\mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)}\dots \boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)}\mathbf{x},
\end{align}
где~$\mathbf{T}^{(L+1+i)}$~--- полносвязные слои с~$h_{i}$ параметрами при $i=1,\dots,P$, а~$\mathbf{T}^{(r)}$~--- двумерные сверточные слои.
Пусть~$\left\|\mathbf{T}^{(L+1+i)}_{ij}\right\| \leqslant \tilde{w}$ и $h_p \leqslant h$.
Тогда в условиях и обозначениях Теоремы~\ref{thm:2Dconv} справедливо неравенство:
\begin{align}
    \left\|\mathbf{H}_{O}\right\| &\leqslant \sqrt{2}\left\|\mathbf{x}\right\|^2q^2(h^2\tilde{w}^2)^P(k^2C^2w^2mn)^L\times \\
    &\quad\times \big(L + 1 + P\frac{h^2\tilde{w}^2}{k^2C^2w^2mn}\big).
\end{align}
\end{theorem}
\begin{proof}
Как и в предыдущих доказательствах  необходимо оценить
\[
    \left\|\mathbf{G}^{(p)}\right\|^2\left\|\mathbf{R}^{(p-1)}\right\|^2,
\]
причем, известно, что
\[
    \left\|T^{(L+1+p)}\right\|^2 \leqslant (h^2\tilde{w}^2) \quad \forall p = 1, \dots, P,
\]
а следовательно получаем:
\[
    \left\|\mathbf{G}^{(p)}\right\|^2\left\|\mathbf{R}^{(p-1)}\right\|^2 \leqslant (h^2\tilde{w}^2)^{P}(k^2C^2w^2mn)^L,
\] 
для $p \leqslant L+1$ и 
\[
    \left\|\mathbf{G}^{(p)}\right\|^2\left\|\mathbf{R}^{(p-1)}\right\|^2 \leqslant (h^2\tilde{w}^2)^{P-1}(k^2C^2w^2mn)^{L+1},
\] 
для $p = L+2 \dots L+P+1$.
В общей форме:
\[
    \left\|\mathbf{G}^{(p)}\right\|^2\left\|\mathbf{R}^{(p-1)}\right\|^2 \leqslant (h^2\tilde{w}^2)^{P-I_{\{p > L+1\}}}(k^2C^2w^2mn)^{L + I_{\{p > L+1\}}},
\]
а следовательно получаем:
\begin{align}
    \left\|F\right\|^2 &\leqslant \sum\limits_{p=1}^{L+P+1}\left\|\mathbf{G}^{(p)}\right\|^2\left\|\mathbf{R}^{(p-1)}\right\|^2\left\|x\right\|^2 \leqslant \\
    &\leqslant (h^2\tilde{w}^2)^P(k^2C^2w^2mn)^{L}\left(L+1 + P\frac{h^2\tilde{w}^2}{k^2C^2w^2mn}\right).
\end{align}
Применяя этот результат к матрице Гессе:
\begin{align}
    \left\|\mathbf{H}_{O}\right\| &\leqslant \sqrt{2}\left\|\mathbf{x}\right\|^2q^2(h^2\tilde{w}^2)^P(k^2C^2w^2mn)^L\times \\
    &\quad \times \left(L + 1 + P\frac{h^2\tilde{w}^2}{k^2C^2w^2mn}\right).
\end{align}
\end{proof}

\section{Матрица Гессе для трансформерной модели глубокого обучения}
Пусть $f_{\mathbf{w}}(\cdot)$ обозначает нейронную сеть, в данном случае слой самовнимания (англ. self-attention) или полный блок трансформера (англ. Transformer), с параметрами $\mathbf{w} \in \Omega$.
При наличии дважды дифференцируемых потерь $l(\cdot, \cdot)$ потери на выборку равны $l_i(\mathbf{w}) := l(f_{\mathbf{w}}(\mathbf{x}_i), \mathbf{y}_i)$.
Эмпирические потери для выборок $L = k$ равны $\mathcal{L}_k(\mathbf{w}) = \frac{1}{k} \sum_{i=1}^k l_i(\mathbf{w})$, с гессианом $\mathbf{H}^{(k)}(\mathbf{w}) = \frac{1}{k} \sum_{i=1}^k \nabla^2_{\mathbf{w}} l_i(\mathbf{w})$.


Пусть заданы входные вектора эмбедингов (англ. embeddings) $\mathbf{X} \in \mathbb{R}^{L \times d_V}$.
Выход слоя одной головы (англ. single-head) слоя самовнимания задается в виде: 
\begin{equation} \label{eq:self_attention}
    \mathbf{F}(\mathbf{X}) = \mathbf{A}(\mathbf{X}) \mathbf{X} \mathbf{W}_V,
\end{equation}
где $\mathbf{A}(\mathbf{X}) = \mathrm{softmax}\left( \frac{\mathbf{X} \mathbf{W}_Q \mathbf{W}_K^\top \mathbf{X}^\top}{\sqrt{d_K}} \right)$, а $\mathbf{W}_Q, \mathbf{W}_K \in \mathbb{R}^{d_V \times d_K}$, $\mathbf{W}_V \in \mathbb{R}^{d_V \times d_V}$.

Используя \eqref{eq:self_attention}, полный блок трансформера выглядит в следующим образом:
\begin{align}
    \text{LayerNorm}\Big(\mathbf{\text{LayerNorm}(\mathbf{X} + \mathbf{F}(\mathbf{X}))} + \mathrm{FFN}(\mathbf{\text{LayerNorm}(\mathbf{X} + \mathbf{F}(\mathbf{X}))})\Big)
\end{align}
где $\mathrm{FFN}(\cdot)$ является блоком полносвязной сети с некоторой нелинейностью.
Слой LayerNorm для входной матрицы $\mathbf{U} \in \mathbb{R}^{m \times n}$ описывается выражением:
\[
    \text{LayerNorm}(\mathbf{U})_{i,j} = \gamma_j \frac{\mathbf{U}_{i,j} - \mu_i}{\sqrt{\sigma_i^2}} + \mathbf{\beta}_j,
\]
где $\mu_i = \frac{1}{m} \sum_{j=1}^m \mathbf{U}_{i,j}, \quad \sigma_i^2 = \frac{1}{m} \sum_{j=1}^m (\mathbf{U}_{i,j} - \mu_i)^2$.

\begin{assumption}\label{assumption:LayerNorm}
Для входной матрицы слоя LayerNorm: $\mathbf{X} + \mathbf{F}(\mathbf{X})$, $\mathbf{Y} + \mathrm{FFN}(\mathbf{Y})$, построчная дисперсия удовлетворяет условию $\min_i \sigma_i^2 > 0$.
\end{assumption}

Предположение~\ref{assumption:LayerNorm} является техническим и требуется для доказательства ряда Теорем. Выполнения данного свойства можно добиться, добавив к знаменателю положительную константу, но это усложнит вычисления.

Для оценки матрицы Гессе рассматривается среднеквадратичная функция ошибки:
\[
    l(\cdot, \textbf{Target}) = \frac{1}{L d_V} \|\cdot - \textbf{Target}\|_F^2.
\]
В дальнейшем для доказательств будет использоваться разложения Гаусса-Ньютона матрицы Гессе $\mathcal{L}_k \circ f_{\mathbf{w}}$:
\begin{equation}\label{eq:gauss_decomposition}
    \frac{\partial^2 (\mathcal{L}_k \circ f_{\mathbf{w}})}{\partial \mathbf{W}_i \partial \mathbf{W}_j} = \frac{\partial f_{\mathbf{w}}}{\partial \mathbf{W}_i} (\cdot)^\top \frac{\partial^2 \mathcal{L}_k}{\partial f_{\mathbf{w}}^2} (f_{\mathbf{w}}(\cdot)) \frac{\partial f_{\mathbf{w}}}{\partial \mathbf{W}_j}(\cdot) + \left( \frac{\partial \mathcal{L}_k}{\partial f_{\mathbf{w}}} (f_{\mathbf{w}}(\cdot)) \otimes \mathbf{I}_{p_i q_i} \right) \frac{\partial^2 f_{\mathbf{w}}}{\partial \mathbf{W}_i \partial \mathbf{W}_j}(\cdot)
\end{equation}

Для начала вычислим обобщенные выражения матрицы Гессе для слоя самовнимания и расширяем их до полного блока модели трансформер. Подход основан на теоретической базе \cite{ormaniec2024attentionhessian}, адаптируя и обобщая ее результаты.

Матрица Гессе функции ошибки $\mathcal{L}_k$ относительно параметров модели $\mathbf{w}$:
\[
    \mathbf{H}^{(k)}(\mathbf{w}) = \nabla^2_{\mathbf{w}} \mathcal{L}_k(\mathbf{w}) = \frac{1}{k} \sum_{i=1}^k \nabla^2_{\mathbf{w}} l_i(\mathbf{w}) = \frac{1}{k} \sum_{i=1}^k \mathbf{H}_i(\mathbf{w})
\]
где $\mathbf{H}_k(\mathbf{w})$ является матрицей Гессе блока самовнимания для параметров $\mathbf{w}$ относящиеся к матрицам $\{ \mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V\}$. Используя разложения Гаусса-Ньютона \eqref{eq:gauss_decomposition}:
\[
    \mathbf{H}_k(\mathbf{W}_i, \mathbf{W_j}) = \frac{\partial^2 l}{\partial \mathbf{W}_i \partial \mathbf{W}_j} = \mathbf{H}_o(\mathbf{W}_i, \mathbf{W}_j) + \mathbf{H}_f(\mathbf{W}_i, \mathbf{W}_j),
\]
где $\mathbf{H}_o$ является outer-product частью матрицы Гессе, а $\mathbf{H}_f$ является матрицей Гессе функции самовнимания.
Результаты этого разложения можно вычислить согласно Теоремам 3.1-3.2 в работе~\cite{ormaniec2024attentionhessian}.

\subsection{Матрица Гессе для слоя самовнимания}
Проведем оценку нормы матрицы Гессе для одного слоя самовнимания. Результат данной оценки показан в Теореме~\ref{thm:self_attention_hessian_estimation}.

\begin{theorem}\label{thm:self_attention_hessian_estimation}

Пусть $\|\cdot\|_2$ является спектральной нормой, тогда для слоя самовнимания получаем:
\[
    \|\mathbf{H}_i(\mathbf{w}^*)\|_2 \leq M,
\]
где
\begin{align}
M &= 3\cdot\max \Bigg(\frac{2L}{d_V} \| \mathbf{X}\|^2_2, \\
    &\frac{8}{L^3 d_V d_K} \| \mathbf{W}_K\|_2^2 \| \mathbf{W}_V\|^2_2 \| \mathbf{X}\|^6_2 + \\
    &\quad+\frac{12}{d_V d_K} \sqrt{\min(L, d_V)} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2) \| \mathbf{W}_V \|_2 \| \mathbf{W}_K\|^2_2 \| \mathbf{X}\|^5_2, \\
    &\frac{4}{L d_V \sqrt{d_K}} \| \mathbf{W}_V\|_2 \| \mathbf{W}_K \|_2 \| \mathbf{X}\|^4_2 +\\
    &\quad+\frac{4\sqrt{\min(L, d_V)}}{L^2\sqrt{d_K}} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2) \|\mathbf{W}_K\|_2 \|\mathbf{X}\|^3_2,\\
    &\frac{8}{L^3 d_V d_K} \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{W}_V\|^2_2 \|\mathbf{X} \|^6_2 + \\
    &\quad+\frac{4\sqrt{\min(L, d_V)} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2)}{L d_V \sqrt{d_K}} \|\mathbf{W}_V\|_2 \cdot\\
    &\quad\quad\cdot\Big(3L \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{X}\|^5_2 + \frac{d_V}{L} \|\mathbf{X}\|^3_2\Big)\Bigg).
\end{align}
\end{theorem}
\begin{proof}

Используя результаты Леммы A.3 из работы \cite{noci2022signalpropagationtransformerstheoretical}, а также свойство~\ref{prop:matrix_product_norm} и свойство~\ref{prop:kronecker_product_norm} получаем:
\begin{align}
    \| \frac{\partial\mathbf{A}}{\partial\mathbf{T}}\|_2 = \frac{1}{L} \| \mathbf{I}_L\|_2 \| \mathbf{I}_L - \frac{1}{L}\mathbf{1}_{L \times L}\|_2 \leq \frac{1}{L}
\end{align}
Данное неравенство верно в силу того, что $\frac{1}{L}\mathbf{1}_{L \times L}$ является матрицей проекции, поэтому $\mathbf{I}_L - \frac{1}{L}\mathbf{1}_{L \times L}$ также матрица проекции и следовательно норма $\| \mathbf{I}_L - \frac{1}{L}\mathbf{1}_{L \times L}\|_2 \leq 1$.

Далее для аппроксимации нормы матрицы $\mathbf{Z}_1$ используем те же свойства~\ref{prop:matrix_product_norm} и \ref{prop:kronecker_product_norm}:
\begin{align}\label{chapter-2:theorem:proof_Znorm}
    \|\mathbf{Z}_1\|_2 &\leq \| \mathbf{I}_L \otimes \mathbf{X}^{\top}\|_2 \left\| \frac{\partial \mathbf{A}}{\partial \mathbf{T}}\right\|_2 \| \mathbf{X} \otimes \mathbf{X}\|_2 \leq\\
    & \leq \| \mathbf{X}\|_2 \frac{1}{L} \|\mathbf{X}\|^2_2 = \frac{1}{L} \|\mathbf{X}\|^3_2
\end{align}
где дополнительно было использовано свойство \ref{prop:transposed_matrix_norm} for $\| \mathbf{X}\|_2 = \| \mathbf{X}^\top\|_2$.

Оценим норму матрицы $\| \mathbf{A}\|_2,$ которая является матрицей, где каждая строка является результатом применения функции~$\text{softmax},$ а следовательно, каждый элемент матрицы~$\mathbf{A}_{i,j} \leq 1$.
Далее используя свойства \ref{prop:matrix_norm_inequalities} получаем $ \|\mathbf{A}\|_{\max} \leq \|\mathbf{A}\|_2 \leq \sqrt{LL} \|\mathbf{A}\|_{\max} = L\| \mathbf{A}\|_{max} \leq L$.
Также получаем, что:
\[
    \|\mathbf{M}_1\|_2 = \|\mathbf{A}\mathbf{X}\|_2 \leq L \|\mathbf{X}\|_2.
\]

Итого, легко получаем outer-product матрицы Гессе $\|\mathbf{H}_o (\mathbf{W}_i, \mathbf{W}_j) \|_2$  для разных матриц.
В случае матрицы~$\mathbf{W}_V$ и матрицы~$\mathbf{W}_V$:
\begin{align}
    \| \mathbf{H}_o(\mathbf{W}_V, \mathbf{W}_V)\|_2 &\leq \frac{2}{L d_V} \| \mathbf{M}_1\|^2_2 1 \leq \frac{2}{L d_V}\| \mathbf{A}\|^2_2 \| \mathbf{X}\|^2_2 \leq\\
    & \leq \frac{2}{L d_V} L^2 \| \mathbf{X}\|^2_2 = \frac{2L}{d_V}\| \mathbf{X}\|^2_2.
\end{align}
Для матриц~$\mathbf{W}_Q$ и~$\mathbf{W}_Q$ получаем:
\begin{align}
    \| \mathbf{H}_o(\mathbf{W}_Q, \mathbf{W}_Q)\|_2 &\leq \| \frac{2}{L d_V d_K} (\mathbf{I}_{d_V} \otimes \mathbf{W}^\top_K) \mathbf{Z}^\top_1 (\mathbf{I}_{L} \otimes \mathbf{W}_V\mathbf{W}^\top_V)\ \mathbf{Z}_1 (\mathbf{I}_{d_V} \otimes \mathbf{W}_K)\|_2 \leq\\
    &\leq \frac{2}{L d_V d_K} \| \mathbf{W}_K\|_2^2 \|\mathbf{Z}_1 \|^2_2 \| \mathbf{W}_V\|^2_2 \leq\\
    &\leq \frac{2}{L d_V d_K}\| \mathbf{W}_K\|_2^2\| \mathbf{W}_V\|^2_2 \frac{1}{L^2} \| \mathbf{X}\|^6_2 =\\
    &= \frac{2}{L^3 d_V d_K} \| \mathbf{W}_K\|_2^2\| \mathbf{W}_V\|^2_2 \mathbf{X}\|^6_2.
\end{align}
Между матрицей~$\mathbf{W}_V$ и матрицей~$\mathbf{W}_Q$:
\begin{align}
    \| \mathbf{H}_o(\mathbf{W}_V, \mathbf{W}_Q)\|_2 &\leq \frac{2}{L d_V \sqrt{d_K}} \|\mathbf{M}_1^\top \otimes \mathbf{W}_V^\top \|_2 \| \mathbf{Z}_1\|_2 \| \mathbf{I}_{d_V} \otimes \mathbf{W}_K \|_2 \leq\\
    &\leq \frac{2}{L d_V \sqrt{d_K}} L \| \mathbf{X}\|_2 \| \mathbf{W}_V\|_2 \frac{1}{L} \| \mathbf{X}\|^3_2 \| \mathbf{W}_K \|_2 =\\
    &= \frac{2}{L d_V \sqrt{d_K}} \| \mathbf{W}_V\|_2 \| \mathbf{W}_K \|_2 \| \mathbf{X}\|^4_2
\end{align}
Между матрицей~$\mathbf{W}_Q$ и матрицей~$\mathbf{W}_K$:
\begin{align}
    &\left\|\mathbf{H}_o(\mathbf{W}_Q, \mathbf{W}_K)\right\|_2 \leq \\
    &\leq \frac{2}{L d_V d_K} \big\|(\mathbf{I}_{d_V} \otimes \mathbf{W}^\top_K) \mathbf{Z}_1^\top(\mathbf{I}_L \otimes \mathbf{W}_V \mathbf{W}^\top_V)\mathbf{Z}_1 (\mathbf{W}_Q \otimes \mathbf{I}_{d_V}) \mathbf{K}{d_K, d_V}\big\|_2 \leq \\
    &\leq \frac{2}{L^3 d_V d_K} \big\|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{W}_V\|_2^2 \|\mathbf{X} \big\|^6_2.
\end{align}
Для всех оценок использовались свойства~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, а также свойства $\|\mathbf{K}_{d_V d_K} \|_2=1$, потому что $\mathbf{K}_{m,n}$ является коммутативной матрицей, описанной в определении~\ref{def:commutation_matrix}. 

Далее проведем анализ матрицы~$\mathbf{H}_f$.
Для этого начнем анализ с матрицы~$\mathbf{R}_m = \mathrm{vec}_r (\mathbf{F} (\mathbf{X}) - \textbf{Target})^T \otimes \mathbf{I}_m,$ который описан в рамках Теоремы 3.2 в работе~\cite{ormaniec2024attentionhessian}.
Так, как~$\mathrm{vec}_r(\cdot)$ является функцией векторизации: 
\begin{align}
    \|\mathrm{vec}_r(\mathbf{F}(\mathbf{X}) - \textbf{Target})\|_2 &= \| \mathbf{F}(\mathbf{X}) - \textbf{Target} \|_F \leq\\
    &\leq\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target} )} \| \mathbf{F}(\mathbf{X}) - \textbf{Target} \|_2,
\end{align}
тогда согласно свойству \ref{prop:matrix_norm_inequalities} получаем: 
\begin{align}
    \| \mathbf{R}_m\| &\leq \sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \|\mathbf{F}(\mathbf{X}) - \textbf{Target}\|_2 \leq\\
    &\leq\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} (\| \mathbf{A} \|_2 \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2) \leq\\
    &\leq \sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2)
\end{align}
где для получения оценок были использованы свойства матриц \ref{prop:matrix_product_norm} и \ref{prop:matrix_sum_norm}. В свою очередь норма матрицы перемешивания (англ. shuffling matrix) оценивается следующим образом:
\begin{align}
    \| \mathbf{S}\|_2 &= \|(\mathbf{I}_{d_V} \otimes \mathbf{K}_{d_V,d_V}) (\mathrm{vec}_r (\mathbf{I}_{d_V}) \otimes \mathbf{I}_{d_V})\|_2 \leq \\
    &\leq \| \mathrm{vec}_r (\mathbf{I}_{d_V}) \|_2 = \| \mathbf{I}_{d_V}\|_{F} = \sqrt{d_V}.
\end{align}

Для верхней оценки нормы матрицы $\| \frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2} \|_2$ воспользуемся Леммой C1 с работы \cite{ormaniec2024attentionhessian}, где указано, что:
\begin{equation}\label{chapter-2:theorem:proof_partialA}
    \frac{\partial^2 \mathbf{A}_{i,j}}{\partial \mathbf{T}_{i,:} \partial \mathbf{T}_{i,:}} = \mathbf{A}_{i,j} \left(2\mathbf{A}_{i,:}\mathbf{A}_{i,:}^{\top} + \mathbf{E}_{j,j}^{L,L} - \text{diag}(\mathbf{A}_{i,:}) - \mathbf{e}_j \mathbf{A}_{i,:}^{\top} - \mathbf{A}_{i,:}\mathbf{e}_j^{\top}\right) \in \mathbb{R}^{L \times L},
\end{equation}
где 
\[
    \mathbf{E}_{j,j}^{L, L} = \mathbf{e}_j \mathbf{e}_j^{\top} \in \mathbb{R}^{L \times L},
\]
поэтому он содержит только один ненулевой элемент, который равен 1 в позиции $(j, j)$.
Кроме того, вторая производная softmax имеет блочно-диагональную структуру, а следовательно используя свойство~\ref{prop:block_matrix_norm} нормы блочно диагональной матрицы получаем:
\[
    \left\|\frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2}\right\|_2 = \max_{i,j} \left\|\frac{\partial^2 \mathbf{A}_{i,j}}{\partial \mathbf{T}_{i,:} \partial \mathbf{T}_{i,:}}\right\|_2.
\]
Приходим к тому, что требуется оценить следующую норму:
\[
    \left\|\frac{\partial^2 \mathbf{A}_{i,j}}{\partial \mathbf{T}_{i,:} \partial \mathbf{T}_{i,:}} \right\|_2.
\]

Как было указано ранее $\mathbf{A}_{i,j} \leq 1,$ а следовательно можем оценить матрицу $\| \mathbf{A}_{i,:}\mathbf{A}_{i,:}^{\top} \|_2$, так как $\mathbf{A}_{i,:}$ является строкой $\text{softmax}$-матрицы, то значение суммы строки равняются $1$.
Таким образом, мы можем использовать векторно-матричные неравенства для получения выражение:
\begin{equation}\label{chapter-2:theorem:proof_Arow}
    \| \mathbf{A}_{i,:}\mathbf{A}_{i,:}^{\top} \|_2 \leq \|\mathbf{A}_{i,:}\|^2_2 \leq \|\mathbf{A}_{i,:}\|_1^2 = 1.
\end{equation}

Аналогично заметим, что 
\begin{equation}\label{chapter-2:theorem:proof_Enorm}
    \|\mathbf{E}_{j,j}^{m,n}\|_2 = \| \mathbf{e}_j \mathbf{e}_j^{\top}\|_2 \leq 1.
\end{equation}

Перейдем к оценке нормы диагональной матрицы $\|diag(\mathbf{A}_{i,:})\|_2$.
Заметим, что для диагональной матрицы верно следующее выражение:
\begin{equation}\label{chapter-2:theorem:proof_Adiag}
    \|diag(\mathbf{A}_{i,:})\|_2 = \max \limits_j \mathbf{A}_{i,j} \leq 1.
\end{equation}

Используя оценки~\eqref{chapter-2:theorem:proof_Arow} и~\eqref{chapter-2:theorem:proof_Adiag} и~\eqref{chapter-2:theorem:proof_Enorm} оценим нормы $\mathbf{e}_j \mathbf{A}_{i,:}^{\top}$ и $\mathbf{A}_{i,:}\mathbf{e}_j^{\top}.$
Матрицы $\mathbf{e}_j \mathbf{A}_{i,:}^{\top}$ and $\mathbf{A}_{i,:}\mathbf{e}_j^{\top}$ являются матрицами ранга~$1,$ причем только с одной не нулевой строкой и колонкой соотвественно с элементами матрицы~$\mathbf{A}_{i,:}.$
Следовательно их спектральные нормы оценивается сверху нормой матрицы~$\|\mathbf{A}_{i,:}\|_2 \leq 1$.

Получаем, что все слагаемые в выражении~\eqref{chapter-2:theorem:proof_partialA} имеют верхнюю оценку $1,$ а следовательно:
\begin{align}
    \left\| \frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2} \right\|_2 \leq 6 
\end{align}

Возвращаясь к выражению~\eqref{chapter-2:theorem:proof_Znorm} получаем оценку матрицы $\| \mathbf{Z}_2 \|_2$:
\begin{align}
    \| \mathbf{Z}_2 \|_2 &= \| \left(\mathbf{I}_L \otimes \mathbf{X}^\top \otimes \mathbf{X}^\top \otimes \mathbf{X}^\top\right) \left(\partial^2\mathbf{A}/\partial\mathbf{T}^2\right) \left(\mathbf{X} \otimes \mathbf{X}\right) \|_2 \leq \\
    &\leq \| \mathbf{X}\|^5_2 \left\| \frac{\partial^2\mathbf{A}}{\partial\mathbf{T}^2} \right\|_2 \leq 6 \| \mathbf{X}\|^5_2
\end{align}
Оцениваем часть~$\mathbf{H}_\mathrm{f}.$ Для нормы между матрицами~$\mathbf{W}_V$ и~$\mathbf{W}_V$:
\begin{align}
    \|\mathbf{H}_\mathrm{f}(\mathbf{W}_V, \mathbf{W}_V)\|_2 = 0
\end{align}
Для нормы между матрицами~$\mathbf{W}_Q$ и~$\mathbf{W}_Q$:
\begin{align}
    \|\mathbf{H}_\mathrm{f}(\mathbf{W}_Q, \mathbf{W}_Q)\|_2 &= \frac{2}{Ld_V d_K} \|\mathbf{R}_{d_V d_K} \left(\mathbf{I}_L \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V} \otimes \mathbf{W}_K^\top\right) \mathbf{Z}_2 \left(\mathbf{I}_{d_V} \otimes \mathbf{W}_K\right)\|_2, \\
    &\leq \frac{2}{Ld_V d_K} \| \mathbf{R}_{d_V d_K} \|_2 \| \mathbf{W}_V \|_2 \| \mathbf{W}_K\|_2 \|\mathbf{Z}_2 \|_2 \| \mathbf{W}_K\|_2 \\
    &\leq 6\frac{2}{Ld_V d_K}\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 +\\
    &\quad +\|\textbf{Target}\|_2\Big)\| \mathbf{W}_V \|_2 \| \mathbf{W}_K\|^2_2\| \mathbf{X}\|^5_2 =\\
    &= \frac{12}{d_V d_K}\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \\
    &\quad+\|\textbf{Target}\|_2\Big)\| \mathbf{W}_V \|_2 \| \mathbf{W}_K\|^2_2\| \mathbf{X}\|^5_2
\end{align}
Для нормы между матрицами~$\mathbf{W}_V$ и~$\mathbf{W}_Q$:
\begin{align}
    \|\mathbf{H}_\mathrm{f}(\mathbf{W}_V, \mathbf{W}_Q)\|_2 &= \frac{2}{Ld_V\sqrt{d_K}} \|\mathbf{R}_{d_V^2} \left(\mathbf{I}_L \otimes \mathbf{S}\right) \mathbf{Z}_1 \left(\mathbf{I}_{d_V} \otimes \mathbf{W}_K\right)\|_2 \leq \\
    &\leq \frac{2}{Ld_V\sqrt{d_K}} \| \mathbf{R}_{d_V^2}\|_2 \| \mathbf{S} \|_2 \|\mathbf{Z}_1 \|_2 \| \mathbf{W}_K\|_2 \leq\\
    &\leq \frac{2}{Ld_V\sqrt{d_K}} \sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 +\\
    &\quad +\|\textbf{Target}\|_2\Big) \sqrt{d_V} \frac{1}{L} \|\mathbf{X}\|^3_2\|\mathbf{W}_K\|_2 = \\
    &= \frac{2\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})}}{L^2\sqrt{d_Vd_K}}\Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \\
    &\quad+\|\textbf{Target}\|_2\Big)\|\mathbf{W}_K\|_2 \|\mathbf{X}\|^3_2
\end{align}
Для нормы между матрицами~$\mathbf{W}_Q$ и~$\mathbf{W}_K$:
\begin{align}
    &\|\mathbf{H}_\mathrm{f}(\mathbf{W}_Q, \mathbf{W}_K)\| \leq \\
    &\leq\frac{2}{Ld_V d_K}\| \mathbf{R}_{d_V d_K} \left(\mathbf{I}_L \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V} \otimes \mathbf{W}_K^\top\right) \mathbf{Z}_2 \left(\mathbf{W}_Q \otimes \mathbf{I}_{d_V}\right) \mathbf{K}_{d_K, d_V}\|_2 + \\
    &\quad + \frac{2}{Ld_V\sqrt{d_K}} \|\mathbf{R}_{d_V} \left(\mathbf{I}_L \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V}\right) \left(\mathbf{Z}_1 \otimes \mathbf{I}_{d_V}\right) \mathbf{S} \otimes \mathbf{I}_{d_K}\|_2 \leq\\
    &\leq  \frac{2}{Ld_V d_K}\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 +\\
    &\quad + \|\textbf{Target}\|_2\Big) \|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_26 \| \mathbf{X}\|^5_2 + \\
    &\quad+\frac{2}{Ld_V\sqrt{d_K}}\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \\
    &\quad +\|\textbf{Target}\|_2\Big) \|\mathbf{W}_V \|_2 \frac{1}{L} \|\mathbf{X}\|^3_2 \sqrt{d_V} =\\
    &=\frac{2\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2)}{Ld_V\sqrt{d_Vd_K}} \|\mathbf{W}_V\|_2 \cdot \\
    & \quad \cdot \Big(3L\|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{X}\|^5_2 + \frac{d_V}{L} \|\mathbf{X}\|^3_2\Big).
\end{align}

Собирая все оценки вместе, используя матричное свойство \ref{prop:matrix_norm_inequalities} для всех блоков $\{K, Q, V\}$:
\begin{align}
    &\| \mathbf{H} (\mathbf{W}_i, \mathbf{W}_j)\|_2 \leq 3\max \limits_{i,j \in \{Q, K, V\}} \Big(\|\mathbf{H}_o(\mathbf{W}_i, \mathbf{W}_j)\|_2 + \|\mathbf{H}_f(\mathbf{W}_i, \mathbf{W}_j)\|_2\Big)
\end{align}
Подставляя оценки получаем следующую оценку на матрицу Гессе:
\begin{align}
    &\| \mathbf{H} (\mathbf{W}_i, \mathbf{W}_j)\|_2 \leq\\
    & \leq 3 \max \Bigg(\frac{2L}{d_V} \| \mathbf{X}\|^2_2, \\
    &\frac{2}{L^3 d_V d_K} \| \mathbf{W}_K\|_2^2 \| \mathbf{W}_V\|^2_2 \| \mathbf{X}\|^6_2 +\\
    &\quad+ \frac{12}{d_V d_K} \sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 +\\
    &\qquad+\|\textbf{Target}\|_2\Big) \| \mathbf{W}_V \|_2 \| \mathbf{W}_K\|^2_2 \| \mathbf{X}\|^5_2, \\
    &\frac{2}{L d_V \sqrt{d_K}} \| \mathbf{W}_V\|_2 \| \mathbf{W}_K \|_2 \| \mathbf{X}\|^4_2 +\\
    &\quad+\frac{2\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})}}{L^2\sqrt{d_V d_K}} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2) \|\mathbf{W}_K\|_2 \|\mathbf{X}\|^3_2,\\
    &\frac{2}{L^3 d_V d_K} \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{W}_V\|^2_2 \|\mathbf{X} \|^6_2 + \\
    &\quad+\frac{2\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2\Big)}{L d_V \sqrt{d_V d_K}} \cdot\\
    &\qquad\cdot\|\mathbf{W}_V\|_2 \Big(3L \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{X}\|^5_2 + \frac{d_V}{L} \|\mathbf{X}\|^3_2\Big)\Bigg).
\end{align}

Полученное выражение почти полностью соответствует выражению~$M,$ где для полного соотвествия требуется воспользоваться неравенством~$\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target}) \le \min(L, d_V)$. 
\end{proof}

Теорема~\ref{thm:self_attention_hessian_estimation} оценивает только один слой самовнивания. Теперь перейдем к оценке полного блока трансформера. Полный трансформер слой содержит слой самовнимания, блок полносвязной сети (англ. FFN), и слоя нормализации выходов (англ. LayerNorm). Весь блок описывается следующими выражениями:
\begin{align}\label{eq:transformer}
    \mathbf{Y} &= \text{LayerNorm}(\mathbf{X} + \mathbf{F}(\mathbf{X})) \\ 
    \mathbf{Z} &= \text{LayerNorm}(\mathbf{Y} + \text{FFN}(\mathbf{Y})), 
\end{align} 
где
\[
    \text{FFN}(\mathbf{Y}) = \sigma(\mathbf{Y} \mathbf{W}_1 + \mathbf{b}_1) \mathbf{W}_2 + \mathbf{b}_2,
\]
с матрицами параметров $\mathbf{W}_1 \in \mathbb{R}^{d_V \times d_{\text{ff}}} $, матрицами$\mathbf{W}_2 \in \mathbb{R}^{d_{\text{ff}} \times d_V}$, векторами~$b_1 \in \mathbb{R}^{d_{\text{ff}}}$, $b_2 \in \mathbb{R}^{d_V}$, а также функцией активации~$\sigma$.
Функция $\text{LayerNorm}(\mathbf{X})$ определяется для входной матрицы~$\mathbf{X} \in \mathbb{R}^{L \times d_V}$ следующим образом:
\begin{align}
    \text{LayerNorm}(\mathbf{X})_{i,j} = \mathbf{\gamma}_j \cdot \frac{\mathbf{X}_{i,j} - \mu_i}{\sqrt{\sigma_i^2}} + \mathbf{\beta}_j,
\end{align}
где параметры~$\mu_i, \sigma_i$ определяются следующим образом:
\[
    \mu_i = \frac{1}{d_V} \sum_{j=1}^{d_V} \mathbf{X}_{i,j}, \quad \sigma_i^2 = \frac{1}{d_V} \sum_{j=1}^{d_V} (\mathbf{X}_{i,j} - \mu_i)^2,
\]
а параметры~$\gamma_j, \beta_j$ являются настраиваемым в процессе оптимизации.

Итого, получаем полный список параметров полного слоя трансформера:
\[
    \mathbf{w} = \{\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V, \mathbf{W}_1, \mathbf{W}_2, \mathbf{b}_1, \mathbf{b}_2, \mathbf{\gamma}, \mathbf{\beta}\}
\], где $\mathbf{\gamma},\mathbf{\beta}$ являются параметрами LaterNorm, для простоты вычисления в некоторых случаях введем предположения, что параметры~$\mathbf{\gamma},\mathbf{\beta}$ являются постоянными и не меняются в процессе оптимизации.

\subsection{Матрица Гессе для LayerNorm слоя}

Для начала вычислим для функции~$\mathrm{LayerNorm}$ матрицу Якоби относительно параметров модели, для этого докажем Теорему~\ref{thm:layernorm_derivative}.
\begin{theorem}\label{thm:layernorm_derivative}
    Пусть задана матрица $\mathbf{X} \in \mathbb{R}^{L \times d_V}$. Определим функцию~$\mathbf{M}(\mathbf{X})$ следующим образом:
    \begin{align}
        \mathbf{M}(\mathbf{X}) &= \mathbf{X} - \tfrac{1}{d_V}\mathbf{X}\mathbf{1}_{d_V}\mathbf{1}_{d_V}^\top, \\
        \sigma(\mathbf{X}) &= \tfrac{1}{\sqrt{d_V}}\big(\mathbf{M}(\mathbf{X})^{\circ 2}\mathbf{1}_{d_V}\big)^{\circ 1/2},\\
        \mathbf{P}(\mathbf{X}) &= \mathrm{diag}^{-1}(\sigma(\mathbf{X})).
    \end{align}
    Тогда для функции LayerNorm :
    \[
        \text{LayerNorm}(\mathbf{X}) = \mathbf{P}(\mathbf{X}) \mathbf{M}(\mathbf{X}),
    \]
    матрица Якоби относительно переменной~$\mathbf{X}$ определяется следующим образом:
    \begin{align}
        \frac{\partial \text{LayerNorm}(\mathbf{X})}{\partial \mathbf{X}} &= (\mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V}) \left(\mathbf{I}_{Ld_V} - \tfrac{1}{d_V}(\mathbf{I}_L \otimes \mathbf{1}_{d_V \times d_V})\right)+\\
    &\quad+\left(\mathbf{I}_L \otimes \mathbf{M}(\mathbf{X})^\top\right)\frac{\partial\mathbf{P}(\mathbf{X})}{\partial\mathbf{X}},
    \end{align}
    где
    \begin{align}
    \frac{\partial \mathbf{P}}{\partial \mathbf{X}}&=\frac{1}{\sqrt{d_V}}\Big( -\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \Big)\cdot\\
    &\quad\cdot\big( \mathbf{e}_1 \otimes \mathbf{e}_1, \dots, \mathbf{e}_L \otimes \mathbf{e}_L \big)\cdot\\
    &\quad\cdot\Big(\mathrm{diag}^{-1}\!\big(\mathrm{vec}_r^{1/2}(\mathbf{M}^{\circ 2}\mathbf{1}_{d_V})\big)(\mathbf{I}_L \otimes \mathbf{1}_{d_V}^\top)\mathrm{diag}\left(\mathrm{vec}_r(\mathbf{M})\right)\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\Big),
    \end{align}
    где $\mathbf{D} = \mathrm{diag}(\sigma(\mathbf{X})).$
\end{theorem}
\begin{proof}
Представим функцию LayerNorm в матричном виде:
\begin{equation}
    \text{LayerNorm}(\mathbf{X}) = \mathbf{P}(\mathbf{X})\mathbf{M}(\mathbf{X}),
\end{equation}
где матрица~$\mathbf{P}(\mathbf{X}) = \mathbf{D}^{-1}$, а матрица~$\mathbf{D} = \textit{diag}(\sigma(\mathbf{X})),$ в свою очередь согласно свойству~\ref{prop:elem_wise_division} матрица~$\mathbf{M}(\mathbf{X}) = (\mathbf{X} - \mu(\mathbf{X})\mathbf{1}^\top_{d_V}).$

Используя Лемму~\ref{lemma:matrix_funcs_product_derivative} получаем выражение для произведения матричнозначных функций:
\begin{equation}
    \frac{\partial\text{LayerNorm}(\mathbf{X})}{\partial \mathbf{X}} = ( \mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V}) \frac{\partial \mathbf{M}}{\partial \mathbf{X}} + (\mathbf{I}_L\otimes \mathbf{M}^\top)\frac{\partial \mathbf{P}}{\partial \mathbf{X}}
\end{equation}

Вычислим значение производной~$\frac{\partial \mathbf{M}}{\partial \mathbf{X}},$ используя матричные вычисления~$\mathbf{M}(\mathbf{X}) = (\mathbf{X} - \mu(\mathbf{X}) \mathbf{1}^{\top}_{d_V}) = (\mathbf{X} - \frac{1}{d_V}\mathbf{X} \mathbf{1}_{d_V} \mathbf{1}^{\top}_{d_V}) = (\mathbf{X} - \frac{1}{d_V}\mathbf{X} \mathbf{1}_{d_V \times d_V})$. Получаем:
\begin{equation}
   \frac{\partial \mathbf{M}}{\partial \mathbf{X}} = \frac{\partial  (\mathbf{X} - \frac{1}{d_V}\mathbf{X} \mathbf{1}_{d_V \times d_V})}{\partial \mathbf{X}} = (\mathbf{I}_L \otimes \mathbf{I}_{d_V}) - \frac{1}{d_V}(\mathbf{I}_L \otimes \mathbf{1}_{d_V \times d_V})
\end{equation}

Далее вычислим значение производной~$\frac{\partial \mathbf{P}}{\partial \mathbf{X}}.$
Для начала получим выражение для нелинейного преобразования~$\sigma(\mathbf{X}).$
Данное выражение в матричном виде принимает вид:
\[
    \sigma(\mathbf{X}) =  \left(\frac{1}{d_V} (\mathbf{X} - \mu(X)\mathbf{1}_{d_V}^\top)^{\circ 2} \mathbf{1}_{d_V}\right)^{\circ \frac{1}{2}} = \frac{1}{\sqrt{d_V}} \left(\mathbf{M}(\mathbf{X})^{\circ 2} \mathbf{1}_{d_V}\right)^{\circ{\frac{1}{2}}},
\]
где~$\circ \alpha$ операция поэлементного взятия степени $\alpha$ описанного в определении~\ref{def:vec_elem_ops}.
Далее, применив цепное правило получаем:
\begin{align}
    \frac{\partial \mathbf{P}}{\partial \mathbf{X}} &= \frac{\partial \mathbf{D}^{-1}}{\partial \mathbf{D}} \frac{\partial\textit{diag}(\sigma(\mathbf{X}))}{\partial \sigma(\mathbf{X})} \frac{\partial \sigma(\mathbf{X})}{\partial \mathbf{X}},
\end{align}
причем используя свойства~\ref{lemma:hadamard_square_derivative}, \ref{lemma:hadamard_root_derivative} и~\ref{prop:matrix_product_derivative} получаем выражение для~$\frac{\partial \sigma(\mathbf{X})}{\partial \mathbf{X}}:$
\begin{equation}
    \frac{\partial \sigma(\mathbf{X})}{\partial \mathbf{X}} = \frac{1}{\sqrt{d_V}} \frac{\partial \tau^{\circ \frac{1}{2}}}{\partial \tau} \frac{\partial \tau}{\partial \mathbf{Q}} \frac{\partial \mathbf{Q}}{\partial {\mathbf{X}}},
\end{equation}
где~$\tau = \mathbf{Q}\cdot\mathbf{1}_L, \mathbf{Q} = \mathbf{M}^{\circ{2}},$ а следовательно подставляя получаем:
\begin{align}
    \frac{\partial \sigma(\mathbf{X})}{\partial \mathbf{X}} &= \frac{1}{\sqrt{d_V}} \frac{\partial \tau^{\circ \frac{1}{2}}}{\partial \tau} \frac{\partial \mathbf{Q}\cdot\mathbf{1}_{d_V}}{\partial \mathbf{Q}} \frac{\partial \mathbf{M}^{\circ{2}}}{\partial \mathbf{M}} \frac{\partial \mathbf{M}}{\partial \mathbf{X}} = \\
    \\ &= \frac{1}{\sqrt{d_V}} \frac{1}{2} \textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\tau)) (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V}) 2\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial \mathbf{X}} = \\
    &= \frac{1}{\sqrt{d_V}}\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\cdot (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M})) \frac{\partial \mathbf{M}}{\partial \mathbf{X}}.
\end{align}

Используя Леммы~\ref{lemma:invert_derivative} и \ref{lemma:diag_derivative} получаем:
\begin{align}
    \frac{\partial \mathbf{P}}{\partial \mathbf{X}} &= \frac{1}{\sqrt{d_V}}\left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right) \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big) \cdot \\
    &\quad\cdot\left(\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\cdot (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\right).
\end{align}

И того мы получили все составляющие для вычисления матрицы Якобы для LayerNorm оператора:
\begin{align}
    &\frac{\partial\text{LayerNorm}(\mathbf{X})}{\partial \mathbf{X}} = ( \mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V}) \frac{\partial \mathbf{M}}{\partial \mathbf{X}} + (\mathbf{I}_L\otimes \mathbf{M}^\top)\frac{\partial \mathbf{P}}{\partial \mathbf{X}} = \\
    & = ( \mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V}) \frac{\partial \mathbf{M}}{\partial \mathbf{X}} +\\
    &\quad+ (\mathbf{I}_L\otimes \mathbf{M}^\top)\frac{1}{\sqrt{d_V}}\left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right) \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big) \cdot \\
    &\qquad\cdot \left(\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\cdot (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\right),
\end{align}
где
\begin{align}
    \mathbf{M}(\mathbf{X}) &= (\mathbf{X} - \frac{1}{d_V}\mathbf{X} \mathbf{1}_{d_V \times d_V})\\
    \mathbf{P}(\mathbf{X}) &= \textit{diag}^{-1}(\sigma(\mathbf{X}))\\
    \frac{\partial \mathbf{M}}{\partial \mathbf{X}} &= (\mathbf{I}_L \otimes \mathbf{I}_{d_V}) - \frac{1}{d_V}(\mathbf{I}_L \otimes \mathbf{1}_{d_V \times d_V}.
\end{align}
\end{proof}

Теперь же вычислим для функции матрицу Гессе относительно параметров модели, для этого докажем Теорему~\ref{thm:layernorm_second_derivative}.
\begin{theorem}\label{thm:layernorm_second_derivative}
    Пусть задан оператор~$LayerNorm$ в виде аналогичном Теореме~\ref{thm:layernorm_derivative}:
    \[
        \text{LayerNorm}(\mathbf{X}) = \mathbf{P}(\mathbf{X}) \mathbf{M}(\mathbf{X}),
    \]
    с матрицей Якоби, полученную в Теореме~\ref{thm:layernorm_derivative}:
    \[
        \frac{\partial \text{LayerNorm}}{\partial \mathbf{X}} = (\mathbf{P} \otimes \mathbf{I}_{d_V}) \mathbf{G} + (\mathbf{I}_L \otimes \mathbf{M}^\top) \mathbf{H},
    \]
    где дополнительно введены обозначения константы:
    \[
        \mathbf{G} = \left(\mathbf{I}_{Ld_V} - \tfrac{1}{d_V}(\mathbf{I}_L \otimes \mathbf{1}_{d_V \times d_V})\right),
    \] а также оператор, аналогичный оператору в Теореме~\ref{thm:layernorm_derivative}:
    \[
        \mathbf{H} = \frac{\partial \mathbf{P}}{\partial \mathbf{X}}.
    \]
    Тогда для функции~$\text{LayerNorm}$ матрица Гессе относительно параметров~$\mathbf{X}$ имеет вид:
    \begin{align}\label{chapter-2:theorem:layernorm_second_derivative:eqstemant}
        \frac{\partial^2 \text{LayerNorm}}{\partial \mathbf{X}^2} &= \left( (\mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V}) \otimes \mathbf{I}_{L d_V} \right)\frac{\partial^2 \mathbf{M}}{\partial \mathbf{X}^2} +\\
        &\quad+\left( \mathbf{I}_{L d_V} \otimes \mathbf{G}^\top \right) \frac{\partial (\mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V})}{\partial \mathbf{X}} + \\
        &\quad+ \left( (\mathbf{I}_L \otimes \mathbf{M}^\top ) \otimes \mathbf{I}_{L d_V} \right) \frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2} + \left( \mathbf{I}_{L d_V} \otimes \mathbf{H}^\top \right) \frac{\partial (\mathbf{I}_L \otimes \mathbf{M}^\top )}{\partial\mathbf{X}}.
    \end{align}
    причем все матрицы явно вычислимые и задаются формулами, которые указаны ниже в доказательстве.
\end{theorem}
\begin{proof}
Используя свойство матричного произведения~\ref{prop:matrix_product_derivative} получаем следующее выражение матрицы Гессе для оператора~\text{LayerNorm}:
\begin{align}
    \frac{\partial^2\text{LayerNorm}}{\partial\mathbf{X}^2} &= \left( (\mathbf{P}(\mathbf{X})  \otimes  \mathbf{I}_{d_V}) \otimes \mathbf{I}_{Ld_V} \right)\frac{\partial^2 \mathbf{M}}{\partial \mathbf{X}^2} +\\
    &\quad+\left( \mathbf{I}_{Ld_V} \otimes  \left(\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\right)^\top \right) \frac{\partial (\mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V})}{\partial \mathbf{X}} + \\
    &\quad+ \left( (\mathbf{I}_L \otimes \mathbf{M}^\top ) \otimes \mathbf{I}_{L d_V}  \right) \frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2} +\\
    &\quad+\left( \mathbf{I}_{L d_V} \otimes \left(\frac{\partial \mathbf{P}}{\partial \mathbf{X}}\right)^\top \right) \frac{\partial (\mathbf{I}_L \otimes \mathbf{M}^\top )}{\partial\mathbf{X}},
\end{align}
причем заметим, что~$\mathbf{P} \in \mathbb{R}^{L \times L}$, $\mathbf{M} \in \mathbb{R}^{L \times d_V}$, $\frac{\partial \mathbf{M}}{\partial \mathbf{X}} \in \mathbb{R}^{Ld_V \times Ld_V}$, $\frac{\partial \mathbf{P}}{\partial \mathbf{X}} \in \mathbb{R}^{L^2 \times Ld_V}.$ Используя свойства~\ref{prop:kronecker_product_derivative} и Леммы~\ref{lemma:transposed_matrix_derivative} получаем следующие выражения первых и вторых производных: 
\begin{align}
    \frac{\partial^2 \mathbf{M}}{\partial \mathbf{X}^2} &= 0, \\
    \frac{\partial (\mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V} )}{\partial \mathbf{X}} &= \frac{\partial (\mathbf{P} \otimes \mathbf{I}_L)}{\partial \mathbf{P}} \frac{\partial \mathbf{P}}{\partial\mathbf{X}} = \left(\mathbf{I}_L \otimes \mathbf{K}_{L, L} \otimes \mathbf{I}_L \right) \left(\mathbf{I}_{L^2} \otimes  \mathrm{vec}_r(\mathbf{I}_L)  \right) \frac{\partial \mathbf{P}}{\partial \mathbf{X}}, \\
    \frac{\partial (\mathbf{I}_L \otimes \mathbf{M}^\top )}{\partial\mathbf{X}} &= \frac{\partial ( \mathbf{I}_L \otimes \mathbf{M}^\top)}{\partial \mathbf{M}^\top} \frac{\partial \mathbf{M}^\top}{\partial \mathbf{M}} \frac{\partial \mathbf{M}}{\partial \mathbf{X}} =\\
    &=\left(\mathbf{I}_{L} \otimes \mathbf{K}_{d_V,L} \otimes \mathbf{I}_L \right) \left(\mathrm{vec}_r(\mathbf{I}_L) \otimes  \mathbf{I}_{L d_V} \right) \mathbf{K}_{d_V, L} \frac{\partial \mathbf{M}}{\partial \mathbf{X}}.
\end{align}

Перейдем к оценке вторых производных матрицы~$\mathbf{P}.$
Рассмотрим каждое слагаемое в матрице подробнее.
Матрица~$\mathbf{D}$ является диагональной матрицей~$\textit{diag}(\sigma(\mathbf{X})),$ причем размер вектор~$\sigma(\mathbf{X})$ имеет размерность  $L\times 1$, тогда матрица~$\mathbf{D} \in \mathbb{R}^{L\times L},$ а следовательно слагаемое~$\left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right) \in \mathbb{R}^{L^2 \times L^2}$.
Каждый базисный вектор~$\mathbf{e}_i$ имеет размерность~$L\times1$, а следовательно~$\mathbf{e}_i \otimes \mathbf{e}_i \in \mathbb{R}^{L^2 \times 1},$ и тогда $ \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big) \in \mathbb{R}^{L^2 \times L}$.
Ранее было доказано, что~$\mathbf{M}(\mathbf{X}) \in \mathbb{R}^{L \times d_V}$, тогда $M\cdot\mathbf{1}_{d_V} \in \mathbb{R}^{L \times 1}$, и следовательно слагаемое~$\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))$ имеет размерность $L \times L$.
Следующие слагаемые~$(\mathbf{I}_L \otimes \mathbf{1}^T_{d_V}) \in \mathbb{R}^{L \times Ld_V}$ и $\textit{diag}(\mathrm{vec}_r (M)) \in \mathbb{R}^{Ld_V \times Ld_V}$.
Последнее слагаемое~$\frac{\partial \mathbf{M}}{\partial \mathbf{X}}$ уже было посчитано ранее, причем его размерность~$Ld_V \times Ld_V$. Итого матрица~$\frac{\partial \mathbf{P}}{\partial \mathbf{X}}$ принадлежит пространству~$\mathbb{R}^{L^2 \times Ld_V}$.
 
Для удобства введем обозначение:
\[
    \frac{\partial \mathbf{P}}{\partial \mathbf{X}} = \frac{1}{\sqrt{d_V}} \mathbf{A}_1(\mathbf{X})\cdot \mathbf{B}_1(\mathbf{X}),
\]
где $\mathbf{A}_1 = \left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right),$ а $\mathbf{B}_1$ все остальное, обе матрицы были вычислены ранее.
Используя свойство произведения матричнозначных функций~\ref{lemma:matrix_funcs_product_derivative} вторая производная принимает вид:
\begin{align}
    \frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2} = \frac{1}{\sqrt{d_V}} \frac{\partial \mathbf{A}_1(\mathbf{X})\cdot \mathbf{B}_1(\mathbf{X})}{\partial \mathbf{X}} = \frac{1}{\sqrt{d_V}} \left(\mathbf{A}_1 \otimes  \mathbf{I}_{Ld_V}  \right) \frac{\partial \mathbf{B}_1}{\partial \mathbf{X}} + \left( \mathbf{I}_{L^2} \otimes  \mathbf{B}_1^\top \right) \frac{\partial \mathbf{A}_1}{\partial \mathbf{X}}.
\end{align}
Разберем вторую производную по частям. Сначала вычислим~$\frac{\partial \mathbf{A}_1}{\partial \mathbf{X}}.$ Используя Лемму~\ref{lemma:matrix_funcs_kronecker_product_derivative} получаем следующее выражение:
\begin{align}
    \frac{\partial \mathbf{A}_1}{\partial \mathbf{X}} &= \frac{\partial \left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right)}{\partial \mathbf{X}} =\\
    &=\left(\mathbf{I}_L \otimes \mathbf{K}_{L, L} \otimes \mathbf{I}_L \right) \Big((\mathbf{I}_{L^2} \otimes \mathrm{vec}_r(\mathbf{\mathbf{D}^{-\top}})) \cdot \frac{\partial -\mathbf{\mathbf{D}^{-1}}}{\partial \mathbf{X}}+ \\
    &\quad+ (\mathrm{vec}_r(-\mathbf{\mathbf{D}^{-1}}) \otimes \mathbf{I}_{L^2}) \cdot \frac{\partial \mathbf{\mathbf{D}^{-\top}}}{\partial \mathbf{X}} \Big).
\end{align}
Далее используя Леммы~\ref{lemma:transposed_matrix_derivative}, \ref{lemma:invert_derivative} получаем выражение на:
\begin{align}
    \frac{\partial -\mathbf{\mathbf{D}^{-1}}}{\partial \mathbf{X}} &= \frac{\partial -\mathbf{\mathbf{D}^{-1}}}{\partial \mathbf{D}}\frac{\partial \mathbf{D}}{\partial \mathbf{X}} = \left( \mathbf{D}^{-1} \otimes \mathbf{D}^{-\top}\right) \frac{\partial \mathbf{D}}{\partial \mathbf{X}},\\
    \frac{\partial \mathbf{\mathbf{D}^{-\top}}}{\partial \mathbf{X}} &= \frac{\partial \mathbf{\mathbf{D}^{-\top}}}{\partial \mathbf{D}^{-1}}\frac{\partial \mathbf{\mathbf{D}^{-1}}}{\partial \mathbf{D}} \frac{\partial \mathbf{\mathbf{D}}}{\partial \mathbf{X}} = \mathbf{K}_{L, L} \left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right) \frac{\partial \mathbf{\mathbf{D}}}{\partial \mathbf{X}},
\end{align}
где $\frac{\partial \mathbf{\mathbf{D}}}{\partial \mathbf{X}}$ вычисляется аналогично тому, как в Теореме~\ref{thm:layernorm_derivative}:
\begin{align}
    \frac{\partial \mathbf{\mathbf{D}}}{\partial \mathbf{X}} &= \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \dots   \mathbf{e}_L \otimes \mathbf{e}_L\Big)\cdot\\
    &\quad\cdot \left(\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\cdot (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\right),
\end{align}
заканчивая вывод оценки~$\frac{\partial \mathbf{A}_1}{\partial \mathbf{X}}.$

Перейдем к оценке~$\frac{\partial \mathbf{B}_1}{\partial \mathbf{X}}.$
Для начала снова представим матрицу~$\mathbf{B}_1$ в виде произведения матриц:
\begin{align}
    \mathbf{B}_1 = \mathbf{E} \mathbf{A}_2 \mathbf{B}_2,
\end{align}
где введены следующие обозначения матриц:
\begin{align}
    \mathbf{A}_2 &= \textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\\
    \mathbf{B}_2 &= (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\\
    \mathbf{E} &= \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big).
\end{align}
Для начала, заметим, что $\mathbf{E}$ является константной матрицей относительно матрицы~$\mathbf{X},$ а следовательно используя результат Леммы~\ref{lemma:matrix_funcs_product_derivative} получаем
\begin{align}
    \frac{\partial\mathbf{B}_1}{\partial\mathbf{X}} &= \frac{\partial \mathbf{E} \mathbf{A}_2 \mathbf{B}_2}{\partial (\mathbf{A}_2 \mathbf{B}_2)} \frac{\partial \mathbf{A}_2 \mathbf{B}_2}{\partial \mathbf{X}} = \left( \mathbf{E} \otimes \mathbf{I}_{L d_V}\right)\frac{\partial \mathbf{A}_2 \mathbf{B}_2}{\partial \mathbf{X}} \\
    &= \left( \mathbf{E} \otimes \mathbf{I}_{L d_V}\right) \left( (\mathbf{A}_2\otimes \mathbf{I}_{L d_V} )\frac{\partial \mathbf{B}_2}{\partial \mathbf{X}} + (\mathbf{I}_L \otimes \mathbf{B}_2^\top) \frac{\partial \mathbf{A}_2}{\partial \mathbf{X}}\right).
\end{align}

Далее осталось оценить матрицы~$\frac{\partial\mathbf{A}_2}{\partial \mathbf{X}}$ и~$\frac{\partial\mathbf{B}_2}{\partial \mathbf{X}}.$
Для оценки~$\frac{\partial\mathbf{B}_2}{\partial \mathbf{X}}$ разобьем на части:
\begin{align}
    \mathbf{B}_2 = \mathbf{J} \mathbf{A}_3 \mathbf{B}_3,
\end{align}
где~$\mathbf{J} = (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})$, $\mathbf{A}_3 = \textit{diag}(\mathrm{vec}_r (\mathbf{M})), \mathbf{B}_3 = \frac{\partial \mathbf{M}}{\partial \mathbf{X}}$. Аналогично, используя Лемму~\ref{lemma:matrix_funcs_product_derivative} получаем:
\begin{align}
    \frac{\partial \mathbf{B}_2}{\partial \mathbf{X}} &= \frac{\partial \mathbf{J} \mathbf{A}_3 \mathbf{B}_3}{\partial (\mathbf{A}_3 \mathbf{B}_3)} \frac{\partial \mathbf{A}_3 \mathbf{B}_3}{\partial \mathbf{X}} = \left( \mathbf{J} \otimes \mathbf{I}_{L d_V}\right)\frac{\partial \mathbf{A}_3 \mathbf{B}_3}{\partial \mathbf{X}} \\
    &= \left( \mathbf{J} \otimes \mathbf{I}_{L d_V}\right) \left((\mathbf{A}_3 \otimes \mathbf{I}_{Ld_V} ) \frac{\partial \mathbf{B}_3}{\partial \mathbf{X}} + ( \mathbf{I}_{Ld_V} \otimes \mathbf{B}_3^\top)\frac{\partial\mathbf{A}_3}{\partial \mathbf{X}}\right),
\end{align}
где
\begin{align}
    \frac{\partial\mathbf{A}_3}{\partial \mathbf{X}} &= \frac{\partial \textit{diag}(\mathrm{vec}_r(\mathbf{M}))}{\partial \mathbf{X}} = \frac{\partial \textit{diag}(\mathbf{v})}{\partial (\mathbf{v})} \frac{\partial \mathrm{vec}_r(\mathbf{M})}{\partial \mathbf{M}} \frac{\partial \mathbf{M}}{\partial \mathbf{X}},\\
    \frac{\partial \mathbf{B}_3}{\partial \mathbf{X}} &= \frac{\partial^2 \mathbf{M}}{\partial\mathbf{X}^2} = 0,
\end{align}
причем, используя Лемму~\ref{lemma:diag_derivative} получаем, что~$\frac{\partial \textit{diag}(\mathbf{v})}{\partial (\mathbf{v})} = \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big),$ где $\mathbf{e}_i \in \mathbb{R}^{Ld_V \times 1}$, а также $\frac{\partial \mathrm{vec}_r(\mathbf{M})}{\partial \mathbf{M}}=\mathbf{I}_{L d_V}.$
Для вычисления матрицы~$\frac{\partial\mathbf{A}_2}{\partial \mathbf{X}}$ воспользуемся Леммами~\ref{lemma:invert_derivative}, \ref{lemma:diag_derivative},\ref{lemma:hadamard_root_derivative},\ref{lemma:identification_theorem_vec_r} получаем выражение:
\begin{align}
    \textcolor{red}{\frac{\partial\mathbf{A}_2}{\partial\mathbf{X}} = \frac{\partial\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))}{\partial \mathbf{X}}=}
\end{align}

Собирая все полученные выражения воедино, получаем выражение для~$\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}$:
\begin{align}
    \frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2} &= \frac{1}{\sqrt{d_V}} \left(\mathbf{A}_1 \otimes  \mathbf{I}_{Ld_V}  \right) \frac{\partial \mathbf{B}_1}{\partial \mathbf{X}} + \left( \mathbf{I}_{L^2} \otimes  \mathbf{B}_1^\top \right) \frac{\partial \mathbf{A}_1}{\partial \mathbf{X}},
\end{align}
где~$\frac{\partial \mathbf{B}_1}{\partial \mathbf{X}},\frac{\partial \mathbf{A}_1}{\partial \mathbf{X}}, \mathbf{B}_1, \mathbf{A}_1$ определены и получены выше.

Итого, все матрицы выражения~\eqref{chapter-2:theorem:layernorm_second_derivative:eqstemant} вычислены, что заканчивает доказательство.
\end{proof}

\subsection{Матрица Гессе для нелинейности ReLU}

\begin{theorem}\label{theorem:relu_derivative_hessian}
Пусть задана матрица~$\mathbf{X} \in \mathbb{R}^{m \times n},$ тогда для оператора~$\mathrm{ReLU}$ почти всюду верно следующее выражение:
\begin{align}
    \frac{\partial \mathrm{ReLU}(\mathbf{X})}{\partial \mathbf{X}} &= \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big), \\
    \frac{\partial^2 \mathrm{ReLU}(\mathbf{X})}{\partial \mathbf{X}^2} &= \mathbf{0}.
\end{align}
\end{theorem}
\begin{proof}
Оператор~$\mathrm{ReLU}$ принимает следующий вид:
\[
    \mathrm{ReLU}(x) = \max(0, x),
\]
то есть, для каждого элемента~$x_{ij}$ в матрице~$\mathbf{X} \in \mathbb{R}^{m \times n}$ получаем:
\[
    \frac{\partial \mathrm{ReLU}(x_{ij})}{\partial x_{ij}} =
    \begin{cases}
    1 & \text{если}~x_{ij} > 0, \\
    0 & \text{если}~x_{ij} < 0, \\
    \text{неопределенно (субградиент }\in[0,1]\text{)} & \text{если}~x_{ij} = 0.
    \end{cases}
\]
В случае скалярной величины~$x \in \mathbb{R},$ множеством с неопределенным градиентом является множество~$\{0\},$ которое является множеством меры 0.
Рассматривая же матрицу~$\mathbf{X} \in \mathbb{R}^{m \times n}$ как точку в~$\mathbb{R}^{m\times n},$ дифференцируемым множеством является множество:
\[
    \mathcal{N} = \bigcup_{i,j} \left\{ \mathbf{X} \in \mathbb{R}^{m \times n} : x_{ij} = 0 \right\}.
\]
Заметим, что каждое множество~$\{x_{ij} = 0\}$ является гиперплоскостью коразмерности~$1$ в пространстве~$\mathbb{R}^{m\times n},$ а следовательно является множеством меры~$0$.
Так как, множество~$\mathcal{N}$ является конечным объединением множеств меры~$0,$ то и множество~$\mathcal{N}$ также имеет меру~$0.$ Получили, что оператор~$\mathrm{ReLU}$ является почти всюду дифференцируем в пространстве~$\mathbb{R}^{m \times n}.$

Для каждой дифференцируемой точки~$\mathbf{X} \notin \mathcal{N},$ применим построчкую векторизацию и Лемму~\ref{lemma:identification_theorem_vec_r}:
\begin{align}
    \mathrm{vec}_r(d\mathrm{ReLU}(\mathbf{X}))
    = \mathrm{diag}(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}}))  \mathrm{vec}_r(d\mathbf{X}),
\end{align}
причем, используя свойство~\ref{prop:vec_r_hadamard_product} и Лемму~\ref{lemma:diag_derivative} для диагональной матрицы получаем:
\begin{align}
    \frac{\partial \mathrm{ReLU}(\mathbf{X})}{\partial \mathbf{X}}
= \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big).
\end{align}

В силу того, что матрица Якоби является кусочно-постоянной, то ее дифференциал равен нулю почти всюду:
\[
    d\left(\frac{\partial \mathrm{ReLU}(\mathbf{X})}{\partial \mathbf{X}}\right) = \mathbf{0}, \quad \mathbf{X} \notin \mathcal{N},
\]
а следовательно и матрица Гессе почти всюду равна нулевой матрице:
\[
    \frac{\partial^2 \mathrm{ReLU}(\mathbf{X})}{\partial \mathbf{X}^2} = \mathbf{0}, \quad \mathbf{X} \notin \mathcal{N}.
\]
\end{proof}

\subsection{Матрица Гессе для трансформера}

\begin{lemma}\label{lemma:attention_phi_from_functional_hessian}
Рассмотрим слой внимания следующего вида:
\[
    \mathbf{F}(\mathbf{X}) = \mathbf{A}(\mathbf{T})  \mathbf{X}\mathbf{W}_V,
    \qquad
    \mathbf{T} = \frac{1}{\sqrt{d_K}} \mathbf{X}\mathbf{W}_Q \mathbf{W}_K^\top \mathbf{X}^\top,
\]
где $\mathbf{X} \in \mathbb{R}^{L \times d_V}$, $\mathbf{W}_Q,\mathbf{W}_K \in \mathbb{R}^{d_V \times d_K}$, $\mathbf{W}_V \in \mathbb{R}^{d_V \times d_V}$.
Матрица внимания $\mathbf{A}(\cdot)$ применяет построчный softmax.
Используем построчную векторизацию~$\mathrm{vec}_r(\cdot)$ и матрицы перестановки~$\mathbf{K}_{m,n}$ из определения~\ref{def:commutation_matrix}.

Определим блоки обобщенного функционального гессиана, используя результаты~\cite{ormaniec2024attentionhessian} в наших обозначениях $\mathrm{vec}_r$ как
\[
    \mathbf{H}_{\mathrm{f}}(\mathbf{W}_i,\mathbf{W}_j)
    = \big( \tfrac{\partial \ell}{\partial \mathbf{F}} \otimes \mathbf{I}_{p_i q_i} \big)  \frac{\partial^2 \mathbf{F}}{\partial \mathbf{W}_i \partial \mathbf{W}_j},
\]
где $p_i q_i$~--- размер матрицы~$\mathbf{W}_i$, а $\tfrac{\partial \ell}{\partial \mathbf{F}} \in \mathbb{R}^{L \times d_V}$~--- градиент функции потерь.

Для квадратичной функции потерь~$\ell(\mathbf{F})=\tfrac{1}{2}\|\mathbf{F}-\mathbf{Target}\|_F^2$ имеем~$\tfrac{\partial \ell}{\partial \mathbf{F}}=\mathbf{F}-\mathbf{Target}$ и матрицу построчного свертывания
\[
    \mathbf{R}_m := \mathrm{vec}_r\big(\mathbf{F}(\mathbf{X}) - \mathbf{Target}\big)^\top \otimes \mathbf{I}_m \in \mathbb{R}^{m \times (m \cdot L d_V)}.
\]
Тогда для~$i \in \{V,Q,K\}$ с $n_i:=p_i q_i$ блоки функционального гессиана могут быть факторизованы как
\[
    \mathbf{H}_{\mathrm{f}}(\mathbf{W}_i,\mathbf{W}_j) = \mathbf{R}_{n_i} \boldsymbol{\Phi}_{ij},
    \qquad
    \boldsymbol{\Phi}_{ij} := \frac{\partial^2 \mathbf{F}}{\partial \mathbf{W}_i \partial \mathbf{W}_j}
    \in \mathbb{R}^{(L d_V \cdot n_i) \times n_j}.
\]
В частности, блоки кривизны модели~$\boldsymbol{\Phi}_{ij}$ получаются из соответствующих выражений в~\cite[Теорема.~3.2]{ormaniec2024attentionhessian} удалением левого свертывания $\mathbf{R}_{n_i}$.

Теперь перечислим явные блоки, необходимые для вывода. Определим фиксированный оператор изменения формы
\[
    \mathbf{S} := \big(\mathbf{I}_{d_V} \otimes \mathbf{K}_{d_V, d_V}\big) \big(\mathrm{vec}_r \mathbf{I}_{d_V} \otimes \mathbf{I}_{d_V}\big)
    \in \mathbb{R}^{d_V^2 \times d_V},
\]
и операторы производных softmax
\begin{align}
    \mathbf{Z}_1 &:= (\mathbf{I}_L \otimes \mathbf{X}^\top)(\partial\mathbf{A}/\partial\mathbf{T})(\mathbf{X} \otimes \mathbf{X}) \in \mathbb{R}^{Ld_V \times d_V^2}, \\
    \mathbf{Z}_2 &:= \big(\mathbf{I}_L \otimes \mathbf{X}^\top \otimes \mathbf{X}^\top \otimes \mathbf{X}^\top\big)
    \frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2}
    (\mathbf{X} \otimes \mathbf{X})
    \in \mathbb{R}^{L d_V^3 \times d_V^2},
\end{align}
где~$\tfrac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2}$ обозначает тензор вторых производных softmax (построчный), согласованный с $\mathrm{vec}_r$ и произведениями Кронекера, как указано выше, а $\mathbf{Z}_1$ — линейный оператор первой производной softmax, используемый в~\cite{ormaniec2024attentionhessian}.

Тогда вторые производные внимания имеют вид:
\begin{align}
    \boldsymbol{\Phi}_{VV} &= \mathbf{0}_{(L d_V \cdot d_V^2) \times d_V^2}, \\
    \boldsymbol{\Phi}_{QQ} &= \frac{2}{L d_V d_K}
    \big(\mathbf{I}_L \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V} \otimes \mathbf{W}_K^\top\big)
    \mathbf{Z}_2
    \big(\mathbf{I}_{d_V} \otimes \mathbf{W}_K\big)
    \in \mathbb{R}^{(L d_V \cdot d_V d_K) \times d_V d_K}, \\
    \boldsymbol{\Phi}_{VQ} &= \frac{2}{L d_V \sqrt{d_K}}
    \big(\mathbf{I}_L \otimes \mathbf{S}\big)
    \mathbf{Z}_1
    \big(\mathbf{I}_{d_V} \otimes \mathbf{W}_K\big)
    \in \mathbb{R}^{(L d_V \cdot d_V^2) \times d_V d_K}, \\
    \boldsymbol{\Phi}_{QK}
    &= \frac{2}{L d_V d_K}
    \big(\mathbf{I}_L \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V} \otimes \mathbf{W}_K^\top\big)
    \mathbf{Z}_2
    \big(\mathbf{W}_Q \otimes \mathbf{I}_{d_V}\big) \mathbf{K}_{d_K, d_V} \\
    &\qquad + \frac{2}{L d_V \sqrt{d_K}}
    \big(\mathbf{I}_{d_V} \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V}\big)
    \big(\mathbf{Z}_1 \otimes \mathbf{I}_{d_V}\big) \mathbf{S} \otimes \mathbf{I}_{d_K} \in \mathbb{R}^{(L d_V \cdot d_V d_K) \times d_V d_K}.
\end{align}

Более того, в силу симметрии вторых производных, $\boldsymbol{\Phi}_{KQ}$ равен $\boldsymbol{\Phi}_{QK}$ с переставленными $\mathbf{W}_Q,\mathbf{W}_K$ и корректировкой перестановки с помощью $\mathbf{K}_{\cdot,\cdot}$.
Аналогичные симметричные соотношения дают $\boldsymbol{\Phi}_{QV}$ и $\boldsymbol{\Phi}_{KV}$ из $\boldsymbol{\Phi}_{VQ}$.
\end{lemma}
\begin{proof}  
По определению обобщенного функционального гессиана в \cite{ormaniec2024attentionhessian},
\[
    \mathbf{H}_{\mathrm{f}}(\mathbf{W}_i,\mathbf{W}_j)
    = \big( \tfrac{\partial \ell}{\partial \mathbf{F}} \otimes \mathbf{I}_{p_i q_i} \big)
    \frac{\partial^2 \mathbf{F}}{\partial \mathbf{W}_i \partial \mathbf{W}_j}.
\]
Для квадратичной функции потерь~$\tfrac{\partial \ell}{\partial \mathbf{F}}=\mathbf{R}_{p_i q_i}$, определенную выше, а следовательно
\[
    \mathbf{H}_{\mathrm{f}}(\mathbf{W}_i,\mathbf{W}_j) = \mathbf{R}_{n_i} \boldsymbol{\Phi}_{ij},
\]
где $\boldsymbol{\Phi}_{ij} = \tfrac{\partial^2 \mathbf{F}}{\partial \mathbf{W}_i \partial \mathbf{W}_j}$.

Явные формы для~$\mathbf{H}_{\mathrm{f}}$ описаны в~\cite[Thm.~3.2]{ormaniec2024attentionhessian}. Используя выражения для~$\mathbf{H}_{\mathrm{f}}$ получаем выражения~$\boldsymbol{\Phi}_{ij}$ просто удаляя ведущей метрицы~$\mathbf{R}_{n_i}$.
\end{proof}

\begin{lemma}\label{lemma:Y_S_norm_bounds}
Пусть заданы матрицы~$\mathbf{X}\in\mathbb{R}^{L\times d_V}$, $\mathbf{Y}=\mathrm{LayerNorm}(\mathbf{F}(\mathbf{X})+\mathbf{X})\in\mathbb{R}^{L\times d_V}$ и задана сеть
\[
    \mathrm{FFN}(\mathbf{Y})=\sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2,\qquad\mathbf{W}_1\in\mathbb{R}^{d_V\times d_{ff}},\quad\mathbf{W}_2\in\mathbb{R}^{d_{ff}\times d_V},
\]
пусть также задана~$\mathbf{S}=\mathbf{Y}+\mathrm{FFN}(\mathbf{Y})\in\mathbb{R}^{L\times d_V}$.
Тогда выполняются следующие оценки спектральных норм:
\begin{align}
    \|\mathbf{Y}\|_2 &\le \|\mathbf{Y}\|_F = \sqrt{Ld_V}, 
    \label{eq:Y_norm_bound}
    \\
    \|\mathrm{FFN}(\mathbf{Y})\|_2 &\le \sqrt{\min(L,d_{ff})} \|\mathbf{Y}\|_2\|\mathbf{W}_1\|_2\|\mathbf{W}_2\|_2,
    \label{eq:FFN_norm_bound}
    \\
    \|\mathbf{S}\|_2 \le \|\mathbf{Y}\|_2 + \|\mathrm{FFN}(\mathbf{Y})\|_2 
    &\le \sqrt{Ld_V}\Big(1+\sqrt{\min(L,d_{ff})}\|\mathbf{W}_1\|_2\|\mathbf{W}_2\|_2\Big).
    \label{eq:S_norm_bound}
\end{align}
\end{lemma}
\begin{proof}
Оценим~$\|\mathbf{Y}\|_2$. Согласно определению LayerNorm в рамках Теоремы~\ref{thm:layernorm_derivative} получаем:
\[
    \mathbf{Y} = \mathbf{P}(\mathbf{S}_0)\mathbf{M}(\mathbf{S}_0), \qquad \mathbf{S}_0:=\mathbf{F}(\mathbf{X})+\mathbf{X},
\]
где~$\mathbf{M}(\mathbf{S}_0)=\mathbf{S}_0-\tfrac{1}{d_V}\mathbf{S}_0\mathbf{1}_{d_V}\mathbf{1}_{d_V}^\top$ и~$\mathbf{P}=\mathrm{diag}^{-1}(\sigma)$ с $\sigma=\tfrac{1}{\sqrt{d_V}}(\mathbf{M}^{\circ 2}\mathbf{1})^{\circ 1/2}$, применяемым построчно.
Для любой строки~$i$ обозначим~$\mathbf{m}_i$ как~$i$-ю строку~$\mathbf{M}$ и~$\sigma_i=\tfrac{1}{\sqrt{d_V}}\|\mathbf{m}_i\|_2$.
Тогда~$i$-я строка~$\mathbf{Y}$ имеет вид~$\mathbf{y}_i=\mathbf{m}_i/\sigma_i$, а следовательно
\[
    \|\mathbf{y}_i\|_2^2 = \frac{\|\mathbf{m}_i\|_2^2}{\sigma_i^2}= \frac{\|\mathbf{m}_i\|_2^2}{(1/d_V)\|\mathbf{m}_i\|_2^2}= d_V.
\]
Таким образом, каждая строка~$\mathbf{Y}$ имеет евклидову норму $\sqrt{d_V}$. Получаем:
\[
    \|\mathbf{Y}\|_F^2 = \sum_{i=1}^L \|\mathbf{y}_i\|_2^2 = Ld_V,\qquad\text{откуда}\qquad\|\mathbf{Y}\|_F = \sqrt{Ld_V}.
\]
Используя из свойства~\ref{prop:matrix_norm_inequalities} неравенство для норм~$\|\mathbf{A}\|_2 \le \|\mathbf{A}\|_F$, получаем~\eqref{eq:Y_norm_bound}.

Оценим~$\|\mathrm{FFN}(\mathbf{Y})\|_2$.
Используя свойство~\ref{prop:matrix_product_norm} получаем следующую оценку:
\[
    \|\mathrm{FFN}(\mathbf{Y})\|_2= \|\text{ReLU}(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2\|_2\le \|\text{ReLU}(\mathbf{Y}\mathbf{W}_1)\|_2 \|\mathbf{W}_2\|_2,
\]
далее, используя свойство~\ref{prop:matrix_norm_inequalities}, получаем
\[
    \|\text{ReLU}(\mathbf{Y}\mathbf{W}_1)\|_2 \le \|\text{ReLU}(\mathbf{Y}\mathbf{W}_1)\|_F,
\]
причем согласно определению~\ref{def:matrix_norms}, норма~$\|\cdot\|_F^2$ представляет собой сумму квадратов.
Поэлементно~$\sigma(\cdot)$ удовлетворяет условию~$0\le \sigma(a)\le |a|$, а следовательно~$\sigma(a)^2 \le a^2$ для каждого элемента~$a \in \mathbb{R}$.
Поэтому получаем:
\[
    \|\sigma(\mathbf{Y}\mathbf{W}_1)\|_F \le \|\mathbf{Y}\mathbf{W}_1\|_F.
\]
Используя неравенство~$\|\cdot\|_F \le \sqrt{d}\|\cdot\|_2$ с $d=\operatorname{rank}(\cdot)$ из свойства~\ref{prop:matrix_norm_inequalities} получаем:
\[
    \|\mathbf{Y}\mathbf{W}_1\|_F \le \sqrt{\operatorname{rank}(\mathbf{Y}\mathbf{W}_1)}\|\mathbf{Y}\mathbf{W}_1\|_2,
\]
так как~$\mathbf{Y}\mathbf{W}_1 \in \mathbb{R}^{L\times d_{ff}}$, $\operatorname{rank}(\mathbf{Y}\mathbf{W}_1)\le \min(L,d_{ff})$. Таким образом получаем используя свойство~\ref{prop:matrix_product_norm}:
\[
    \|\mathbf{Y}\mathbf{W}_1\|_F \le \sqrt{\min(L,d_{ff})}\|\mathbf{Y}\mathbf{W}_1\|_2\le \sqrt{\min(L,d_{ff})}\|\mathbf{Y}\|_2 \|\mathbf{W}_1\|_2
\]
Собирая все вместе получаем:
\[
    \|\mathrm{FFN}(\mathbf{Y})\|_2\le \|\sigma(\mathbf{Y}\mathbf{W}_1)\|_F \|\mathbf{W}_2\|_2\le \sqrt{\min(L,d_{ff})}\|\mathbf{Y}\|_2\|\mathbf{W}_1\|_2\|\mathbf{W}_2\|_2,
\]
что заканчивает оценку~\eqref{eq:FFN_norm_bound}.

Оценим~$\|\mathbf{S}\|_2$.
Используя из свойства~\ref{prop:matrix_sum_norm} неравенство для нормы суммы, получаем:
\[
    \|\mathbf{S}\|_2=\|\mathbf{Y}+\mathrm{FFN}(\mathbf{Y})\|_2\le \|\mathbf{Y}\|_2 + \|\mathrm{FFN}(\mathbf{Y})\|_2,
\]
откуда подставляя~\eqref{eq:Y_norm_bound} и~\eqref{eq:FFN_norm_bound}, получаем~\eqref{eq:S_norm_bound}.
\end{proof}

\begin{lemma}\label{lemma:layernorm_deriv_hessian_norm}
Пусть заданы матрицы~$\mathbf{X} \in \mathbb{R}^{m \times n}$.
Производная LayerNorm $\mathbf{J}_{\mathrm{LN}}(\mathbf{X}) = \frac{\partial \text{LayerNorm}(\mathbf{X})}{\partial \mathbf{X}}$ вычисляется в соответствии с Теоремой~\ref{thm:layernorm_derivative}, а ее гессиан $\mathbf{H}_{\mathrm{LN}}(\mathbf{X}) = \frac{\partial^2 \text{LayerNorm}(\mathbf{X})}{\partial \mathbf{X}^2}$ вычисляется как в Теореме \ref{thm:layernorm_second_derivative}.

Тогда выполняются следующие оценки:
\begin{align}
    \big\|\mathbf{J}_{\mathrm{LN}}(\mathbf{X})\big\|_2
    &\le \frac{1}{\sigma_{\min}} + \frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3}, \label{eq:ln_jac_norm_bound}\\[4pt]
    \big\|\mathbf{H}_{\mathrm{LN}}(\mathbf{X})\big\|_2
    &\le \frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3}\Big(1+\sqrt{\tfrac{m}{n}}\Big)
    + \frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3}
    + \frac{3\|\mathbf{X}\|_2^3}{n\sigma_{\min}^5},
    \label{eq:ln_hess_norm_bound}
\end{align}
где $\sigma_{\min}$ обозначает $\min \limits_i \|\mathbf{M}_i \|_2$, где $\mathbf{M}(\mathbf{X}) = \mathbf{X}(\mathbf{I}_n - \tfrac{1}{n}\mathbf{1}_n\mathbf{1}_n^\top)$
\end{lemma}
\begin{proof}
Согласно Теореме~\ref{thm:layernorm_derivative}:
\[
    \mathbf{J}_{\mathrm{LN}}(\mathbf{X})= (\mathbf{P}\otimes \mathbf{I}_n)\mathbf{G}+ (\mathbf{I}_m\otimes \mathbf{M}^\top)\mathbf{H},
\]
где $\mathbf{G}=\mathbf{I}_{mn}-\tfrac{1}{n}(\mathbf{I}_m\otimes \mathbf{1}_{n\times n})$,
$\mathbf{H}=\tfrac{\partial \mathbf{P}}{\partial \mathbf{X}}$, и $\mathbf{P}=\mathrm{diag}^{-1}(\boldsymbol{\sigma})$.
Используя cвойства~\ref{prop:kronecker_product_norm}, \ref{prop:matrix_product_norm}, \ref{prop:matrix_sum_norm} получаем:
\begin{align}
    \|\mathbf{J}_{\mathrm{LN}}(\mathbf{X})\|_2 &\le \|\mathbf{P}\otimes \mathbf{I}_n\|_2\|\mathbf{G}\|_2 + \|\mathbf{I}_m\otimes \mathbf{M}^\top\|_2\|\mathbf{H}\|_2 =\\
    &=\|\mathbf{P}\|_2\|\mathbf{G}\|_2 + \|\mathbf{M}\|_2\|\mathbf{H}\|_2.
\end{align}
Оценим каждый множитель по отдельности. Множитель~$\|\mathbf{G}\|_2\le 1$, поскольку~$\tfrac{1}{n}\mathbf{1}_{n\times n}$ является проекцией, следовательно~$\|\mathbf{I}_n-\tfrac{1}{n}\mathbf{1}_{n\times n}\|_2\le 1$, и произведение Кронекера сохраняет оценку спектральной нормы согласно свойств~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, а также Леммы~\ref{lemma:1_spectral_norm}. Множитель~$\|\mathbf{P}\|_2 = \|\mathbf{D}^{-1}\|_2 = 1/\sigma_{\min}$, где $\mathbf{D}=\mathrm{diag}(\boldsymbol{\sigma})$. Множитель~$\|\mathbf{M}\|_2 \le \|\mathbf{X}\|_2$, потому что $\mathbf{M}(\mathbf{X}) = \mathbf{X}(\mathbf{I}_n - \tfrac{1}{n}\mathbf{1}_n\mathbf{1}_n^\top)$, и правый множитель является проектором с нормой $\le 1$ согласно свойства~\ref{prop:matrix_product_norm}. Для $\|\mathbf{H}\|_2=\big\|\tfrac{\partial \mathbf{P}}{\partial \mathbf{X}}\big\|_2$, Теорема~\ref{thm:layernorm_derivative} вместе с Леммами~\ref{lemma:invert_derivative}, \ref{lemma:diag_derivative}, \ref{lemma:hadamard_square_derivative}, \ref{lemma:hadamard_root_derivative} и свойствами~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm} дают оценку:
\begin{align}
    \Big\|\frac{\partial \mathbf{P}}{\partial \mathbf{X}}\Big\|_2 &\le \frac{1}{\sqrt{n}}\|\mathbf{D}^{-1}\otimes \mathbf{D}^{-\top}\|_2\Big\|\mathrm{diag}^{-1}\!\big(\mathrm{vec}_r^{\circ 1/2}(\mathbf{M}^{\circ 2}\mathbf{1}_n)\big)\Big\|_2 \cdot\\
    &\quad\cdot\|\mathbf{I}_m\otimes \mathbf{1}_n^\top\|_2\|\mathrm{diag}(\mathrm{vec}_r(\mathbf{M}))\|_2\Big\|\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\Big\|_2.
\end{align}
Используя свойство~\ref{prop:matrix_norm_inequalities} получаем оценки:
\begin{align}
    \|\mathbf{D}^{-1}\otimes \mathbf{D}^{-\top}\|_2 &=\|\mathbf{D}^{-1}\|_2^2=\frac{1}{\sigma_{\min}^{2}},\\
    \big\|\mathrm{diag}^{-1}(\cdot)\big\|_2 &= \frac{1}{\min_i\sqrt{\sum_{v} M_{i,v}^2}} = \frac{1}{\sqrt{n}\sigma_{\min}},\\
    \|\mathbf{I}_m\otimes \mathbf{1}^\top\|_2 &=\sqrt{n},\\
    \|\mathrm{diag}(\mathrm{vec}_r(\mathbf{M}))\|_2 &=\|\mathbf{M}\|_{\max}\le \|\mathbf{M}\|_2,\\
    \big\|\tfrac{\partial \mathbf{M}}{\partial \mathbf{X}}\big\|_2 &\le 1,
\end{align}
откуда получаем оценку:
\[
    \|\mathbf{H}\|_2 \le \frac{1}{\sqrt{n}\sigma_{\min}^{2}}\cdot \frac{1}{\sqrt{n}\sigma_{\min}}\cdot \sqrt{n}\cdot \|\mathbf{M}\|_2\cdot 1\le \frac{\|\mathbf{X}\|_2}{\sqrt{n}\sigma_{\min}^3}.
\]
Собирая все полученные оценки, получаем \eqref{eq:ln_jac_norm_bound}:
\[
    \|\mathbf{J}_{\mathrm{LN}}(\mathbf{X})\|_2
    \le \frac{1}{\sigma_{\min}}\cdot 1 + \|\mathbf{X}\|_2\cdot \frac{\|\mathbf{X}\|_2}{\sqrt{n}\sigma_{\min}^3}
    = \frac{1}{\sigma_{\min}} + \frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3}.
\]

Из Теоремы~\ref{thm:layernorm_second_derivative} (с $m,n$), используя $\tfrac{\partial^2 \mathbf{M}}{\partial \mathbf{X}^2}=0$,
\[
    \mathbf{H}_{\mathrm{LN}}(\mathbf{X})=(\mathbf{I}_{mn}\otimes \mathbf{G}^\top)\frac{\partial (\mathbf{P}\otimes \mathbf{I}_n)}{\partial \mathbf{X}}+ \big( (\mathbf{I}_m\otimes \mathbf{M}^\top)\otimes \mathbf{I}_{mn}\big)\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}+ (\mathbf{I}_{mn}\otimes \mathbf{H}^\top)\frac{\partial (\mathbf{I}_m\otimes \mathbf{M}^\top)}{\partial \mathbf{X}}.
\]
Оценим три слагаемых отдельно с помощью свойств~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}.
Первое слагаемое. По Предложению~\ref{prop:kronecker_product_derivative} получаем:
\[
    \frac{\partial (\mathbf{P}\otimes \mathbf{I}_n)}{\partial \mathbf{X}}=(\mathbf{I}_m \otimes \mathbf{K}_{n,m} \otimes \mathbf{I}_n)(\mathbf{I}_{m^2} \otimes \mathrm{vec}_r(\mathbf{I}_n))\frac{\partial \mathbf{P}}{\partial \mathbf{X}},
\]
а следовательно
\begin{align}
    \Big\|(\mathbf{I}_{mn}\otimes \mathbf{G}^\top)\frac{\partial (\mathbf{P}\otimes \mathbf{I}_n)}{\partial \mathbf{X}}\Big\|_2 &\le \|\mathbf{G}\|_2 \|\mathbf{I}_{m^2} \otimes \mathrm{vec}_r(\mathbf{I}_n)\|_2 \Big\|\frac{\partial \mathbf{P}}{\partial \mathbf{X}}\Big\|_2 =\\
    &=1 \cdot \sqrt{n} \cdot \frac{\|\mathbf{X}\|_2}{\sqrt{n}\sigma_{\min}^3}= \frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3}.
\end{align}
Второе слагаемое. Используя $\|\mathbf{I}_m\otimes \mathbf{M}^\top\|_2=\|\mathbf{M}\|_2\le \|\mathbf{X}\|_2$ и оценку для $\big\|\tfrac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\big\|_2$ получаем:
\[
    \Big\|\big( (\mathbf{I}_m\otimes \mathbf{M}^\top)\otimes \mathbf{I}_{mn}\big)\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\Big\|_2\le \|\mathbf{X}\|_2 \Big\|\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\Big\|_2.
\]
Далее оценим~$\big\|\tfrac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\big\|_2$, следуя той же цепочке, что и в доказательстве Теоремы~\ref{thm:layernorm_second_derivative}
запишем~$\tfrac{\partial \mathbf{P}}{\partial \mathbf{X}}=\tfrac{1}{\sqrt{n}}\mathbf{A}_1(\mathbf{X})\mathbf{E}\mathbf{B}_1(\mathbf{X})$ и продифференцируем, используя свойство~\ref{prop:matrix_product_norm} с Леммами~\ref{lemma:invert_derivative}, \ref{lemma:diag_derivative}, \ref{lemma:hadamard_square_derivative}, \ref{lemma:hadamard_root_derivative} и свойствами~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, \ref{prop:matrix_norm_inequalities}. Получаем:
\[
    \Big\|\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\Big\|_2\le\frac{1}{\sqrt{n}\sigma_{\min}^3}\|\mathbf{X}\|_2+ \frac{3}{n\sigma_{\min}^5}\|\mathbf{X}\|_2^2,
\]
а следовательно,
\[
    \Big\|\big( (\mathbf{I}_m\otimes \mathbf{M}^\top)\otimes \mathbf{I}_{mn}\big)\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\Big\|_2\le \frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3}+ \frac{3\|\mathbf{X}\|_2^3}{n\sigma_{\min}^5}.
\]

Третье слагаемое. По свойству~\ref{prop:kronecker_product_derivative} и Лемме~\ref{lemma:transposed_matrix_derivative} получаем:
\[
    \frac{\partial (\mathbf{I}_m\otimes \mathbf{M}^\top)}{\partial \mathbf{X}}=(\mathbf{I}_m \otimes \mathbf{K}_{n,m} \otimes \mathbf{I}_m)(\mathrm{vec}_r(\mathbf{I}_m)\otimes \mathbf{I}_{mn})\frac{\partial \mathbf{M}}{\partial \mathbf{X}},
\]
откуда получаем:
\begin{align}
    \Big\|(\mathbf{I}_{mn}\otimes \mathbf{H}^\top)\frac{\partial (\mathbf{I}_m\otimes \mathbf{M}^\top)}{\partial \mathbf{X}}\Big\|_2 &\le \|\mathbf{H}\|_2 \|\mathrm{vec}_r(\mathbf{I}_m)\otimes \mathbf{I}_{mn}\|_2 \Big\|\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\Big\|_2 =\\
    &=\frac{\|\mathbf{X}\|_2}{\sqrt{n}\sigma_{\min}^3}\cdot \sqrt{m}\cdot 1= \frac{\sqrt{m}}{\sqrt{n}}\frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3}.
\end{align}

Суммируя все слагаемые с помощью свойства~\ref{prop:matrix_sum_norm}  получаем~\eqref{eq:ln_hess_norm_bound}:
\begin{align}
    \|\mathbf{H}_{\mathrm{LN}}(\mathbf{X})\|_2 &\le \frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3} + \Big(\frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3} + \frac{3\|\mathbf{X}\|_2^3}{n\sigma_{\min}^5}\Big) + \frac{\sqrt{m}}{\sqrt{n}}\frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3} = \\
    &= \frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3}\Big(1+\sqrt{\tfrac{m}{n}}\Big) + \frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3} + \frac{3\|\mathbf{X}\|_2^3}{n\sigma_{\min}^5}.
\end{align}
\end{proof}

\begin{theorem}\label{thm:transformer_derivative}
Для модели глубокого обучения архитектуры трансформер~\ref{eq:transformer} матрица Якоби~$\frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i}$ вычисляется в следующем виде:
\begin{align}
    \frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i} &= \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{W}_i},\qquad i \in \{1,2\},
\end{align}
где
\begin{equation}
    \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{W}_i} = \begin{cases}
        \left(\mathbf{I}_L \otimes \mathbf{W}_2^\top \right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{Y} \otimes \mathbf{I}_{d_{ff}}\right), & \text{for } i = 1 \\
        \sigma(\mathbf{Y} \mathbf{W}_1) \otimes \mathbf{I}_{d_V}, & \text{for } i = 2
    \end{cases},
\end{equation}
причем~$\frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}$ вычисляется согласно Теоремы~\ref{thm:layernorm_derivative}.
\begin{align}
    \frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i} &= \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{Y}} \frac{\partial \mathbf{Y}}{\partial \mathbf{W}_i}, \qquad i \in \{ K, Q, V\},
\end{align}
где
\begin{align}
    \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{Y}} &= \left( \mathbf{I}_L \otimes \mathbf{W}_2^\top\right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{I}_L \otimes \mathbf{W}_1^\top \right) + \left( \mathbf{I}_L \otimes \mathbf{I}_{d_V}\right),
\end{align}
причем~$\frac{\partial \mathbf{Y}}{\partial\mathbf{W}_i} = \frac{\partial \text{LayerNorm}(\mathbf{F} (\mathbf{X}) + \mathbf{X})}{\partial (\mathbf{F} (\mathbf{X}) + \mathbf{X})} \frac{\partial \mathbf{F}(\mathbf{X})}{\partial \mathbf{W}_i},$ где $\frac{\partial \mathbf{F}(\mathbf{X})}{\partial \mathbf{W}_i}$ вычисляется при помощи Леммы~A.2 в работе~\cite{noci2022signalpropagationtransformerstheoretical}, а матрица~$\frac{\partial \text{LayerNorm}(\mathbf{F} (\mathbf{X}) + \mathbf{X})}{\partial (\mathbf{F} (\mathbf{X}) + \mathbf{X})}$ вычисляется согласно Теоремы~\ref{thm:layernorm_derivative}.
\end{theorem}
\begin{proof}
В дальнейшем при доказательстве вводим следующие обозначения и предположения~$\mathbf{X} \in R^{L \times d_V}, \mathbf{Y} \in R^{L\times d_V}, \mathbf{W}_1 \in R^{d_V \times d_{ff}}, \text{ReLU}(\mathbf{Y\mathbf{W}_1}) \in R^{L \times d_{ff}}, \mathbf{W}_2 \in R^{d_{ff} \times d_V}.$
Трансформер блок определен в выражении~\eqref{eq:transformer}, а именно:
\begin{align}
    \mathbf{Y} &= \text{LayerNorm}(\mathbf{F}(\mathbf{X}) + \mathbf{X}),\\
    \mathbf{Z} &= \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y}).
\end{align}

Начнем вычисления матрицы Гессе для полного трансформера с матриц~$\frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i},$ тогда для~$i \in \{1,2\}$ получаем:
\begin{equation}
    \frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i} = \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{W}_i},
\end{equation}
где
\begin{equation}
    \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{W}_i} = \frac{\partial (\text{FFN}(\mathbf{Y}))}{\partial \mathbf{W}_i} = \frac{\partial \mathbf{I}_L \sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2 \mathbf{I}_{d_V}}{\partial \mathbf{W}_i},
\end{equation}
причем используя свойство~\ref{prop:matrix_product_derivative} об производной произведения матриц получаем:
\begin{align}
    \frac{\partial \mathbf{I}_L \sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2 \mathbf{I}_{d_V}}{\partial \mathbf{W}_2} &= \sigma(\mathbf{Y} \mathbf{W}_1) \otimes \mathbf{I}_{d_V}\\
    \frac{\partial \mathbf{I}_L \sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2 \mathbf{I}_{d_V}}{\partial \mathbf{W}_1} &= \frac{\partial \sigma(\mathbf{Y}\mathbf{W}_1) \mathbf{W}_2}{\partial \sigma(\mathbf{Y}\mathbf{W}_1)} \frac{\partial \sigma(\mathbf{Y}\mathbf{W}_1)}{\partial \mathbf{Y}\mathbf{W}_1} \frac{\partial \mathbf{Y}\mathbf{W}_1}{\partial \mathbf{W}_1} =\\
    &= \left(\mathbf{I}_L \otimes \mathbf{W}_2^\top \right) \frac{\partial \sigma(\mathbf{Y}\mathbf{W}_1)}{\partial \mathbf{Y}\mathbf{W}_1} \left( \mathbf{I}_L \otimes \mathbf{W}_1^\top\right).
\end{align}
Используя результаты Теоремы~\ref{theorem:relu_derivative_hessian} для производной оператора ReLU для матрицы~$\frac{\partial \sigma(\mathbf{Y}\mathbf{W}_1)}{\partial \mathbf{Y}\mathbf{W}_1}$ получаем следующее выражение:
\begin{align}
    \frac{\partial \mathbf{I}_L \sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2 \mathbf{I}_{d_V}}{\partial \mathbf{W}_i} = \left(\mathbf{I}_L \otimes \mathbf{W}_2^\top \right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{Y} \otimes \mathbf{I}_{d_{ff}}\right).
\end{align}
Тогда в общем виде для~$i \in \{1, 2\}$ получаем следующее выражение: 
\begin{equation}
    \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{W}_i} = \begin{cases}
        \left(\mathbf{I}_L \otimes \mathbf{W}_2^\top \right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{Y} \otimes \mathbf{I}_{d_{ff}}\right), \text{если}~i = 1 \\
        \sigma(\mathbf{Y} \mathbf{W}_1) \otimes \mathbf{I}_{d_V}, \text{если}~i = 2
    \end{cases}.
\end{equation}
Тогда весь блок тронсформера имеет следующую производную:
\begin{equation}
    \frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i} = 
    \begin{cases}
        \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}\left(\mathbf{I}_L \otimes \mathbf{W}_2^\top \right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{Y} \otimes \mathbf{I}_{d_{ff}}\right), i = 1 \\
        \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}\sigma(\mathbf{Y} \mathbf{W}_1) \otimes \mathbf{I}_{d_V}, i = 2
    \end{cases}
\end{equation}
причем, согласно Теореме~\ref{thm:layernorm_derivative} об производной LayerNorm получаем следующее выражение в нашем случае:
\begin{align}
    &\frac{\partial\text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} =\\
    &=( \mathbf{P}(\text{FFN}(\mathbf{Y}) + \mathbf{Y}) \otimes \mathbf{I}_{d_V}) \frac{\partial \mathbf{M}}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} +\\
    &\quad+ (\mathbf{I}_L\otimes \mathbf{M}^\top)\frac{1}{\sqrt{d_V}}\left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right) \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big) \cdot\\
    &\qquad\cdot \left(\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\cdot (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}\right),
\end{align}
где
\begin{align}
    \mathbf{M}(\text{FFN}(\mathbf{Y}) + \mathbf{Y}) &= ((\text{FFN}(\mathbf{Y}) + \mathbf{Y}) - \frac{1}{d_V}(\text{FFN}(\mathbf{Y}) + \mathbf{Y}) \mathbf{1}_{d_V \times d_V}),\\
    \mathbf{P}((\text{FFN}(\mathbf{Y}) + \mathbf{Y})) &= \textit{diag}^{-1}(\sigma(\text{FFN}(\mathbf{Y}) + \mathbf{Y}),\\
    \frac{\partial \mathbf{M}}{\partial(\text{FFN}(\mathbf{Y}) + \mathbf{Y})} &= (\mathbf{I}_L \otimes \mathbf{I}_{d_V}) - \frac{1}{d_V}(\mathbf{I}_L \otimes \mathbf{1}_{d_V \times d_V}),
\end{align}
где~$\sigma$ вычисляется согласно определению оператору LayerNorm.

Далее, перейдем к вычислению матриц Гессе~$\frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i}$ для~$i \in \{ K, Q, V\},$ где получаем:
\begin{align}
    \frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i} = \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{Y}} \frac{\partial \mathbf{Y}}{\partial \mathbf{W}_i},
\end{align}
где используя свойство~\ref{prop:matrix_product_derivative} и результат Теоремы~\ref{theorem:relu_derivative_hessian} получаем:
\begin{align}
    \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{Y}} &= \frac{\partial \text{FFN}(\mathbf{Y}) }{\partial \mathbf{Y}} + \frac{\partial \mathbf{Y}}{\partial \mathbf{Y}} =\\
    &= \frac{\partial \text{FFN}(\mathbf{Y}) }{\partial \mathbf{Y}} + \left( \mathbf{I}_L \otimes \mathbf{I}_{d_V}\right) =\\
    &=\frac{\partial \sigma(\mathbf{Y} \mathbf{W}_1) \mathbf{W}_2}{\partial \mathbf{Y}} +\left( \mathbf{I}_L \otimes \mathbf{I}_{d_V}\right)=\\ 
    &= \left( \mathbf{I}_L \otimes \mathbf{W}_2^\top\right) \frac{\partial \sigma (\mathbf{Y} \mathbf{W}_1)}{\partial \mathbf{Y}\mathbf{W}_1} \frac{\partial \mathbf{Y}\mathbf{W}_1}{\partial \mathbf{Y}} + \left( \mathbf{I}_L \otimes \mathbf{I}_{d_V}\right) = \\
    & = \left( \mathbf{I}_L \otimes \mathbf{W}_2^\top\right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{I}_L \otimes \mathbf{W}_1^\top \right) + \left( \mathbf{I}_L \otimes \mathbf{I}_{d_V}\right).
\end{align}
Для вычисления матрицы~$\frac{\partial \mathbf{Y}}{\partial \mathbf{W}_i}$ используем результат Леммы~A.2 с работы~\cite{noci2022signalpropagationtransformerstheoretical}:
\begin{align}
    \frac{\partial \mathbf{F}}{\partial \mathbf{W}_V} &= \text{softmax}\left(\frac{\mathbf{X}\mathbf{W}_Q\mathbf{W}_{K}^{\top}\mathbf{X}^\top}{\sqrt{d_K}}\right) \mathbf{X} \otimes \mathbf{I}_{d_V}\\
    \frac{\partial \mathbf{F}}{\partial \mathbf{W}_Q} &= \left(\mathbf{I}_L \otimes \mathbf{W}_{V}^{\top}\mathbf{X}^\top\right) \frac{\partial \mathbf{A}}{\partial \mathbf{M}} \left(\frac{\mathbf{X} \otimes \mathbf{X}\mathbf{W}_K}{\sqrt{d_K}}\right),
\end{align}
где
\begin{equation}
    \frac{\partial \mathbf{A}}{\partial \mathbf{M}} = \text{blockdiag}\left(\frac{\partial \mathbf{A}_i}{\partial \mathbf{M}_i^\top}\right),
\end{equation}
причем данное выражение сильно упрощается используя свойства матрицы~$\mathbf{A}$:
\begin{align}
    \frac{\partial \mathbf{A}_i}{\partial \mathbf{M}_i^\top} = \text{diag}(\mathbf{A}_i) - \mathbf{A}_i\mathbf{A}_i^\top,
\end{align}
где~$\mathbf{A}_i$ является $i$-й строкой матрицы~$\mathbf{A}$ в формате вектора. Итого в условия равномерного внимания (англ. uniform-attention) данное выражение упрощается до:
\begin{equation}
    \frac{\partial \mathbf{A}}{\partial \mathbf{M}} = \frac{1}{n}\mathbf{I}_L \otimes \left(\mathbf{I}_L - \frac{1}{L}\mathbf{1}_{L \times L}\right)
\end{equation}
Аналогично, используя Лемму~\ref{lemma:transposed_matrix_derivative} вычисляем производну относительно матрицы~$\mathbf{W}_K$:  
\begin{align}
    \frac{\partial \mathbf{F}}{\partial \mathbf{W}_K} &= \left(\mathbf{I}_L \otimes \mathbf{W}_{V}^{\top}\mathbf{X}^\top\right) \frac{\partial \mathbf{A}}{\partial \mathbf{M}} \left(\frac{(\mathbf{X} \mathbf{W}_Q \otimes \mathbf{X})\mathbf{K}_{d_V d_K}}{\sqrt{d_k}}\right).
\end{align}
Получаем, что матрица~$\frac{\partial \mathbf{Y}}{\partial\mathbf{W}_i}$ для~$i \in \{ K, Q, V\}$ вычисляется следующим образом:
\begin{align}
    \frac{\partial\mathbf{Y}}{\partial\mathbf{W}_i} &= \frac{\partial \text{LayerNorm}(\mathbf{F} (\mathbf{X}) + \mathbf{X})}{\partial \mathbf{W}_i} =\\
    &=\frac{\partial\text{LayerNorm}(\mathbf{F} (\mathbf{X}) + \mathbf{X})}{\partial (\mathbf{F} (\mathbf{X}) + \mathbf{X})} \frac{\partial \mathbf{F}(\mathbf{X})}{\partial \mathbf{W}_i},
\end{align}
где~$\frac{\partial \mathbf{F}(\mathbf{X})}{\partial \mathbf{W}_i}$ вычисляется согласно Леммы~A.2 с работы~\cite{noci2022signalpropagationtransformerstheoretical}, а матрица~$\frac{\partial \text{LayerNorm}(\mathbf{F} (\mathbf{X}) + \mathbf{X})}{\partial (\mathbf{F} (\mathbf{X}) + \mathbf{X})}$ вычисляется согласно Теоремы~\ref{thm:layernorm_derivative}.
\end{proof}

В Теореме~\ref{thm:transformer_derivative} получен вид матрицы Якоби для полного блока трансформера, теперь можно перейти к вычислению матрицы Гессе для, который получен в виде Теоремы~\ref{thm:transformer_hessian}.

\begin{theorem}\label{thm:transformer_hessian}
Пусть заданы матрицы параметров модели трансформера~$\mathbf{X} \in \mathbb{R}^{L \times d_V}$, $\mathbf{Y} \in \mathbb{R}^{L \times d_V}$, $\mathbf{W}_1 \in \mathbb{R}^{d_V \times d_{ff}}$, $\mathbf{W}_2 \in \mathbb{R}^{d_{ff} \times d_V}$, $\mathbf{W}_Q, \mathbf{W}_K \in \mathbb{R}^{d_V \times d_K}$, $\mathbf{W}_V \in \mathbb{R}^{d_V \times d_V},$
где блок трансформатор описан в виде следующих матричнозначных функций:
\[
    \mathbf{S}(\mathbf{Y},\mathbf{W}_1,\mathbf{W}_2) = \sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2 + \mathbf{Y} \in \mathbb{R}^{L \times d_V}, \qquad\mathbf{Z} = \mathrm{LayerNorm}(\mathbf{S}) \in \mathbb{R}^{L \times d_V},
\]
для которых в условиях Теорем~\ref{thm:layernorm_derivative} и \ref{thm:layernorm_second_derivative} вычислимые матрицы Якобы и Гессе вида:
\[
    \mathbf{J}_Z := \frac{\partial\mathrm{LayerNorm}(\mathbf{S})}{\partial \mathbf{S}} \in \mathbb{R}^{L d_V \times L d_V}, \quad\mathbf{H}_Z := \frac{\partial^2\mathrm{LayerNorm}(\mathbf{S})}{\partial \mathbf{S}^2} \in \mathbb{R}^{(L d_V)^2 \times L d_V}.
\]
Также в условиях Теорем~\ref{thm:layernorm_derivative}, \ref{thm:layernorm_second_derivative} и \ref{theorem:relu_derivative_hessian} введем следующее:
\begin{align}
    \mathbf{D}_\sigma := \mathrm{diag}\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{Y}\mathbf{W}_1>0\}})\big) \in \mathbb{R}^{L d_{ff} \times L d_{ff}},\\
    \mathbf{J}_{SY} := \frac{\partial \mathbf{S}}{\partial \mathbf{Y}} = (\mathbf{I}_L \otimes \mathbf{W}_2^\top)\mathbf{D}_\sigma (\mathbf{I}_L \otimes \mathbf{W}_1^\top) + (\mathbf{I}_L \otimes \mathbf{I}_{d_V}) \in \mathbb{R}^{L d_V \times L d_V},
\end{align}
где для матрицы~$\mathbf{Y} = \mathrm{LayerNorm}(\mathbf{F}(\mathbf{X}) + \mathbf{X})$ в условиях Теорем~~\ref{thm:layernorm_derivative}, \ref{thm:layernorm_second_derivative} определено:
\begin{align}
    \mathbf{J}_Y &:= \frac{\partial \mathrm{LayerNorm}(\mathbf{F}(\mathbf{X})+\mathbf{X})}{\partial (\mathbf{F}(\mathbf{X})+\mathbf{X})} \in \mathbb{R}^{L d_V \times L d_V}, \\
    \mathbf{H}_Y &:= \frac{\partial^2 \mathrm{LayerNorm}(\mathbf{F}(\mathbf{X})+\mathbf{X})}{\partial (\mathbf{F}(\mathbf{X})+\mathbf{X})^2} \in \mathbb{R}^{(L d_V)^2 \times L d_V},
\end{align}
где для удобства введем следующие обозначения:~ $n_1 = d_V d_{ff}, n_2 = d_{ff} d_V, n_Q = n_K = d_V d_K, n_V = d_V^2.$
Пусть матрицы Якоби вычислимы в условиях Теоремы~\ref{thm:transformer_derivative} в следующем виде:
\begin{align}
     \mathbf{G}_V &:= \frac{\partial \mathbf{F}}{\partial \mathbf{W}_V} \in \mathbb{R}^{L d_V \times n_V},\\
     \mathbf{G}_Q &:= \frac{\partial \mathbf{F}}{\partial \mathbf{W}_Q} \in \mathbb{R}^{L d_V \times n_Q},\\
     \mathbf{G}_K &:= \frac{\partial \mathbf{F}}{\partial \mathbf{W}_K} \in \mathbb{R}^{L d_V \times n_K},\\
     \mathbf{B}_1 &:= \frac{\partial \mathbf{S}}{\partial \mathbf{W}_1} = (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \in \mathbb{R}^{L d_V \times n_1},\\
     \mathbf{B}_2 &:= \frac{\partial \mathbf{S}}{\partial \mathbf{W}_2} = \sigma(\mathbf{Y}\mathbf{W}_1) \otimes \mathbf{I}_{d_V} \in \mathbb{R}^{L d_V \times n_2},\\
     \mathbf{B}_k &:= \frac{\partial \mathbf{S}}{\partial \mathbf{W}_k} = \mathbf{J}_{SY} \mathbf{J}_Y \mathbf{G}_k \in \mathbb{R}^{L d_V \times n_k}, \quad k \in \{K,Q,V\}.
\end{align}

Тогда матрицы Гессе трансформера~$\mathbf{Z}$ по параметрам модели~$(\mathbf{W}_i,\mathbf{W}_j)$ задается в виде:
\begin{equation}\label{eq:block_hessian_transformer}
    \;\mathbf{H}_{\mathrm{tr}}^{(i,j)} := \frac{\partial^2 \mathbf{Z}}{\partial \mathbf{W}_i \partial \mathbf{W}_j}
    = \left( \mathbf{J}_Z \otimes \mathbf{I}_{n_i} \right) \boldsymbol{\xi}_{ij}
      + \left( \mathbf{I}_{L d_V} \otimes \mathbf{B}_i^\top \right) \mathbf{H}_Z \mathbf{B}_j,
\end{equation}
где размерность матрицы Гессе~$\mathbf{H}_{\mathrm{tr}}^{(i,j)} \in \mathbb{R}^{(L d_V \cdot n_i) \times n_j},$ также введены дополнительные матрицы для удобства:
\[
    \boldsymbol{\xi}_{ij} := \frac{\partial}{\partial \mathbf{W}_j} \left( \frac{\partial \mathbf{S}}{\partial \mathbf{W}_i} \right) \in \mathbb{R}^{(L d_V \cdot n_i) \times n_j}.
\]
Матрицы $\boldsymbol{\xi}_{ij}$ вычисляются для всех пар~$(i,j)$ почти всюду.

Для пар FFN:
\begin{align}
    \boldsymbol{\xi}_{11} &= \mathbf{0}_{(L d_V \cdot n_1) \times n_1}, \\
    \boldsymbol{\xi}_{22} &= \mathbf{0}_{(L d_V \cdot n_2) \times n_2}, \\
    \boldsymbol{\xi}_{12} &= \left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V} \right) \left( \mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V}) \right)\left( \mathbf{D}_\sigma  (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \right),\\
    \boldsymbol{\xi}_{21} &= \left( \mathbf{I}_{L d_V} \otimes \left( (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})^\top \mathbf{D}_\sigma^\top \right) \right)\left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, L} \otimes \mathbf{I}_{d_{ff}} \right) \left( \mathrm{vec}_r(\mathbf{I}_L) \otimes \mathbf{I}_{d_V d_{ff}} \right) \mathbf{K}_{d_{ff}, d_V},
\end{align}
где матрицы~$\boldsymbol{\xi}_{12},\boldsymbol{\xi}_{21}$ имеют размерности $(L d_V \cdot n_1) \times n_2$ и $(L d_V \cdot n_2) \times n_1$ соответственно.

Для пар FFN с параметрами слоев внимания для всех~$k \in \{K,Q,V\}$:
\begin{align}
    \boldsymbol{\xi}_{1k} &= \left( (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma \otimes \mathbf{I}_{n_k}\right)\left( \mathbf{I}_L \otimes \mathbf{K}_{d_{ff}, d_V} \otimes \mathbf{I}_{d_{ff}} \right)\left( \mathbf{I}_{L d_V} \otimes \mathrm{vec}_r(\mathbf{I}_{d_{ff}}) \right) \left( \mathbf{J}_Y \mathbf{G}_k \right),\\
    \boldsymbol{\xi}_{2k} &= \left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V} \right)\left( \mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V}) \right)\left( \mathbf{D}_\sigma (\mathbf{I}_L \otimes \mathbf{W}_1^\top)  \mathbf{J}_Y \mathbf{G}_k \right),
\end{align}
где размерности матрицы~$\boldsymbol{\xi}_{1k} \in \mathbb{R}^{(L d_V \cdot n_1) \times n_k}$ и матрицы~$\boldsymbol{\xi}_{2k} \in \mathbb{R}^{(L d_V \cdot n_2) \times n_k}$.

Для пар слоев внимания~$k,\ell \in \{K,Q,V\}$:
\begin{align}
    \boldsymbol{\xi}_{k\ell} 
    = \left( \mathbf{J}_{SY} \otimes \mathbf{I}_{n_k} \right)
    \left[\left( \mathbf{I}_{L d_V} \otimes \mathbf{G}_k^\top \right) \left( \mathbf{H}_Y \mathbf{G}_\ell \right)+ \left( \mathbf{J}_Y \otimes \mathbf{I}_{n_k} \right) \boldsymbol{\Phi}_{k\ell}\right],
\end{align}
где~$\boldsymbol{\Phi}_{k\ell} := \frac{\partial \mathbf{G}_k}{\partial \mathbf{W}_\ell} \in \mathbb{R}^{(L d_V \cdot n_k) \times n_\ell}$ является второй производной слоя внимания~$\mathbf{F}$ по ее параметрам, которые вычислены в рамках Леммы~\ref{lemma:attention_phi_from_functional_hessian}. Все матрицы имеют следующие размерности~$\boldsymbol{\xi}_{k\ell} \in \mathbb{R}^{(L d_V \cdot n_k) \times n_\ell}$.

Также матрица Гессе удовлетворяет следующим свойствам почти везде:
\[
    \mathbf{H}_{\mathrm{tr}}^{(i,j)} = \mathbf{H}_{\mathrm{tr}}^{(j,i)},
\]
так как, во первых единственные нелинейности с потенциально ненулевым вторым дифференциалом является оператор LayerNorm, для которого получены матрицы~$\mathbf{H}_Z,\mathbf{H}_Y$ в рамках Теоремы~\ref{thm:layernorm_second_derivative} и которые являются симметричными по построению и оператор ReLU, для которого матрица Гессе является нулевой согласно Теоремы~\ref{theorem:relu_derivative_hessian}, а во вторых все другие отображения являются линейными, а следовательно согласно Леммы~\ref{lemma:matrix_funcs_product_derivative} и свойства производной произведения Кронекера их частные производные являются коммутативными почти всюду.
\end{theorem}
\begin{proof}
Вычислим производной матрицы Якоби с Теоремы~\ref{thm:transformer_derivative} используя Лемму~\ref{lemma:matrix_funcs_product_derivative} об производной матричного умножения, также свойство производной произведения Кронекера~\ref{prop:kronecker_product_derivative}, также Лемму~\ref{lemma:transposed_matrix_derivative} об производной транспонированной матрицы, Лемму~\ref{lemma:identification_theorem_vec_r} и Теорему~\ref{theorem:relu_derivative_hessian}. Для удобства доказательства разделим его на 4 шага.

На 1-м шаге для всех $i \in \{1,2,K,Q,V\}$ получаем:
\begin{align}
    \frac{\partial \mathbf{Z}}{\partial \mathbf{W}_i} \;=\; \mathbf{J}_Z  \mathbf{B}_i,\qquad\mathbf{J}_Z \in \mathbb{R}^{L d_V \times L d_V},
\end{align}
где $\mathbf{B}_i := \frac{\partial \mathbf{S}}{\partial \mathbf{W}_i}$ задается следующим образом:
\begin{align}
    \mathbf{B}_1 &= (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \in \mathbb{R}^{L d_V \times n_1}, \\
    \mathbf{B}_2 &= \sigma(\mathbf{Y}\mathbf{W}_1) \otimes \mathbf{I}_{d_V} \in \mathbb{R}^{L d_V \times n_2},\\
    \mathbf{B}_k &= \mathbf{J}_{SY} \mathbf{J}_Y \mathbf{G}_k \in \mathbb{R}^{L d_V \times n_k},\qquad k \in \{K,Q,V\},
\end{align}
где матрица~$\mathbf{J}_{SY}$ вычисляется следующим образом:
\begin{align}
    \mathbf{J}_{SY} = \frac{\partial \mathbf{S}}{\partial \mathbf{Y}} = (\mathbf{I}_L \otimes \mathbf{W}_2^\top)\mathbf{D}_\sigma(\mathbf{I}_L \otimes \mathbf{W}_1^\top) + (\mathbf{I}_L \otimes \mathbf{I}_{d_V}) \in \mathbb{R}^{L d_V \times L d_V},
\end{align}
матрица имеет следующую размерность~$\mathbf{J}_Y \in \mathbb{R}^{L d_V \times L d_V},$ а матрица~$\mathbf{G}_k$ описана в Теореме~\ref{thm:transformer_derivative}.
Используя Лемму~\ref{lemma:matrix_funcs_product_derivative} и Теорему~\ref{thm:layernorm_second_derivative} получаем выражение для блока матрицы Гессе:
\begin{align}
     \frac{\partial^2 \mathbf{Z}}{\partial \mathbf{W}_i \partial \mathbf{W}_j} &= \left( \mathbf{J}_Z \otimes \mathbf{I}_{n_i} \right) \boldsymbol{\xi}_{ij}  + \left( \mathbf{I}_{L d_V} \otimes \mathbf{B}_i^\top \right) \mathbf{H}_Z \mathbf{B}_j,\\
     \boldsymbol{\xi}_{ij} &:= \frac{\partial \mathbf{B}_i}{\partial \mathbf{W}_j} \in \mathbb{R}^{(L d_V \cdot n_i) \times n_j}.
\end{align}


На 2-м шаге вычисляем размерности и вид матриц ~$\mathbf{B}_i.$ Используя результаты Теорем~\ref{thm:transformer_derivative} и~\ref{theorem:relu_derivative_hessian} получаем следующие выражения:
\begin{align}
    \mathbf{B}_1 &= (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \in \mathbb{R}^{L d_V \times n_1}, \\
    \mathbf{B}_2 &= \sigma(\mathbf{Y}\mathbf{W}_1) \otimes \mathbf{I}_{d_V} \in \mathbb{R}^{L d_V \times n_2},
\end{align}
где матрица~$\mathbf{D}_\sigma \in \mathbb{R}^{L d_{ff} \times L d_{ff}}$, матрица~$(\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \in \mathbb{R}^{L d_{ff} \times d_V d_{ff}}$.
Тогда для всех матриц~$\mathbf{B}_k,$ где~$k \in \{K,Q,V\}$ получаем:
\begin{align}
    \mathbf{B}_k = \mathbf{J}_{SY} \mathbf{J}_Y  \mathbf{G}_k \in \mathbb{R}^{L d_V \times n_k}.
\end{align}

На 3-м шаге вычисляем вычисляем матрицы~$\boldsymbol{\xi}_{ij}$ для всех пар~$(i, j)$.

Начнем вычисления с пар FFN. Заметим, что матрица~$\mathbf{B}_1$ не зависит от матрицы~$\mathbf{W}_1,$ а следовательно~$\boldsymbol{\xi}_{11} = \mathbf{0}.$ Аналогично матрица~$\mathbf{B}_2$ не зависит от матрицы~$\mathbf{W}_2,$ а следовательно~$\boldsymbol{\xi}_{22} = \mathbf{0}.$ Вычислим~$\frac{\partial \mathbf{B}_2}{\partial \mathbf{W}_1}$ используя свойство~\ref{prop:kronecker_product_derivative} производной произведения Кронекера для $\frac{\partial (\mathbf{X} \otimes \mathbf{Y})}{\partial \mathbf{X}},$ где $\mathbf{X}=\sigma(\mathbf{Y}\mathbf{W}_1)$ и $\mathbf{Y}=\mathbf{I}_{d_V}$:
\begin{align}
    \frac{\partial \mathbf{B}_2}{\partial \mathbf{W}_1}=\left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V} \right)\left( \mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V}) \right)\frac{\partial \mathrm{vec}_r(\sigma(\mathbf{Y}\mathbf{W}_1))}{\partial \mathbf{W}_1},
\end{align}
далее используя то, что $\frac{\partial\mathrm{vec}_r(\sigma(\mathbf{Y}\mathbf{W}_1))}{\partial \mathbf{W}_1} = \mathbf{D}_\sigma(\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})$ получаем оценку на~$\boldsymbol{\xi}_{12}:$
\begin{align}
    \boldsymbol{\xi}_{12}= \left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V} \right) \left( \mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V}) \right)\left( \mathbf{D}_\sigma  (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \right).
\end{align}
Вычислим~$\frac{\partial \mathbf{B}_1}{\partial \mathbf{W}_2}$ используя Лемму~\ref{lemma:matrix_funcs_product_derivative}, где в качестве левого множителя выступает~$(\mathbf{I}_L \otimes \mathbf{W}_2^\top)$:
\begin{align}
    \frac{\partial\mathbf{B}_1}{\partial \mathbf{W}_2}= \left( \mathbf{I}_{L d_V} \otimes \left( (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})^\top \mathbf{D}_\sigma^\top \right) \right) \frac{\partial(\mathbf{I}_L \otimes \mathbf{W}_2^\top)}{\partial \mathbf{W}_2},
\end{align}
далее используя свойство~\ref{prop:kronecker_product_derivative} об производной произведения Кронекера и Лемму~\ref{lemma:transposed_matrix_derivative} об производной транспонированной матрицы получем:
\begin{align}
    \frac{\partial(\mathbf{I}_L \otimes \mathbf{W}_2^\top)}{\partial \mathbf{W}_2}= \left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, L} \otimes \mathbf{I}_{d_{ff}} \right) \left( \mathrm{vec}_r(\mathbf{I}_L) \otimes \mathbf{I}_{d_V d_{ff}} \right) \mathbf{K}_{d_{ff}, d_V}.
\end{align}
Далее собирая все полученные матрицы, получаем оценку на~$\boldsymbol{\xi}_{21}:$
\begin{align}
    \boldsymbol{\xi}_{21} = \left( \mathbf{I}_{L d_V} \otimes \left( (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})^\top \mathbf{D}_\sigma^\top \right) \right)\left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, L} \otimes \mathbf{I}_{d_{ff}} \right) \left( \mathrm{vec}_r(\mathbf{I}_L) \otimes \mathbf{I}_{d_V d_{ff}} \right) \mathbf{K}_{d_{ff}, d_V}.
\end{align}

Перейдем к оценке пар FFN с параметрами слоев внимания для всех $k \in \{K, Q, V\}.$ Для матрицы~$\mathbf{B}_1 = (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}),$ заметим, что почти всюду матрица~$\frac{\partial \mathbf{D}_\sigma}{\partial \mathbf{Y}}=\mathbf{0}$ равняется нулю согласно Теоремы~\ref{theorem:relu_derivative_hessian}, а следовательно только последний множитель зависит от матрицы~$\mathbf{W}_k.$ Используя Лемму~\ref{lemma:matrix_funcs_product_derivative}, где первый множитель является константой, а также цепное правило относительно переменной~$\mathbf{Y}$ получаем:
\begin{align}
    \frac{\partial(\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})}{\partial \mathbf{W}_k} = \left( \frac{\partial (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})}{\partial \mathbf{Y}} \right) \frac{\partial\mathbf{Y}}{\partial \mathbf{W}_k},
\end{align}
причем согласно свойства~\ref{prop:kronecker_product_derivative} об производной произведения Кронекера с матрицей~$\mathbf{X} = \mathbf{Y}$ и матрицей~$\mathbf{Y}=\mathbf{I}_{d_{ff}}$ получаем:
\begin{align}
    \frac{\partial (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})}{\partial \mathbf{Y}} = \left( \mathbf{I}_L \otimes \mathbf{K}_{d_{ff}, d_V} \otimes \mathbf{I}_{d_{ff}} \right)\left( \mathbf{I}_{L d_V} \otimes \mathrm{vec}_r(\mathbf{I}_{d_{ff}}) \right),
\end{align}
а в свою очередь согласно Теоремы~\ref{thm:transformer_derivative} матрица~$\frac{\partial\mathbf{Y}}{\partial \mathbf{W}_k} = \mathbf{J}_Y \mathbf{G}_k.$ Тогда получаем оценку на матрицу~$\boldsymbol{\xi}_{1k}$ следующего вида:
\begin{align}
    \boldsymbol{\xi}_{1k} = \left( (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma \otimes \mathbf{I}_{n_k}\right)\left( \mathbf{I}_L \otimes \mathbf{K}_{d_{ff}, d_V} \otimes \mathbf{I}_{d_{ff}} \right)\left( \mathbf{I}_{L d_V} \otimes \mathrm{vec}_r(\mathbf{I}_{d_{ff}}) \right) \left( \mathbf{J}_Y \mathbf{G}_k \right).
\end{align}
Перейдем к матрице~$\mathbf{B}_2 = \sigma(\mathbf{Y}\mathbf{W}_1) \otimes \mathbf{I}_{d_V},$ заметим, что только первый множитель произведения Кронекера зависит от матриц~$\mathbf{W}_k,$ а следовательно используя свойство~\ref{prop:kronecker_product_derivative} производной произведения Кронекера и цепное правило получим следующее выражение:
\[
    \boldsymbol{\xi}_{2k} = \left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V} \right)\left( \mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V}) \right)\left( \mathbf{D}_\sigma (\mathbf{I}_L \otimes \mathbf{W}_1^\top)  \mathbf{J}_Y \mathbf{G}_k \right),
\]
где использовано свойство~\ref{prop:matrix_product_derivative} для преобразования~$\frac{\partial (\mathbf{Y}\mathbf{W}_1)}{\partial \mathbf{Y}} = \mathbf{I}_L \otimes \mathbf{W}_1^\top,$ а также Теорема~\ref{theorem:relu_derivative_hessian} для выражения $\frac{\partial \sigma(\cdot)}{\partial (\cdot)} = \mathbf{D}_\sigma,$ а также согласно Теореме~\ref{thm:transformer_derivative} матрица~$\frac{\partial\mathbf{Y}}{\partial \mathbf{W}_k} = \mathbf{J}_Y \mathbf{G}_k.$

Перейдем к оценке пар слоев внимания~$(k,\ell)$ with $k,\ell \in \{K,Q,V\}$.
Рассмотрим матрицу~$\mathbf{B}_k = \mathbf{J}_{SY} \mathbf{J}_Y  \mathbf{G}_k,$ заметим, что почти всюду~$\frac{\partial \mathbf{J}_{SY}}{\partial \mathbf{Y}} = \mathbf{0},$ так как матричнозначная функция~$\mathbf{D}_\sigma$ является кусочно-постоянной, согласно Теоремы~\ref{theorem:relu_derivative_hessian}, а следовательно используя Лемму~\ref{lemma:matrix_funcs_product_derivative} с матрицей~$\mathbf{A}(\cdot)=\mathbf{J}_Y,$ и с матрицей~$\mathbf{B}(\cdot)=\mathbf{G}_k$ получаем:
\begin{align}
    \frac{\partial\mathbf{B}_k}{\partial \mathbf{W}_\ell}= (\mathbf{J}_{SY} \otimes \mathbf{I}_{n_k}) \frac{\partial(\mathbf{J}_Y \mathbf{G}_k)}{\partial \mathbf{W}_\ell},
\end{align}
далее вычислим матрицу~$\frac{\partial(\mathbf{J}_Y \mathbf{G}_k)}{\partial \mathbf{W}_\ell}$ используя свойство производной матричного произведения:
\begin{align}
    \frac{\partial(\mathbf{J}_Y \mathbf{G}_k)}{\partial \mathbf{W}_\ell} = (\mathbf{J}_Y \otimes \mathbf{I}_{n_k}) \boldsymbol{\Phi}_{k\ell}+ \left( \mathbf{I}_{L d_V} \otimes \mathbf{G}_k^\top \right) \frac{\partial\mathbf{J}_Y}{\partial \mathbf{W}_\ell}.
\end{align}
В условия Теоремы~\ref{thm:layernorm_second_derivative} легко получить следующее выражение~$\frac{\partial\mathbf{J}_Y}{\partial \mathbf{W}_\ell} = \mathbf{H}_Y \mathbf{G}_\ell,$ а следовательно получаем оценки:
\begin{align}
    \boldsymbol{\xi}_{k\ell} = \left( \mathbf{J}_{SY} \otimes \mathbf{I}_{n_k} \right)\left[\left( \mathbf{I}_{L d_V} \otimes \mathbf{G}_k^\top \right) \left( \mathbf{H}_Y \mathbf{G}_\ell \right)+ \left( \mathbf{J}_Y \otimes \mathbf{I}_{n_k} \right) \boldsymbol{\Phi}_{k\ell}\right].
\end{align}


На 4-м шаге проверим симметричность полученных выражений.
Во первых единственные нелинейности с потенциально ненулевым вторым дифференциалом является оператор LayerNorm, для которого получены матрицы~$\mathbf{H}_Z,\mathbf{H}_Y$ в рамках Теоремы~\ref{thm:layernorm_second_derivative} и которые являются симметричными по построению и оператор ReLU, для которого матрица Гессе является нулевой согласно Теоремы~\ref{theorem:relu_derivative_hessian}, а во вторых все другие отображения являются линейными, а следовательно согласно Леммы~\ref{lemma:matrix_funcs_product_derivative} и свойства производной произведения Кронекера их частные производные являются коммутативными почти всюду.
\end{proof}

\subsection{Спектральные оценки матрицы Гессе для трансформера}

\begin{theorem}\label{thm:transformer_hessian_estimate}
Пусть матрица Гессе~$\mathbf{H}_{\mathrm{tr}}^{(i,j)}$ описывает матрицу Гессе между~$(i,j)$-м блоком трансформер модели~\eqref{eq:block_hessian_transformer}, где~$i,j\in\{1,2,K,Q,V\}, n_i=\dim(\mathbf{W}_i).$
Тогда для каждой пары~$(i,j)$ получаем оценку нормы:
\begin{equation}\label{eq:block_bound_transformer}
    \big\|\mathbf{H}_{\mathrm{tr}}^{(i,j)}\big\|_2 \le \|\mathbf{J}_Z\|_2 \|\boldsymbol{\xi}_{ij}\|_2 + \|\mathbf{B}_i\|_2 \|\mathbf{H}_Z\|_2 \|\mathbf{B}_j\|_2,
\end{equation}
где~$\boldsymbol{\xi}_{ij}=\frac{\partial}{\partial \mathbf{W}_j}\!\left(\frac{\partial \mathbf{S}}{\partial \mathbf{W}_i}\right), \mathbf{B}_i=\frac{\partial \mathbf{S}}{\partial \mathbf{W}_i}$. 

Пусть матрица~$\mathbf{H}_{\mathrm{tr}}$ является полной матрицей Гессе размера~$m_b \times n_b$ состоящей из блоков матрицы~$\mathbf{H}_{\mathrm{tr}}^{(i,j)},$ где $m_b=n_b=5,i\in\{1,2,K,Q,V\}, j\in\{1,2,K,Q,V\}$).
Тогда
\begin{equation}\label{eq:full_bound_transformer}
    \|\mathbf{H}_{\mathrm{tr}}\|_2 \le
    \sqrt{m_b n_b} \max \limits_{i,j} \left(\frac{2}{L d_V}\|\frac{\partial \mathbf{Z}}{\partial \mathbf{W}_i}\|_2 \|\frac{\partial \mathbf{Z}}{\partial \mathbf{W}_j}\|_2 + \|\mathbf{R}^{\text{tr}}_m\|_2 \|\mathbf{H}_{\text{tr}}^{(i,j)}\|_2 \right).
\end{equation}
\end{theorem}
\begin{proof}
Рассмотрим блоки матрицы Гессе~\eqref{eq:block_hessian_transformer}:
\begin{align}
    \mathbf{H}_{\mathrm{tr}}^{(i,j)} = \big( \mathbf{J}_Z \otimes \mathbf{I}_{n_i} \big) \boldsymbol{\xi}_{ij} + \big( \mathbf{I}_{L d_V} \otimes \mathbf{B}_i^\top \big) \mathbf{H}_Z  \mathbf{B}_j.
\end{align}
Используя свойства норм матриц~\ref{prop:matrix_sum_norm},~\ref{prop:matrix_product_norm} и~\ref{prop:kronecker_product_norm} получаем оценку
\begin{align}
    \big\|\mathbf{H}_{\mathrm{tr}}^{(i,j)}\big\|_2 &\le\big\|\mathbf{J}_Z \otimes \mathbf{I}_{n_i}\big\|_2 \|\boldsymbol{\xi}_{ij}\|_2+\big\|\mathbf{I}_{L d_V} \otimes \mathbf{B}_i^\top\big\|_2 \|\mathbf{H}_Z\|_2 \|\mathbf{B}_j\|_2 = \\
    &= \|\mathbf{J}_Z\|_2 \|\boldsymbol{\xi}_{ij}\|_2+\|\mathbf{B}_i\|_2 \|\mathbf{H}_Z\|_2 \|\mathbf{B}_j\|_2,
\end{align}
описанной в условиях Теоремы~\eqref{eq:block_bound_transformer}.

Оценим все слагаемые операторных норм в полученной оценке, а именно норму~$\|\mathbf{B}_i\|_2,$ и норму~$\|\boldsymbol{\xi}_{ij}\|_2,$ которые используются в формуле~\eqref{eq:block_bound_transformer}.
Используя свойствы матричных норм~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, \ref{prop:matrix_sum_norm}, \ref{prop:matrix_norm_inequalities}, \ref{prop:transposed_matrix_norm}, а также определение~\ref{def:commutation_matrix} коммутативных матрицы, получаем, что $\|\mathbf{K}_{m,n}\|_2=1,$ а также согласно свойству~\ref{prop:matrix_norm_inequalities} получаем нормы~$\|\mathrm{vec}_r(\mathbf{I}_{d})\|_2=\|\mathbf{I}_{d}\|_F=\sqrt{d}$ и~$\|\mathbf{I}_p\|_2=1$.
В доказательстве Теоремы~\ref{thm:self_attention_hessian_estimation} было доказано, что :
\begin{align}
    \Big\|\frac{\partial \mathbf{A}}{\partial \mathbf{T}}\Big\|_2 &\le \frac{1}{L},\\
    \|\mathbf{Z}_1\|_2 &= \|(\mathbf{I}_L \otimes \mathbf{X}^\top)(\partial \mathbf{A}/\partial \mathbf{T})(\mathbf{X}\otimes \mathbf{X})\|_2\le \|\mathbf{X}\|_2 \frac{1}{L} \|\mathbf{X}\|_2^2= \frac{1}{L}\|\mathbf{X}\|_2^3, \\
    \Big\|\frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2}\Big\|_2 &\le 6, \\
    \|\mathbf{Z}_2\|_2 &\le \|\mathbf{X}\|_2^5 \Big\|\frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2}\Big\|_2 \le 6 \|\mathbf{X}\|_2^5,\\
    \|\mathbf{A}\|_2 &\le \sqrt{L L}\|\mathbf{A}\|_{\max} = L,
\end{align}
а следовательно согласно свойству~\ref{prop:matrix_product_norm} получаем оценку $\|\mathbf{A}\mathbf{X}\|_2 \le \|\mathbf{A}\|_2 \|\mathbf{X}\|_2 \le L \|\mathbf{X}\|_2.$
Оценим матрицы~$\boldsymbol{\Phi}_{k\ell}$ полученные в рамках Леммы~\ref{lemma:attention_phi_from_functional_hessian}.
Используя свойства матричных норм~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, а также верхние оценки на матрицы~$\|\mathbf{Z}_1\|_2$, $\|\mathbf{Z}_2\|_2$ получаем:
\begin{align}
    \|\boldsymbol{\Phi}_{VV}\|_2 &= 0,\\
    \|\boldsymbol{\Phi}_{QQ}\|_2 &\le \frac{2}{L d_V d_K}\|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2 \|\mathbf{Z}_2\|_2 \|\mathbf{W}_K\|_2 \le\\
    &\le\frac{12}{L d_V d_K} \|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2^2 \|\mathbf{X}\|_2^5,\\
    \|\boldsymbol{\Phi}_{VQ}\|_2 &\le \frac{2}{L d_V \sqrt{d_K}}\|\mathbf{I}_L \otimes \mathbf{S}\|_2  \|\mathbf{Z}_1\|_2  \|\mathbf{I}_{d_V} \otimes \mathbf{W}_K\|_2 \le\\
    &\le\frac{2}{L^2 \sqrt{d_V d_K}} \|\mathbf{W}_K\|_2 \|\mathbf{X}\|_2^3,\\
    \|\boldsymbol{\Phi}_{QK}\|_2 &\le \frac{2}{L d_V d_K} \|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2 \|\mathbf{Z}_2\|_2 \|\mathbf{W}_Q\|_2
    + \frac{2}{L d_V \sqrt{d_K}} \|\mathbf{W}_V\|_2  \|\mathbf{Z}_1\|_2  \|\mathbf{S}\|_2 \le \\
    &\le \frac{12}{L d_V d_K} \|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \|\mathbf{X}\|_2^5
    + \frac{2}{L^2 \sqrt{d_V d_K}} \|\mathbf{W}_V\|_2 \|\mathbf{X}\|_2^3.
\end{align}
Для оценки матричных норм~$\|\mathbf{B}_i\|_2$ рассмотрим чему они равны при разных~$i$ из определения в Теоремах~\ref{thm:transformer_hessian}, \ref{theorem:relu_derivative_hessian}. Для матрицы~$\mathbf{B}_1 = (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}),$ тогда используя свойства матричных норм~\ref{prop:kronecker_product_norm}, \ref{prop:matrix_product_norm}, \ref{prop:transposed_matrix_norm}, а также оценку~$\|\mathbf{D}_\sigma\|_2 \le 1$ получаем:
\begin{equation}\label{eq:B1_norm}
    \|\mathbf{B}_1\|_2 \le \|\mathbf{I}_L \otimes \mathbf{W}_2^\top\|_2 \|\mathbf{D}_\sigma\|_2 \|\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}\|_2
    = \|\mathbf{W}_2\|_2 \|\mathbf{Y}\|_2.
\end{equation}
Для матрицы~$\mathbf{B}_2 = \sigma(\mathbf{Y}\mathbf{W}_1) \otimes \mathbf{I}_{d_V}$ воспользовавшись свойством~\ref{prop:kronecker_product_norm} получаем:
\begin{equation}\label{eq:B2_norm}
    \|\mathbf{B}_2\|_2 = \|\sigma(\mathbf{Y}\mathbf{W}_1)\|_2.
\end{equation}
Для матриц~$\mathbf{B}_k = \mathbf{J}_{SY}\mathbf{J}_Y\mathbf{G}_k,$ где~$k\in\{K,Q,V\},$ используя свойство~\ref{prop:matrix_product_norm}:
\begin{equation}\label{eq:Bk_norm}
    \|\mathbf{B}_k\|_2 \le \|\mathbf{J}_{SY}\|_2  \|\mathbf{J}_Y\|_2  \|\mathbf{G}_k\|_2.
\end{equation}
Для матрицы~$\mathbf{J}_{SY} = (\mathbf{I}_L \otimes \mathbf{W}_2^\top)\mathbf{D}_\sigma(\mathbf{I}_L \otimes \mathbf{W}_1^\top) + (\mathbf{I}_L \otimes \mathbf{I}_{d_V})$ используя свойства~\ref{prop:matrix_sum_norm}, \ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, \ref{prop:transposed_matrix_norm}, а также оценку~$\|\mathbf{D}_\sigma\|_2 \le 1$ получаем оценку матричной нормы:
\begin{align}\label{eq:JSY_norm}
    \|\mathbf{J}_{SY}\|_2 &\le \|\mathbf{I}_L \otimes \mathbf{W}_2^\top\|_2 \|\mathbf{D}_\sigma\|_2 \|\mathbf{I}_L \otimes \mathbf{W}_1^\top\|_2 + \|\mathbf{I}_L \otimes \mathbf{I}_{d_V}\|_2 =\\
    &=\|\mathbf{W}_2\|_2 \|\mathbf{W}_1\|_2 + 1.
\end{align}
Для матриц~$\|\mathbf{G}_V\|_2,\|\mathbf{G}_Q\|_2,\|\mathbf{G}_K\|_2,$ используя свойства~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm} получаем оценки на нормы:
\begin{align}\label{eq:Gk_bounds}
    \|\mathbf{G}_V\|_2 &\le L \|\mathbf{X}\|_2,\\
    \|\mathbf{G}_Q\|_2 &\le \frac{1}{L\sqrt{d_K}} \|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2 \|\mathbf{X}\|_2^3, \\
    \|\mathbf{G}_K\|_2 &\le \frac{1}{L\sqrt{d_K}} \|\mathbf{W}_V\|_2 \|\mathbf{W}_Q\|_2 \|\mathbf{X}\|_2^3.
\end{align}
Для оценки матричных норм~$\|\boldsymbol{\xi}_{ij}\|_2,$ рассмотрим чему они равны из определения в Теореме~\ref{thm:transformer_hessian}.
В случае пар FFN для матриц~$\|\boldsymbol{\xi}_{11}\|_2,\|\boldsymbol{\xi}_{12}\|_2,\|\boldsymbol{\xi}_{21}\|_2,\|\boldsymbol{\xi}_{22}\|_2$ используя свойства~\ref{prop:kronecker_product_norm}, \ref{prop:matrix_product_norm}, \ref{prop:matrix_norm_inequalities} матричных норм, а также свойство коммутативных матриц~$\|\mathbf{K}_{m,n}\|_2=1$ получаем оценки:
\begin{align}\label{eq:xiffn}
    \|\boldsymbol{\xi}_{11}\|_2 &= 0,\\
    \|\boldsymbol{\xi}_{22}\|_2 &= 0,\\
    \|\boldsymbol{\xi}_{12}\|_2 &\le \|\mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V}\|_2  \|\mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V})\|_2  \|\mathbf{D}_\sigma\|_2  \|\mathbf{Y}\otimes \mathbf{I}_{d_{ff}}\|_2 \nonumber\\
    &= 1 \cdot \|\mathrm{vec}_r(\mathbf{I}_{d_V})\|_2 \cdot 1 \cdot \|\mathbf{Y}\|_2
    = \sqrt{d_V} \|\mathbf{Y}\|_2, \\
    \|\boldsymbol{\xi}_{21}\|_2 &\le \|\mathbf{I}_L \otimes \mathbf{W}_2^\top\|_2  \|\mathbf{D}_\sigma\|_2  \|\mathbf{I}_L \otimes \mathbf{K}_{d_{ff}, d_V} \otimes \mathbf{I}_{d_{ff}}\|_2  \|\mathbf{I}_{L d_V} \otimes \mathrm{vec}_r(\mathbf{I}_{d_{ff}})\|_2 \nonumber\\
    &= \|\mathbf{W}_2\|_2 \cdot 1 \cdot 1 \cdot \|\mathrm{vec}_r(\mathbf{I}_{d_{ff}})\|_2 = \sqrt{d_{ff}} \|\mathbf{W}_2\|_2.
\end{align}
В случае пар FFN с параметрами слоев внимания для всех~$k\in\{K,Q,V\}$ получаем оценки:
\begin{align}\label{eq:xiffnk}
    \|\boldsymbol{\xi}_{1k}\|_2 &\le \|(\mathbf{I}_L \otimes \mathbf{W}_2^\top)\mathbf{D}_\sigma \otimes \mathbf{I}_{n_k}\|_2 \|\mathbf{I}_L \otimes \mathbf{K}_{d_{ff}, d_V} \otimes \mathbf{I}_{d_{ff}}\|_2 \cdot\\
    &\quad\cdot\|\mathbf{I}_{L d_V} \otimes \mathrm{vec}_r(\mathbf{I}_{d_{ff}})\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2 \nonumber\\
    &\le \|\mathbf{W}_2\|_2 \cdot 1 \cdot 1 \cdot \sqrt{d_{ff}} \cdot\|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2 =\\
    &= \sqrt{d_{ff}} \|\mathbf{W}_2\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2, \\
    \|\boldsymbol{\xi}_{2k}\|_2 &\le \|\mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V}\|_2 \|\mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V})\|_2 \|\mathbf{D}_\sigma\|_2  \|\mathbf{I}_L \otimes \mathbf{W}_1^\top\|_2 \cdot\\
    &\quad\cdot \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2 \nonumber\\
    &\le 1 \cdot \sqrt{d_V} \cdot 1 \cdot \|\mathbf{W}_1\|_2 \cdot \|\mathbf{J}_Y\|_2 \cdot \|\mathbf{G}_k\|_2 =\\
    &= \sqrt{d_V} \|\mathbf{W}_1\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2.
\end{align}
Для пар слоев внимания~$k,\ell\in\{K,Q,V\}$
\begin{align}
    \boldsymbol{\xi}_{k\ell}
    = \big(\mathbf{J}_{SY} \otimes \mathbf{I}_{n_k}\big)
    \Big[ \big(\mathbf{I}_{L d_V} \otimes \mathbf{G}_k^\top\big) (\mathbf{H}_Y \mathbf{G}_\ell)+ \big(\mathbf{J}_Y \otimes \mathbf{I}_{n_k}\big) \boldsymbol{\Phi}_{k\ell}\Big],
\end{align}
используя свойства~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm} получаем следующие оценки:
\begin{align}\label{eq:xikell}
    \|\boldsymbol{\xi}_{k\ell}\|_2 &\le \|\mathbf{J}_{SY}\|_2 \Big( \|\mathbf{I}_{L d_V} \otimes \mathbf{G}_k^\top\|_2 \|\mathbf{H}_Y\|_2 \|\mathbf{G}_\ell\|_2
    + \|\mathbf{J}_Y\|_2 \|\boldsymbol{\Phi}_{k\ell}\|_2 \Big) =\\
    &=\|\mathbf{J}_{SY}\|_2 \Big( \|\mathbf{G}_k\|_2 \|\mathbf{H}_Y\|_2 \|\mathbf{G}_\ell\|_2
    + \|\mathbf{J}_Y\|_2 \|\boldsymbol{\Phi}_{k\ell}\|_2 \Big).
\end{align}
Итого собирая все части выражения~\eqref{eq:block_bound_transformer}, используя для каждой пары~$(i,j)$ оценки норм матриц~$\|\boldsymbol{\xi}_{ij}\|_2$ с выражений~\eqref{eq:xiffn},\eqref{eq:xiffnk},\eqref{eq:xikell}, а также оценки норм матриц~$\|\mathbf{B}_i\|_2$ с выражений \eqref{eq:B1_norm},\eqref{eq:Bk_norm},\eqref{eq:JSY_norm},\eqref{eq:Gk_bounds} и подставляя в выражение~\eqref{eq:block_bound_transformer} получаем следующие оценки норм на все блоки матрицы Гесее:
\begin{align}
    \big\|\mathbf{H}_{\mathrm{tr}}^{(1,1)}\big\|_2
    &\le \|\mathbf{J}_Z\|_2 \cdot 0 + \|\mathbf{B}_1\|_2^2 \|\mathbf{H}_Z\|_2 \le\\
    &\le \|\mathbf{H}_Z\|_2 (\|\mathbf{W}_2\|_2 \|\mathbf{Y}\|_2)^2,\\
    \big\|\mathbf{H}_{\mathrm{tr}}^{(1,2)}\big\|_2 &\le \|\mathbf{J}_Z\|_2  \sqrt{d_V}\|\mathbf{Y}\|_2 + \|\mathbf{H}_Z\|_2  (\|\mathbf{W}_2\|_2 \|\mathbf{Y}\|_2) \|\sigma(\mathbf{Y}\mathbf{W}_1)\|_2,\\
    \big\|\mathbf{H}_{\mathrm{tr}}^{(1,k)}\big\|_2 &\le \|\mathbf{J}_Z\|_2 \sqrt{d_{ff}}\|\mathbf{W}_2\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2 +\\
    &\quad+\|\mathbf{H}_Z\|_2  (\|\mathbf{W}_2\|_2 \|\mathbf{Y}\|_2)  (\|\mathbf{J}_{SY}\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2),\\
    \big\|\mathbf{H}_{\mathrm{tr}}^{(k,\ell)}\big\|_2
    &\le \|\mathbf{J}_Z\|_2 \|\mathbf{J}_{SY}\|_2 \Big( \|\mathbf{G}_k\|_2 \|\mathbf{H}_Y\|_2 \|\mathbf{G}_\ell\|_2 + \|\mathbf{J}_Y\|_2 \|\boldsymbol{\Phi}_{k\ell}\|_2 \Big) +\\
    &\quad + \|\mathbf{H}_Z\|_2  (\|\mathbf{J}_{SY}\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2)  (\|\mathbf{J}_{SY}\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_\ell\|_2),
\end{align}

В оценке матричной норм~$\|\mathbf{Y} \|_2$ and $\| \mathbf{S}\|_2$ были использованы результаты Леммы~\ref{lemma:Y_S_norm_bounds}. Нормы матриц~$\mathbf{H}_Z,\mathbf{H}_Y$ оцениваются в рамках Леммы~\ref{lemma:layernorm_deriv_hessian_norm}.
\end{proof}