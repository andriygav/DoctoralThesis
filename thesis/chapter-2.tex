В современных задачах оптимизации, к которым редуцируется процесс обучения моделей глубокого обучения, фундаментальную роль играет анализ свойств целевой функции потерь $\mathcal{L}(\boldsymbol{\theta})$, заданной на многомерном пространстве параметров $\boldsymbol{\theta} \in \mathbb{R}^n$.
Глубокие нейронные сети, обладающие способностью к аппроксимации сложных нелинейных зависимостей, порождают высокосложные не выпуклые функции потерь с многочисленными локальными минимумами, седловыми точками и сложным ландшафтом оптимизационной задачи.
Если градиент $\nabla \mathcal{L}(\boldsymbol{\theta})$ характеризует скорость и направление наискорейшего спуска в параметрическом пространстве, то матрица Гессе $\mathbf{H}(\mathcal{L})$~--- симметричная матрица вторых частных производных функции потерь~--- предоставляет информацию о ее локальной кривизне, описывающая геометрические свойства ландшафта.
Матрица Гессе содержит информацию о локальном поведении функции в окрестности заданной точки, позволяя не только предсказывать траекторию оптимизации, но и анализировать устойчивость найденных решений.

Формальное определение матрицы Гессе для функции $\mathcal{L}(\boldsymbol{\theta})$ от $n$ параметров задается следующим выражением:
\[
\mathbf{H}(\mathcal{L})_{ij} = \frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j},
\]
где индексы $i, j = 1, \dots, n$ соответствуют компонентам вектора параметров $\boldsymbol{\theta}$.
Для функций, обладающих непрерывными вторыми производными, матрица Гессе симметрична в силу теоремы Шварца-Клеро-Янга о равенстве смешанных производных.
Эта симметричность обеспечивает вещественность всех собственных значений и ортогональность соответствующих собственных векторов, что имеет фундаментальное значение для спектрального анализа в дальнейшем.

Применения матрицы Гессе в контексте глубокого обучения проявляется в решении разнообразных теоретических и практических задач.
В аспекте оптимизации, на основе матрицы Гессе строятся методы второго порядка, такие как метод Ньютона, который использует обратную матрицу Гессе~$\mathbf{H}^{-1}$ для вычисления адаптивных направлений обновления параметров, учитывающих локальную кривизну поверхности потерь.
Это свойство обеспечивает существенное ускорение сходимости в окрестности локального минимума по сравнению с методами первого порядка, основанными исключительно на градиентной информации.
Однако прямая реализация методов второго порядка сопряжена с существенными вычислительными сложностями, что стимулировало развитие квази-ньютоновских методов и методов приближенного вычисления обратной матрицы Гессе.

В контексте теоретического анализа моделей машинного обучения, матрица Гессе вносит существенный вклад в оценку сложности и обобщающей способности моделей. 
Собственные значения матрицы Гессе, вычисленные в стационарной точке, содержат информацию о геометрии ландшафта функции потерь.
Спектральный анализ матрицы Гессе позволяет количественно охарактеризовать локальную кривизну функции потерь вдоль различных направлений в параметрическом пространстве, выявить наличие седловых точек и оценить устойчивость найденного решения:
\begin{itemize}
    \item[--] Малые собственные значения соответствуют направлениям с незначительной кривизной~--- так называемым ``плоским'' регионам, где параметры могут варьироваться без существенного роста ошибки.
    Эти направления часто ассоциируются с параметрами, оказывающими незначительное влияние на выход модели, или с симметриями в архитектуре сети.
    В противоположность этому, большие собственные значения указывают на ``острые'' минимумы с выраженной кривизной.
    Эмпирические и теоретические исследования подтверждают, что плоские минимумы демонстрируют улучшенную обобщающую способность, обусловленную их пониженной чувствительностью к малым возмущениям в данных и параметрах модели.
    Этот феномен, известный как "острота" минимума, активно изучаится в современной теории глубокого обучения и оптимизации.
    \item[--] След матрицы Гессе, равный сумме ее собственных значений, является интегральной характеристикой общей кривизны функции потерь. 
    Детальный анализ спектрального состава матрицы Гессе, в частности оценка кратности собственных значений вблизи нуля, позволяет количественно оценить эффективную размерность пространства параметров, влияющих на выход модели, и идентифицировать структурную избыточность в архитектуре нейронной сети.
    Кроме того, распределение собственных значений матрицы Гессе тесно связано с устойчивостью модели к шуму, способность к интерполяции данных и обобщающая способность на тестовых выборках.
\end{itemize}

Таким образом, матрица Гессе является не только эффективным инструментов для ускорения процесса оптимизации, но и является аналитическим аппаратом для анализа внутренних свойств модели: от устойчивости найденного решения до прогнозирования его способности к обобщению на новые данные.
Однако, использование матрицы Гессе в задачах глубокого обучения с миллионами и миллиардами параметров сталкивается с вычислительными ограничениями, поскольку требования к памяти и вычислительным ресурсам для хранения и обращения плотной матрицы размерности~$n \times n$ становятся непрактичными для реальных приложений.
Это методологическое ограничение обуславливает актуальность разработки эффективных методов аппроксимации матрицы Гессе и ее ключевых спектральных характеристик, чему и посвящена настоящая глава.
Современные подходы к решению этой проблемы включают методы случайного проектирования, разложения Кронекера, диагональные и блочно-диагональные аппроксимации, а также методы, основанные на теории случайных матриц.

\section{Полносвязная нейросетевая модель глубокого обучения}

Рассмотрим формальную постановку задачи $K$--классовой классификации с использованием функции потерь кросс-энтропии.
В данной постановке входные данные представляются вектором $\mathbf{x} \in \mathbb{R}^{l}$, а выходные данные~--- вектором $\mathbf{y} \in \mathbb{R}^{K}$, имеющим структуру one-hot кодирования, где все компоненты равны нулю, за исключением позиции $y_k = 1$, соответствующей истинной метке класса для входного образца $\mathbf{x}$.
Такое представление данных является стандартным для задач классификации и позволяет естественным образом использовать категориальное распределение для моделирования неопределенности предсказаний.

Рассматривается $L$-слойная полносвязная нейронная сеть $f_{\boldsymbol{\theta}}(\cdot)$ с функцией активации ReLU, применяемой после каждого линейного преобразования.
Выбор функции активации ReLU обусловлен ее вычислительной эффективностью и свойством устранять проблему затухающих градиентов, а также ее удобство для анализа в теоретическом анализе нейросетевых архитектур.
Для функции активации ReLU, определяемой как $\sigma(\mathbf{x}) = \left[ \mathbf{x} \geqslant\mathbf{0} \right] \mathbf{x},$ где $[\cdot]$ обозначает поэлементную индикаторную функцию, выход сети представляет собой вектор логитов $\mathbf{z} \in \mathbb{R}^{K}$.
Вычисление логитов осуществляется посредством последовательного применения следующих рекуррентных соотношений:
\begin{align}
    \mathbf{z}^{(p)} &= \mathbf{W}^{(p)} \mathbf{x}^{(p)} + \mathbf{b}^{(p)}, \\
    \mathbf{x}^{(p+1)} &= \sigma(\mathbf{z}^{(p)}).
\end{align}
Здесь $\mathbf{x}^{(p)}$ и $\mathbf{z}^{(p)}$ обозначают вход и выход $p$-го слоя соответственно, при этом полагается $\mathbf{x}^{(1)} = \mathbf{x}$ и $\mathbf{z} = f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{z}^{(L)}$. 
Совокупность всех параметров модели обозначается как $\boldsymbol{\theta} = \mathrm{col}(\mathbf{w}^{(1)}, \mathbf{b}^{(1)}, \ldots, \mathbf{w}^{(L)}, \mathbf{b}^{(L)}) \in \mathbb{R}^{n}$. Для $p$-го слоя $\mathbf{w}^{(p)}$ представляет собой векторизованную матрицу весов $\mathbf{W}^{(p)}$, а $\mathbf{b}^{(p)}$~--- соответствующий вектор смещений.
Общее число параметров $n$ в таких моделях может достигать миллионов и даже миллиардов, что и создает вычислительные трудности для точного вычисления матрицы Гессе.

Выходы модели определяется как $\mathbf{p} = \mathrm{softmax}(\mathbf{z}) \in \mathbb{R}^{K}$, где каждая компонента вычисляется по формуле:
\[
    p_i = \mathrm{softmax}(\mathbf{z})_i = \dfrac{\exp{(z_i)}}{\sum_{j=1}^{K} \exp{(z_j)}} \in (0; 1).
\] 
Функция потерь представляет собой стандартную кросс-энтропийную функцию ошибки:
\[
    \ell(\mathbf{z}, \mathbf{y}) = \mathrm{CE}(\mathbf{p}, \mathbf{y}) = - \sum_{k=1}^{K} y_k \log p_k \in \mathbb{R}^{+}.
\]
Эта функция является выпуклой по логитам $\mathbf{z}$, но невыпуклой по параметрам сети $\boldsymbol{\theta}$ из-за сложной композиционной структуры нейронной сети.

Согласно установленным результатам в литературе \cite{sagun2018empiricalanalysishessianoverparametrized}, применение цепного правила для матриц второго порядка \cite{skorski2019chainruleshessianhigher} позволяет декомпозировать матрицу Гессе на сумму двух структурно различных компонент:
\[
    \mathbf{H}_{i}(\boldsymbol{\theta}) = \underbrace{\nabla_{\boldsymbol{\theta}} \mathbf{z}_i \dfrac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_{i}^2} \nabla_{\boldsymbol{\theta}} \mathbf{z}_i^{\text{T}} }_{\text{G-компонента}} + \underbrace{\sum\limits_{k=1}^{K} \dfrac{\partial \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial z_{ik}} \nabla^2_{\boldsymbol{\theta}} z_{ik}}_{\text{H-компонента}},
\]
где $\nabla_{\boldsymbol{\theta}} \mathbf{z}_i \in \mathbb{R}^{P \times K}$ представляет собой матрицу Якоби функции нейронной сети по параметрам, а $\dfrac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_{i}^2}$~--- матрицу Гессе функции потерь относительно выходных логитов для $i$-го наблюдения. Первое слагаемое (G-компонента) отражает влияние кривизны функции потерь, в то время как второе слагаемое (H-компонента)~--- кривизну самой нейронной сети.

Эмпирические исследования \cite{pmlr-v97-ghorbani19b,sagun2018empiricalanalysishessianoverparametrized,papyan2019spectrumdeepnethessiansscale} демонстрируют, что спектральное распределение матрицы Гессе характеризуется наличием основной массы собственных значений, сосредоточенной вблизи нуля (обусловленной H-компонентой), и выбросов, распределенных в области ненулевых значений (обусловленных G-компонентой). Это бимодальное распределение собственных значений является характерной чертой гессиинов глубоких нейронных сетей и отражает фундаментальные свойства параметрического пространства таких моделей. Вследствие данной спектральной структуры, для практического анализа наиболее релевантной является G-компонента, что обосновывает использование следующей аппроксимации:
\[
    \mathbf{H}_{i}(\boldsymbol{\theta}) \approx \nabla_{\boldsymbol{\theta}} \mathbf{z}_i \dfrac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_{i}^2} \nabla_{\boldsymbol{\theta}} \mathbf{z}_i^{\text{T}}.
\]

Дополнительное теоретическое обоснование данной аппроксимации предоставляется в рамках теории ядра нейронного касательного пространства \cite{NEURIPS2018_5a4be1fa,Lee_2020}, где предполагается линейная зависимость логитов $\mathbf{z}$ от параметров $\boldsymbol{\theta}$ в окрестности точки оптимума. Данное предположение имплицирует исчезающую кривизну логитов $\nabla^2_{\boldsymbol{\theta}} z_{ik}$, что влечет тождественное обращение в ноль H-компоненты.

На основе работ \cite{wu2022dissecting}, предлагающих аналитическую аппроксимацию G-компоненты для полносвязных нейронных сетей, принимается следующая параметризация: $\mathbf{H}_{i}(\boldsymbol{\theta}) \approx \mathbf{F}_i^{\text{T}} \mathbf{A}_i \mathbf{F}_i$.
Эта факторизация позволяет эффективно вычислять приближения гессиана без явного построения полной матрицы, используя разложения в матрицы меньшей размерности.
Введем систему обозначений (для упрощения записи индекс $i$ опущен):

\begin{itemize}
    \item Матричное представление функции активации ReLU:
    \[
        \mathbf{D}^{(p)} = \mathrm{diag}([\mathbf{z}^{(p)} \geqslant \mathbf{0}]),
    \]
    Эта диагональная матрица кодирует паттерн активации нейронов на $p$-м слое и играет основную роль в определении функциональной структуры сети.
    
    \item Матрица прямого распространения от $p$-го слоя к выходу:
    \[
        \mathbf{G}^{(p)} = \dfrac{\partial \mathbf{z}}{\partial \mathbf{z}^{(p)}} = \mathbf{W}^{(L)} \mathbf{D}^{(L-1)} \mathbf{W}^{(L-1)} \mathbf{D}^{(L-2)} \cdot \ldots \cdot \mathbf{D}^{(p)}, 
    \]
    Эта матрица описывает, как изменения в активациях на $p$-м слое через последующие слои к выходу сети.
    
    \item Блочная матрица всех производных логитов по параметрам:
    \[
        \mathbf{F}^{\text{T}} =
        \begin{pmatrix}
            (\mathbf{G}^{(1)})^{\text{T}} \otimes \mathbf{x}^{(1)} \\
            (\mathbf{G}^{(1)})^{\text{T}} \\ 
            \vdots \\
            (\mathbf{G}^{(L)})^{\text{T}} \otimes \mathbf{x}^{(L)} \\
            (\mathbf{G}^{(L)})^{\text{T}} \\ 
        \end{pmatrix}, 
    \]
    где $\otimes$ обозначает произведение Кронекера. Эта блочная структура естественным образом отражает слоистую архитектуру сети и позволяет эффективное вычисление.
    
    \item Гессиан функции потерь относительно логитов, имеющий структуру ковариационной матрицы \cite{singla2019understanding}:
    \[
        \mathbf{A} = \nabla^2_\mathbf{z} \ell(\mathbf{z}, \mathbf{y}) = \mathrm{diag}(\mathbf{p}) - \mathbf{p} \mathbf{p}^{\text{T}}.
    \]
    Эта матрица является положительно полуопределенной и вырожденной, что отражает инвариантность функции softmax к сдвигам в пространстве логитов.
\end{itemize}

На основе предложенной параметризации получена верхняя оценка спектральной нормы матрицы Гессе в полносвязной нейронной сети, формулируемая в теореме~\eqref{thm:hess}.

\section{Матричные модели глубокого обучения}

В данном разделе рассматривается общий класс матричных моделей глубокого обучения, которые представляют собой композицию последовательных линейных преобразований и нелинейных функций активации.
Частным случаем такого представления являются сверточные нейронные сети (англ. CNN), а также другие архитектуры, где каждый слой может быть представлен в виде линейного оператора.

Пусть $f_{\boldsymbol{\theta}}(\mathbf{x})$ является суперпозицией $L+1$ слоев с активациями ReLU, что формально записывается как:
\[
    f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{T}^{(L+1)} \circ \sigma \circ \dots \circ \sigma \circ \mathbf{T}^{(1)}(\mathbf{x}).
\]

В этом представлении каждый $\mathbf{T}^{(p+1)}$ представляет собой линейный оператор или его матричное представление, а $\sigma$ обозначает функцию активации ReLU, применяемую поэлементно.
Такая композиционная структура позволяет описывать глубокие нейронные сети как последовательность преобразований, где каждый слой осуществляет линейное отображение с последующей нелинейной активацией.

Промежуточные результаты вычисления функции сети могут быть представлены в виде системы уравнений:
\begin{align}
  \begin{cases}
    \mathbf{z}^{(p+1)} &= \mathbf{T}^{(p+1)}\mathbf{x}^{(p)}, \\
    \mathbf{x}^{(p+1)} &= \sigma(\mathbf{z}^{(p+1)})
  \end{cases}
\end{align}
где выход сети определяется как $f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{z} := \mathbf{z}^{(L+1)}$, а входные данные задаются как $\mathbf{x}^{(0)} := \mathbf{x}$.
Здесь $\mathbf{z}^{(p+1)}$ представляет собой выход линейного оператора на $(p+1)$-м слое до применения активации, а $\mathbf{x}^{(p+1)}$~--- результат после применения функции активации ReLU, который является входом для следующего слоя.

Рассмотрим матрицу~$\boldsymbol{\Lambda}^{(p+1)} := \mathrm{diag}(\mathbf{x}^{(p+1)} > 0)$, зависящую от входных данных, которая кодирует паттерн активации нейронов на $(p+1)$-м слое.
Элементы этой матрицы равны 1 для нейронов с положительной активацией и 0 в противном случае, что отражает свойство функции ReLU "отсекать" отрицательные значения и представляет функцию активации в виде линейного оператора.
Используя эти диагональные матрицы, всю функцию нейронной сети можно представить в виде произведения матриц, а имменно супперпозиций линейных операторов:
\begin{equation}\label{eq::m-net:repr}
    f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)}\dots \boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)}\mathbf{x}.
\end{equation}

Данное представление удобно, так как позволяет рассматривать глубокую нейронную сеть с активациями ReLU как кусочно-линейную функцию, где нелинейность возникает исключительно за счет бинарных переключений в матрицах $\boldsymbol{\Lambda}^{(p)}$, зависящих от входных данных.

Вектор параметров модели объединяет все обучаемые параметры сети: $\boldsymbol{\theta} = \mathrm{col}(\mathbf{W}^{(L+1)}, \dots, \mathbf{W}^{(1)})$, где каждый линейный оператор $\mathbf{T}^{(p)}$ дифференцируемо параметризуется соответствующей частью вектора параметров $\mathbf{W}^{(p)}$. Для анализа модели вводится производная слоя по его параметрам:
\[
    \mathbf{Q}^{(p)} := \frac{\partial \mathbf{T}^{(p)}}{\partial \mathbf{W}^{(p)}},
\]
а затем строится блочно-диагональная матрица, объединяющая эти производные по всем слоям:
\[
    \mathbf{Q}:= \mathrm{diag}({\mathbf{Q}}^{(1)}, \dots, {\mathbf{Q}}^{(L+1)}).
\]

Матрица $\mathbf{Q}^{(p)}$ полностью описывает расположение параметров в $p$-м слое и их влияние на линейное преобразование этого слоя.

Для дальнейшего анализа вводятся дополнительные обозначения, которые описывают предшествующие и последующие преобразования относительно $p$-го слоя. Преобразования, которые последуют после $p$-го слоя:
\begin{align}
    \mathbf{G}^{(p)} &:= \mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)} \dots \mathbf{T}^{(p+1)}\boldsymbol{\Lambda}^{(p)}; \\
    \mathbf{G}^{(L+1)} &:= \mathbf{I};
\end{align}
и преобразования, которые предшествовали $p$-му слою:
\begin{align}
    \mathbf{R}^{(p)} &:= \boldsymbol{\Lambda}^{(p)}\mathbf{T}^{(p)} \dots \boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)};\;\;p = \overline{1,L}; \\ 
    \mathbf{R}^{(0)} &:= \mathbf{I}.
\end{align}
Матрица $\mathbf{G}^{(p)}$ описывает линейные преобразования от $p$-го слоя к выходу сети, в то время как $\mathbf{R}^{(p)}$ описывает линейные преобразование входа до $p$-го слоя.

Используя введенные обозначения, запишем следующие выражения:
\begin{align}
    \mathbf{z} &= \mathbf{G}^{(p)}\mathbf{z}^{(p)},\\
    \mathbf{x}^{(p)} &= \mathbf{R}^{(p)}\mathbf{x}, \\
    \mathbf{z} &= f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{G}^{(p)}\mathbf{T}^{(p)}\mathbf{R}^{(p-1)}\mathbf{x}.
\end{align}
Первое уравнение выражает выход сети через промежуточные значения на $p$-м слое, второе показывает преобразование входного сигнала до $p$-го слоя, а третье дает полное представление выхода сети через параметры $p$-го слоя и преобразования до и после него.

Объединяя матрицы $\mathbf{G}^{(p)}$ и $\mathbf{R}^{(p)}$ в единый оператор, получаем расширенную матрицу:
\begin{align}
\mathbf{F}^T := 
    \begin{pmatrix}
        \mathbf{G}^{(1)^T} \otimes \mathbf{R}^{(0)}\mathbf{x} \\
        \vdots \\
        \mathbf{G}^{(k)^T} \otimes \mathbf{R}^{(k-1)}\mathbf{x} \\
        \vdots \\
        \mathbf{G}^{(L+1)^T} \otimes \mathbf{R}^{(L)}\mathbf{x}
    \end{pmatrix}.
\end{align}
Эта блочная матрица содержит в себе всю информацию о том, как изменения параметров в различных слоях влияют на выход сети.
Каждый блок этой матрицы соответствует определенному слою и содержит информацию как о линейных преобразованиях от этого слоя к выходу ($\mathbf{G}^{(k)^T}$), так и о преобразовании входа до этого слоя ($\mathbf{R}^{(k-1)}\mathbf{x}$).

В случае использования функции потерь кросс-энтропии (CE) для задач классификации, гессиан функции потерь относительно логитов имеет структуру:
\[
    \mathbf{A} := \nabla^2_{\mathbf{z}} \ell = \mathrm{diag}(\mathbf{p}) - \mathbf{p}\mathbf{p}^T,
\]
где $\mathbf{p} := \mathrm{softmax}(\mathbf{z})$ представляет вектор вероятностей, предсказанных моделью.
Матрица $\mathbf{A}$ является ковариационной матрицей многомерного распределения и обладает свойствами положительной полуопределенности и вырожденности, что отражает инвариантность функции softmax к сдвигам в пространстве логитов.

\subsection{Матричная факторизация матрицы Гессе}

\begin{theorem}\label{theorem:m-net:hesstruct}
    Пусть функция нейронной сети $f_{\boldsymbol{\theta}}(\mathbf{x})$ представима в виде \eqref{eq::m-net:repr}, тогда матрица Гессе функции потерь относительно параметров модели может быть представлен в факторизованной форме:
    $\mathbf{H}_O(\boldsymbol{\theta}) = \mathbf{Q}^{T}\mathbf{F}^{T}\mathbf{A}\mathbf{F}\mathbf{Q}$, где $\mathbf{H}_O$ описывает G-компоненту матрицы Гессе.
\end{theorem}
\begin{proof}
Доказательство теоремы основано на последовательном применении цепного правила матричного дифференцирования и использовании свойств произведения Кронекера.
Исходное представление выхода матричной нейросетевой модели:
\[
    \mathbf{z} = f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)}\mathbf{T}^{(L)}....\boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)}\mathbf{x}.
\]

Производные выхода сети по параметрам модели вычисляются с использованием цепного правила:
\begin{align}
 \frac{\partial\mathbf{z}}{\partial\mathbf{W}^{(p)}} = \frac{\partial\mathbf{z}}{\partial\mathbf{z}^{(p)}} \frac{\partial\mathbf{z}^{(p)}}{\partial\mathbf{T}^{(p)}} \frac{\partial\mathbf{T}^{(p)}}{\partial\mathbf{W}^{(p)}}.
\end{align}

Для вычисления $\frac{\partial\mathbf{z}^{(p)}}{\partial\mathbf{T}^{(p)}}$ используется тождество для векторизации матричных произведений: $vec(\mathbf{B}\mathbf{V}\mathbf{A}^T) = (\mathbf{A} \otimes \mathbf{B})vec(\mathbf{V})$.
Применяя это тождество с $\mathbf{A} = \mathbf{I}$ и векторизуя $\mathbf{z}^{(p)} = \mathbf{T}^{(p)}\mathbf{x}^{(p-1)}$, получаем:
\[
    vec(\mathbf{z}^{(p)}) = vec(\mathbf{T}^{(p)}\mathbf{x}^{(p-1)}) = (\mathbf{I} \otimes \mathbf{x}^{(p-1)})vec(\mathbf{T}^{(p)}).
\]

Отсюда следует, что:
\[
    \frac{\partial \mathbf{z}^{(p)}}{\partial \mathbf{T}^{(p)}} = \mathbf{I} \otimes \mathbf{x}^{(p-1)^T}.
\]

Используя выражение $\mathbf{z} = \mathbf{G}^{(p)}\mathbf{z}^{(p)}$, получаем производную выхода сети по промежуточному значению:
\[
    \frac{\partial \mathbf{z}}{\partial \mathbf{z}^{(p)}} = \mathbf{G}^{(p)}.
\]

По определению $\mathbf{Q}^{(p)}$ имеем:
\[
    \frac{\partial\mathbf{T}^{(p)}}{\partial\mathbf{W}^{(p)}} = {\mathbf{Q}}^{(p)}.
\]

Для объединения этих выражений используется свойство произведения Кронекера: если $\mathbf{A}_i \in \mathbb{R}^{m_i \times n_i}$, то $\mathbf{A}_1 \otimes \mathbf{A}_2 = (\mathbf{A}_1 \otimes \mathbf{I}_{m_2})(\mathbf{I}_{m_1} \otimes \mathbf{A}_2)$.
Применяя это свойство с $m_2 = 1$, получаем:
\[
    \mathbf{G}^{(p)}\big(\mathbf{I} \otimes \mathbf{x}^{(p-1)^T}\big) = \big(\mathbf{G}^{(p)} \otimes \mathbf{I}_1\big)\big(\mathbf{I} \otimes \mathbf{x}^{(p-1)^T}\big) = \mathbf{G}^{(p)} \otimes \mathbf{x}^{(p-1)^T}.
\]

Подставляя все компоненты в исходную формулу для производной, получаем окончательное выражение:
\[
    \frac{\partial\mathbf{z}}{\partial\mathbf{W}^{(p)}} = (\mathbf{G}^{(p)} \otimes \mathbf{I}_1)(\mathbf{I} \otimes \mathbf{x}^{(p-1)^T})\mathbf{Q}^{(p)} = (\mathbf{G}^{(p)} \otimes \mathbf{x}^{(p-1)^T})\mathbf{Q}^{(p)}.
\]

Используя результаты работ по анализу гессиана в нейронных сетях \cite{singh2023hessianperspectivenatureconvolutional}, получаем выражение для блоков матрицы Гессе:
\begin{align}
    \mathbf{H}_O^{(kl)} &= J(\boldsymbol{\theta})^T \mathbf{A} J(\boldsymbol{\theta}) = \\
    &= \mathbf{Q}^{(k)^T}(\mathbf{G}^{(k)^T} \otimes \mathbf{R}^{(k-1)}\mathbf{x})A(\mathbf{G}^{(l)} \otimes \mathbf{x}^T\mathbf{R}^{(l-1)^T})\mathbf{Q}^{(l)}.
\end{align}

Объединяя все блоки в единую матрицу, получаем итоговое выражение для матрицы Гессе:
\[
    \mathbf{H}_O = \mathbf{Q}^\mathbf{T}\mathbf{F}\mathbf{A}\mathbf{F}^\mathbf{T}\mathbf{Q}.
\]
\end{proof}

Данная теорема устанавливает фундаментальный результат о структуре гессиана в матричных моделях глубокого обучения.
Предложенная факторизация позволяет эффективно анализировать и вычислять гессиан без необходимости явного построения полной матрицы вторых производных, что особенно важно для моделей с большим количеством параметров.
Структура $\mathbf{H}_O = \mathbf{Q}^\mathbf{T}\mathbf{F}\mathbf{A}\mathbf{F}^\mathbf{T}\mathbf{Q}$ подчеркивает, что гессиан может быть представлен как преобразование "внутреннего" гессиана $\mathbf{A}$ (зависящего только от логитов и функции потерь) с помощью матриц $\mathbf{F}$ и $\mathbf{Q}$, которые capture архитектурные свойства сети и параметризацию слоев соответственно.

\section{Матрица Гессе для трансформерной модели глубокого обучения}
Пусть $f_{\mathbf{w}}(\cdot)$ обозначает нейронную сеть, в данном случае слой самовнимания (англ. self-attention) или полный блок трансформера (англ. Transformer), с параметрами $\mathbf{w} \in \Omega$.
При наличии дважды дифференцируемых потерь $l(\cdot, \cdot)$ потери на выборку равны $l_i(\mathbf{w}) := l(f_{\mathbf{w}}(\mathbf{x}_i), \mathbf{y}_i)$.
Эмпирические потери для выборок $L = k$ равны $\mathcal{L}_k(\mathbf{w}) = \frac{1}{k} \sum_{i=1}^k l_i(\mathbf{w})$, с гессианом $\mathbf{H}^{(k)}(\mathbf{w}) = \frac{1}{k} \sum_{i=1}^k \nabla^2_{\mathbf{w}} l_i(\mathbf{w})$.


Пусть заданы входные вектора эмбедингов (англ. embeddings) $\mathbf{X} \in \mathbb{R}^{L \times d_V}$.
Выход слоя одной головы (англ. single-head) слоя самовнимания задается в виде: 
\begin{equation} \label{eq:self_attention}
    \mathbf{F}(\mathbf{X}) = \mathbf{A}(\mathbf{X}) \mathbf{X} \mathbf{W}_V,
\end{equation}
где $\mathbf{A}(\mathbf{X}) = \mathrm{softmax}\left( \frac{\mathbf{X} \mathbf{W}_Q \mathbf{W}_K^\top \mathbf{X}^\top}{\sqrt{d_K}} \right)$, а $\mathbf{W}_Q, \mathbf{W}_K \in \mathbb{R}^{d_V \times d_K}$, $\mathbf{W}_V \in \mathbb{R}^{d_V \times d_V}$.

Используя \eqref{eq:self_attention}, полный блок трансформера выглядит в следующим образом:
\begin{align}
    \text{LayerNorm}\Big(\mathbf{\text{LayerNorm}(\mathbf{X} + \mathbf{F}(\mathbf{X}))} + \mathrm{FFN}(\mathbf{\text{LayerNorm}(\mathbf{X} + \mathbf{F}(\mathbf{X}))})\Big)
\end{align}
где $\mathrm{FFN}(\cdot)$ является блоком полносвязной сети с некоторой нелинейностью.
Слой LayerNorm для входной матрицы $\mathbf{U} \in \mathbb{R}^{m \times n}$ описывается выражением:
\[
    \text{LayerNorm}(\mathbf{U})_{i,j} = \gamma_j \frac{\mathbf{U}_{i,j} - \mu_i}{\sqrt{\sigma_i^2}} + \mathbf{\beta}_j,
\]
где $\mu_i = \frac{1}{m} \sum_{j=1}^m \mathbf{U}_{i,j}, \quad \sigma_i^2 = \frac{1}{m} \sum_{j=1}^m (\mathbf{U}_{i,j} - \mu_i)^2$.

\begin{assumption}\label{assumption:LayerNorm}
Для входной матрицы слоя LayerNorm: $\mathbf{X} + \mathbf{F}(\mathbf{X})$, $\mathbf{Y} + \mathrm{FFN}(\mathbf{Y})$, построчная дисперсия удовлетворяет условию $\min_i \sigma_i^2 > 0$.
\end{assumption}

Предположение~\ref{assumption:LayerNorm} является техническим и требуется для доказательства ряда теорем. Выполнения данного свойства можно добиться, добавив к знаменателю положительную константу, но это усложнит вычисления.

Для оценки матрицы Гессе рассматривается среднеквадратичная функция ошибки:
\[
    l(\cdot, \textbf{Target}) = \frac{1}{L d_V} \|\cdot - \textbf{Target}\|_F^2.
\]
В дальнейшем для доказательств будет использоваться разложения Гаусса-Ньютона матрицы Гессе $\mathcal{L}_k \circ f_{\mathbf{w}}$:
\begin{equation}\label{eq:gauss_decomposition}
    \frac{\partial^2 (\mathcal{L}_k \circ f_{\mathbf{w}})}{\partial \mathbf{W}_i \partial \mathbf{W}_j} = \frac{\partial f_{\mathbf{w}}}{\partial \mathbf{W}_i} (\cdot)^\top \frac{\partial^2 \mathcal{L}_k}{\partial f_{\mathbf{w}}^2} (f_{\mathbf{w}}(\cdot)) \frac{\partial f_{\mathbf{w}}}{\partial \mathbf{W}_j}(\cdot) + \left( \frac{\partial \mathcal{L}_k}{\partial f_{\mathbf{w}}} (f_{\mathbf{w}}(\cdot)) \otimes \mathbf{I}_{p_i q_i} \right) \frac{\partial^2 f_{\mathbf{w}}}{\partial \mathbf{W}_i \partial \mathbf{W}_j}(\cdot)
\end{equation}

Для начала вычислим обобщённые выражения матрицы Гессе для слоя самовнимания и расширяем их до полного блока модели трансформер. Подход основан на теоретической базе \cite{ormaniec2024attentionhessian}, адаптируя и обобщая её результаты.

Матрица Гессе функции ошибки $\mathcal{L}_k$ относительно параметров модели $\mathbf{w}$:
\[
    \mathbf{H}^{(k)}(\mathbf{w}) = \nabla^2_{\mathbf{w}} \mathcal{L}_k(\mathbf{w}) = \frac{1}{k} \sum_{i=1}^k \nabla^2_{\mathbf{w}} l_i(\mathbf{w}) = \frac{1}{k} \sum_{i=1}^k \mathbf{H}_i(\mathbf{w})
\]
где $\mathbf{H}_k(\mathbf{w})$ является матрицей Гессе блока самовнимания для параметров $\mathbf{w}$ относящиеся к матрицам $\{ \mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V\}$. Используя разложения Гаусса-Ньютона \eqref{eq:gauss_decomposition}:
\[
    \mathbf{H}_k(\mathbf{W}_i, \mathbf{W_j}) = \frac{\partial^2 l}{\partial \mathbf{W}_i \partial \mathbf{W}_j} = \mathbf{H}_o(\mathbf{W}_i, \mathbf{W}_j) + \mathbf{H}_f(\mathbf{W}_i, \mathbf{W}_j),
\]
где $\mathbf{H}_o$ является outer-product частью матрицы Гессе, а $\mathbf{H}_f$ является матрицей Гессе функции самовнимания.
Результаты этого разложения можно вычислить согласно теоремам 3.1-3.2 в работе~\cite{ormaniec2024attentionhessian}.

\subsection{Матрица Гессе для слоя самовнимания}
Проведем оценку нормы матрицы Гессе для одного слоя самовнимания. Результат данной оценки показан в теореме~\ref{thm:self_attention_hessian_estimation}.

\begin{theorem}\label{thm:self_attention_hessian_estimation}

Пусть $\|\cdot\|_2$ является спектральной нормой, тогда для слоя самовнимания получаем:
\[
    \|\mathbf{H}_i(\mathbf{w}^*)\|_2 \leq M,
\]
где
\begin{align}
M &= 3\cdot\max \Bigg(\frac{2L}{d_V} \| \mathbf{X}\|^2_2, \\
    &\frac{8}{L^3 d_V d_K} \| \mathbf{W}_K\|_2^2 \| \mathbf{W}_V\|^2_2 \| \mathbf{X}\|^6_2 + \\
    &\quad+\frac{12}{d_V d_K} \sqrt{\min(L, d_V)} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2) \| \mathbf{W}_V \|_2 \| \mathbf{W}_K\|^2_2 \| \mathbf{X}\|^5_2, \\
    &\frac{4}{L d_V \sqrt{d_K}} \| \mathbf{W}_V\|_2 \| \mathbf{W}_K \|_2 \| \mathbf{X}\|^4_2 +\\
    &\quad+\frac{4\sqrt{\min(L, d_V)}}{L^2\sqrt{d_K}} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2) \|\mathbf{W}_K\|_2 \|\mathbf{X}\|^3_2,\\
    &\frac{8}{L^3 d_V d_K} \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{W}_V\|^2_2 \|\mathbf{X} \|^6_2 + \\
    &\quad+\frac{4\sqrt{\min(L, d_V)} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2)}{L d_V \sqrt{d_K}} \|\mathbf{W}_V\|_2 \cdot\\
    &\quad\quad\cdot\Big(3L \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{X}\|^5_2 + \frac{d_V}{L} \|\mathbf{X}\|^3_2\Big)\Bigg).
\end{align}
\end{theorem}
\begin{proof}

Используя результаты Леммы A.3 из работы \cite{noci2022signalpropagationtransformerstheoretical}, а также свойство~\ref{prop:matrix_product_norm} и свойство~\ref{prop:kronecker_product_norm} получаем:
\begin{align}
    \| \frac{\partial\mathbf{A}}{\partial\mathbf{T}}\|_2 = \frac{1}{L} \| \mathbf{I}_L\|_2 \| \mathbf{I}_L - \frac{1}{L}\mathbf{1}_{L \times L}\|_2 \leq \frac{1}{L}
\end{align}
Данное неравенство верно в силу того, что $\frac{1}{L}\mathbf{1}_{L \times L}$ является матрицей проекции, поэтому $\mathbf{I}_L - \frac{1}{L}\mathbf{1}_{L \times L}$ также матрица проекции и следовательно норма $\| \mathbf{I}_L - \frac{1}{L}\mathbf{1}_{L \times L}\|_2 \leq 1$.

Далее для аппроксимации нормы матрицы $\mathbf{Z}_1$ используем те же свойства~\ref{prop:matrix_product_norm} и \ref{prop:kronecker_product_norm}:
\begin{align}\label{chapter-2:theorem:proof_Znorm}
    \|\mathbf{Z}_1\|_2 &\leq \| \mathbf{I}_L \otimes \mathbf{X}^{\top}\|_2 \left\| \frac{\partial \mathbf{A}}{\partial \mathbf{T}}\right\|_2 \| \mathbf{X} \otimes \mathbf{X}\|_2 \leq\\
    & \leq \| \mathbf{X}\|_2 \frac{1}{L} \|\mathbf{X}\|^2_2 = \frac{1}{L} \|\mathbf{X}\|^3_2
\end{align}
где дополнительно было использовано свойство \ref{prop:transposed_matrix_norm} for $\| \mathbf{X}\|_2 = \| \mathbf{X}^\top\|_2$.

Оценим норму матрицы $\| \mathbf{A}\|_2,$ которая является матрицей, где каждая строка является результатом применения функции~$\text{softmax},$ а следовательно, каждый элемент матрицы~$\mathbf{A}_{i,j} \leq 1$.
Далее используя свойства \ref{prop:matrix_norm_inequalities} получаем $ \|\mathbf{A}\|_{\max} \leq \|\mathbf{A}\|_2 \leq \sqrt{LL} \|\mathbf{A}\|_{\max} = L\| \mathbf{A}\|_{max} \leq L$.
Также получаем, что:
\[
    \|\mathbf{M}_1\|_2 = \|\mathbf{A}\mathbf{X}\|_2 \leq L \|\mathbf{X}\|_2.
\]

Итого, легко получаем outer-product матрицы Гессе $\|\mathbf{H}_o (\mathbf{W}_i, \mathbf{W}_j) \|_2$  для разных матриц.
В случае матрицы~$\mathbf{W}_V$ и матрицы~$\mathbf{W}_V$:
\begin{align}
    \| \mathbf{H}_o(\mathbf{W}_V, \mathbf{W}_V)\|_2 &\leq \frac{2}{L d_V} \| \mathbf{M}_1\|^2_2 1 \leq \frac{2}{L d_V}\| \mathbf{A}\|^2_2 \| \mathbf{X}\|^2_2 \leq\\
    & \leq \frac{2}{L d_V} L^2 \| \mathbf{X}\|^2_2 = \frac{2L}{d_V}\| \mathbf{X}\|^2_2.
\end{align}
Для матриц~$\mathbf{W}_Q$ и~$\mathbf{W}_Q$ получаем:
\begin{align}
    \| \mathbf{H}_o(\mathbf{W}_Q, \mathbf{W}_Q)\|_2 &\leq \| \frac{2}{L d_V d_K} (\mathbf{I}_{d_V} \otimes \mathbf{W}^\top_K) \mathbf{Z}^\top_1 (\mathbf{I}_{L} \otimes \mathbf{W}_V\mathbf{W}^\top_V)\ \mathbf{Z}_1 (\mathbf{I}_{d_V} \otimes \mathbf{W}_K)\|_2 \leq\\
    &\leq \frac{2}{L d_V d_K} \| \mathbf{W}_K\|_2^2 \|\mathbf{Z}_1 \|^2_2 \| \mathbf{W}_V\|^2_2 \leq\\
    &\leq \frac{2}{L d_V d_K}\| \mathbf{W}_K\|_2^2\| \mathbf{W}_V\|^2_2 \frac{1}{L^2} \| \mathbf{X}\|^6_2 =\\
    &= \frac{2}{L^3 d_V d_K} \| \mathbf{W}_K\|_2^2\| \mathbf{W}_V\|^2_2 \mathbf{X}\|^6_2.
\end{align}
Между матрицей~$\mathbf{W}_V$ и матрицей~$\mathbf{W}_Q$:
\begin{align}
    \| \mathbf{H}_o(\mathbf{W}_V, \mathbf{W}_Q)\|_2 &\leq \frac{2}{L d_V \sqrt{d_K}} \|\mathbf{M}_1^\top \otimes \mathbf{W}_V^\top \|_2 \| \mathbf{Z}_1\|_2 \| \mathbf{I}_{d_V} \otimes \mathbf{W}_K \|_2 \leq\\
    &\leq \frac{2}{L d_V \sqrt{d_K}} L \| \mathbf{X}\|_2 \| \mathbf{W}_V\|_2 \frac{1}{L} \| \mathbf{X}\|^3_2 \| \mathbf{W}_K \|_2 =\\
    &= \frac{2}{L d_V \sqrt{d_K}} \| \mathbf{W}_V\|_2 \| \mathbf{W}_K \|_2 \| \mathbf{X}\|^4_2
\end{align}
Между матрицей~$\mathbf{W}_Q$ и матрицей~$\mathbf{W}_K$:
\begin{align}
    &\left\|\mathbf{H}_o(\mathbf{W}_Q, \mathbf{W}_K)\right\|_2 \leq \\
    &\leq \frac{2}{L d_V d_K} \big\|(\mathbf{I}_{d_V} \otimes \mathbf{W}^\top_K) \mathbf{Z}_1^\top(\mathbf{I}_L \otimes \mathbf{W}_V \mathbf{W}^\top_V)\mathbf{Z}_1 (\mathbf{W}_Q \otimes \mathbf{I}_{d_V}) \mathbf{K}{d_K, d_V}\big\|_2 \leq \\
    &\leq \frac{2}{L^3 d_V d_K} \big\|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{W}_V\|_2^2 \|\mathbf{X} \big\|^6_2.
\end{align}
Для всех оценок использовались свойства~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, а также свойства $\|\mathbf{K}_{d_V d_K} \|_2=1$, потому что $\mathbf{K}_{m,n}$ является коммутативной матрицей, описанной в определении~\ref{def:commutation_matrix}. 

Далее проведем анализ матрицы~$\mathbf{H}_f$.
Для этого начнем анализ с матрицы~$\mathbf{R}_m = \mathrm{vec}_r (\mathbf{F} (\mathbf{X}) - \textbf{Target})^T \otimes \mathbf{I}_m,$ который описан в рамках теоремы 3.2 в работе~\cite{ormaniec2024attentionhessian}.
Так, как~$\mathrm{vec}_r(\cdot)$ является функцией векторизации: 
\begin{align}
    \|\mathrm{vec}_r(\mathbf{F}(\mathbf{X}) - \textbf{Target})\|_2 &= \| \mathbf{F}(\mathbf{X}) - \textbf{Target} \|_F \leq\\
    &\leq\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target} )} \| \mathbf{F}(\mathbf{X}) - \textbf{Target} \|_2,
\end{align}
тогда согласно свойству \ref{prop:matrix_norm_inequalities} получаем: 
\begin{align}
    \| \mathbf{R}_m\| &\leq \sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \|\mathbf{F}(\mathbf{X}) - \textbf{Target}\|_2 \leq\\
    &\leq\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} (\| \mathbf{A} \|_2 \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2) \leq\\
    &\leq \sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2)
\end{align}
где для получения оценок были использованы свойства матриц \ref{prop:matrix_product_norm} и \ref{prop:matrix_sum_norm}. В свою очередь норма матрицы перемешивания (англ. shuffling matrix) оценивается следующим образом:
\begin{align}
    \| \mathbf{S}\|_2 &= \|(\mathbf{I}_{d_V} \otimes \mathbf{K}_{d_V,d_V}) (\mathrm{vec}_r (\mathbf{I}_{d_V}) \otimes \mathbf{I}_{d_V})\|_2 \leq \\
    &\leq \| \mathrm{vec}_r (\mathbf{I}_{d_V}) \|_2 = \| \mathbf{I}_{d_V}\|_{F} = \sqrt{d_V}.
\end{align}

Для верхней оценки нормы матрицы $\| \frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2} \|_2$ воспользуемся леммой C1 с работы \cite{ormaniec2024attentionhessian}, где указано, что:
\begin{equation}\label{chapter-2:theorem:proof_partialA}
    \frac{\partial^2 \mathbf{A}_{i,j}}{\partial \mathbf{T}_{i,:} \partial \mathbf{T}_{i,:}} = \mathbf{A}_{i,j} \left(2\mathbf{A}_{i,:}\mathbf{A}_{i,:}^{\top} + \mathbf{E}_{j,j}^{L,L} - \text{diag}(\mathbf{A}_{i,:}) - \mathbf{e}_j \mathbf{A}_{i,:}^{\top} - \mathbf{A}_{i,:}\mathbf{e}_j^{\top}\right) \in \mathbb{R}^{L \times L},
\end{equation}
где 
\[
    \mathbf{E}_{j,j}^{L, L} = \mathbf{e}_j \mathbf{e}_j^{\top} \in \mathbb{R}^{L \times L},
\]
поэтому он содержит только один ненулевой элемент, который равен 1 в позиции $(j, j)$.
Кроме того, вторая производная softmax имеет блочно-диагональную структуру, а следовательно используя свойство~\ref{prop:block_matrix_norm} нормы блочно диагональной матрицы получаем:
\[
    \left\|\frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2}\right\|_2 = \max_{i,j} \left\|\frac{\partial^2 \mathbf{A}_{i,j}}{\partial \mathbf{T}_{i,:} \partial \mathbf{T}_{i,:}}\right\|_2.
\]
Приходим к тому, что требуется оценить следующую норму:
\[
    \left\|\frac{\partial^2 \mathbf{A}_{i,j}}{\partial \mathbf{T}_{i,:} \partial \mathbf{T}_{i,:}} \right\|_2.
\]

Как было указано ранее $\mathbf{A}_{i,j} \leq 1,$ а следовательно можем оценить матрицу $\| \mathbf{A}_{i,:}\mathbf{A}_{i,:}^{\top} \|_2$, так как $\mathbf{A}_{i,:}$ является строкой $\text{softmax}$-матрицы, то значение суммы строки равняются $1$.
Таким образом, мы можем использовать векторно-матричные неравенства для получения выражение:
\begin{equation}\label{chapter-2:theorem:proof_Arow}
    \| \mathbf{A}_{i,:}\mathbf{A}_{i,:}^{\top} \|_2 \leq \|\mathbf{A}_{i,:}\|^2_2 \leq \|\mathbf{A}_{i,:}\|_1^2 = 1.
\end{equation}

Аналогично заметим, что 
\begin{equation}\label{chapter-2:theorem:proof_Enorm}
    \|\mathbf{E}_{j,j}^{m,n}\|_2 = \| \mathbf{e}_j \mathbf{e}_j^{\top}\|_2 \leq 1.
\end{equation}

Перейдем к оценке нормы диагональной матрицы $\|diag(\mathbf{A}_{i,:})\|_2$.
Заметим, что для диагональной матрицы верно следующее выражение:
\begin{equation}\label{chapter-2:theorem:proof_Adiag}
    \|diag(\mathbf{A}_{i,:})\|_2 = \max \limits_j \mathbf{A}_{i,j} \leq 1.
\end{equation}

Используя оценки~\eqref{chapter-2:theorem:proof_Arow} и~\eqref{chapter-2:theorem:proof_Adiag} и~\eqref{chapter-2:theorem:proof_Enorm} оценим нормы $\mathbf{e}_j \mathbf{A}_{i,:}^{\top}$ и $\mathbf{A}_{i,:}\mathbf{e}_j^{\top}.$
Матрицы $\mathbf{e}_j \mathbf{A}_{i,:}^{\top}$ and $\mathbf{A}_{i,:}\mathbf{e}_j^{\top}$ являются матрицами ранга~$1,$ причем только с одной не нулевой строкой и колонкой соотвественно с элементами матрицы~$\mathbf{A}_{i,:}.$
Следовательно их спектральные нормы оценивается сверху нормой матрицы~$\|\mathbf{A}_{i,:}\|_2 \leq 1$.

Получаем, что все слагаемые в выражении~\eqref{chapter-2:theorem:proof_partialA} имеют верхнюю оценку $1,$ а следовательно:
\begin{align}
    \left\| \frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2} \right\|_2 \leq 6 
\end{align}

Возвращаясь к выражению~\eqref{chapter-2:theorem:proof_Znorm} получаем оценку матрицы $\| \mathbf{Z}_2 \|_2$:
\begin{align}
    \| \mathbf{Z}_2 \|_2 &= \| \left(\mathbf{I}_L \otimes \mathbf{X}^\top \otimes \mathbf{X}^\top \otimes \mathbf{X}^\top\right) \left(\partial^2\mathbf{A}/\partial\mathbf{T}^2\right) \left(\mathbf{X} \otimes \mathbf{X}\right) \|_2 \leq \\
    &\leq \| \mathbf{X}\|^5_2 \left\| \frac{\partial^2\mathbf{A}}{\partial\mathbf{T}^2} \right\|_2 \leq 6 \| \mathbf{X}\|^5_2
\end{align}
Оцениваем часть~$\mathbf{H}_\mathrm{f}.$ Для нормы между матрицами~$\mathbf{W}_V$ и~$\mathbf{W}_V$:
\begin{align}
    \|\mathbf{H}_\mathrm{f}(\mathbf{W}_V, \mathbf{W}_V)\|_2 = 0
\end{align}
Для нормы между матрицами~$\mathbf{W}_Q$ и~$\mathbf{W}_Q$:
\begin{align}
    \|\mathbf{H}_\mathrm{f}(\mathbf{W}_Q, \mathbf{W}_Q)\|_2 &= \frac{2}{Ld_V d_K} \|\mathbf{R}_{d_V d_K} \left(\mathbf{I}_L \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V} \otimes \mathbf{W}_K^\top\right) \mathbf{Z}_2 \left(\mathbf{I}_{d_V} \otimes \mathbf{W}_K\right)\|_2, \\
    &\leq \frac{2}{Ld_V d_K} \| \mathbf{R}_{d_V d_K} \|_2 \| \mathbf{W}_V \|_2 \| \mathbf{W}_K\|_2 \|\mathbf{Z}_2 \|_2 \| \mathbf{W}_K\|_2 \\
    &\leq 6\frac{2}{Ld_V d_K}\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 +\\
    &\quad +\|\textbf{Target}\|_2\Big)\| \mathbf{W}_V \|_2 \| \mathbf{W}_K\|^2_2\| \mathbf{X}\|^5_2 =\\
    &= \frac{12}{d_V d_K}\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \\
    &\quad+\|\textbf{Target}\|_2\Big)\| \mathbf{W}_V \|_2 \| \mathbf{W}_K\|^2_2\| \mathbf{X}\|^5_2
\end{align}
Для нормы между матрицами~$\mathbf{W}_V$ и~$\mathbf{W}_Q$:
\begin{align}
    \|\mathbf{H}_\mathrm{f}(\mathbf{W}_V, \mathbf{W}_Q)\|_2 &= \frac{2}{Ld_V\sqrt{d_K}} \|\mathbf{R}_{d_V^2} \left(\mathbf{I}_L \otimes \mathbf{S}\right) \mathbf{Z}_1 \left(\mathbf{I}_{d_V} \otimes \mathbf{W}_K\right)\|_2 \leq \\
    &\leq \frac{2}{Ld_V\sqrt{d_K}} \| \mathbf{R}_{d_V^2}\|_2 \| \mathbf{S} \|_2 \|\mathbf{Z}_1 \|_2 \| \mathbf{W}_K\|_2 \leq\\
    &\leq \frac{2}{Ld_V\sqrt{d_K}} \sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 +\\
    &\quad +\|\textbf{Target}\|_2\Big) \sqrt{d_V} \frac{1}{L} \|\mathbf{X}\|^3_2\|\mathbf{W}_K\|_2 = \\
    &= \frac{2\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})}}{L^2\sqrt{d_Vd_K}}\Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \\
    &\quad+\|\textbf{Target}\|_2\Big)\|\mathbf{W}_K\|_2 \|\mathbf{X}\|^3_2
\end{align}
Для нормы между матрицами~$\mathbf{W}_Q$ и~$\mathbf{W}_K$:
\begin{align}
    &\|\mathbf{H}_\mathrm{f}(\mathbf{W}_Q, \mathbf{W}_K)\| \leq \\
    &\leq\frac{2}{Ld_V d_K}\| \mathbf{R}_{d_V d_K} \left(\mathbf{I}_L \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V} \otimes \mathbf{W}_K^\top\right) \mathbf{Z}_2 \left(\mathbf{W}_Q \otimes \mathbf{I}_{d_V}\right) \mathbf{K}_{d_K, d_V}\|_2 + \\
    &\quad + \frac{2}{Ld_V\sqrt{d_K}} \|\mathbf{R}_{d_V} \left(\mathbf{I}_L \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V}\right) \left(\mathbf{Z}_1 \otimes \mathbf{I}_{d_V}\right) \mathbf{S} \otimes \mathbf{I}_{d_K}\|_2 \leq\\
    &\leq  \frac{2}{Ld_V d_K}\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 +\\
    &\quad + \|\textbf{Target}\|_2\Big) \|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_26 \| \mathbf{X}\|^5_2 + \\
    &\quad+\frac{2}{Ld_V\sqrt{d_K}}\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \\
    &\quad +\|\textbf{Target}\|_2\Big) \|\mathbf{W}_V \|_2 \frac{1}{L} \|\mathbf{X}\|^3_2 \sqrt{d_V} =\\
    &=\frac{2\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2)}{Ld_V\sqrt{d_Vd_K}} \|\mathbf{W}_V\|_2 \cdot \\
    & \quad \cdot \Big(3L\|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{X}\|^5_2 + \frac{d_V}{L} \|\mathbf{X}\|^3_2\Big).
\end{align}

Собирая все оценки вместе, используя матричное свойство \ref{prop:matrix_norm_inequalities} для всех блоков $\{K, Q, V\}$:
\begin{align}
    &\| \mathbf{H} (\mathbf{W}_i, \mathbf{W}_j)\|_2 \leq 3\max \limits_{i,j \in \{Q, K, V\}} \Big(\|\mathbf{H}_o(\mathbf{W}_i, \mathbf{W}_j)\|_2 + \|\mathbf{H}_f(\mathbf{W}_i, \mathbf{W}_j)\|_2\Big)
\end{align}
Подставляя оценки получаем следующую оценку на матрицу Гессе:
\begin{align}
    &\| \mathbf{H} (\mathbf{W}_i, \mathbf{W}_j)\|_2 \leq\\
    & \leq 3 \max \Bigg(\frac{2L}{d_V} \| \mathbf{X}\|^2_2, \\
    &\frac{2}{L^3 d_V d_K} \| \mathbf{W}_K\|_2^2 \| \mathbf{W}_V\|^2_2 \| \mathbf{X}\|^6_2 +\\
    &\quad+ \frac{12}{d_V d_K} \sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 +\\
    &\qquad+\|\textbf{Target}\|_2\Big) \| \mathbf{W}_V \|_2 \| \mathbf{W}_K\|^2_2 \| \mathbf{X}\|^5_2, \\
    &\frac{2}{L d_V \sqrt{d_K}} \| \mathbf{W}_V\|_2 \| \mathbf{W}_K \|_2 \| \mathbf{X}\|^4_2 +\\
    &\quad+\frac{2\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})}}{L^2\sqrt{d_V d_K}} (L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2) \|\mathbf{W}_K\|_2 \|\mathbf{X}\|^3_2,\\
    &\frac{2}{L^3 d_V d_K} \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{W}_V\|^2_2 \|\mathbf{X} \|^6_2 + \\
    &\quad+\frac{2\sqrt{\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target})} \Big(L \|\mathbf{X}\|_2 \|\mathbf{W}_V \|_2 + \|\textbf{Target}\|_2\Big)}{L d_V \sqrt{d_V d_K}} \cdot\\
    &\qquad\cdot\|\mathbf{W}_V\|_2 \Big(3L \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \| \mathbf{X}\|^5_2 + \frac{d_V}{L} \|\mathbf{X}\|^3_2\Big)\Bigg).
\end{align}

Полученное выражение почти полностью соответствует выражению~$M,$ где для полного соотвествия требуется воспользоваться неравенством~$\mathrm{rank}(\mathbf{F}(\mathbf{X}) - \textbf{Target}) \le \min(L, d_V)$. 
\end{proof}

Теорема~\ref{thm:self_attention_hessian_estimation} оценивает только один слой самовнивания. Теперь перейдем к оценке полного блока трансформера. Полный трансформер слой содержит слой самовнимания, блок полносвязной сети (англ. FFN), и слоя нормализации выходов (англ. LayerNorm). Весь блок описывается следующими выражениями:
\begin{align}\label{eq:transformer}
    \mathbf{Y} &= \text{LayerNorm}(\mathbf{X} + \mathbf{F}(\mathbf{X})) \\ 
    \mathbf{Z} &= \text{LayerNorm}(\mathbf{Y} + \text{FFN}(\mathbf{Y})), 
\end{align} 
где
\[
    \text{FFN}(\mathbf{Y}) = \sigma(\mathbf{Y} \mathbf{W}_1 + \mathbf{b}_1) \mathbf{W}_2 + \mathbf{b}_2,
\]
с матрицами параметров $\mathbf{W}_1 \in \mathbb{R}^{d_V \times d_{\text{ff}}} $, матрицами$\mathbf{W}_2 \in \mathbb{R}^{d_{\text{ff}} \times d_V}$, векторами~$b_1 \in \mathbb{R}^{d_{\text{ff}}}$, $b_2 \in \mathbb{R}^{d_V}$, а также функцией активации~$\sigma$.
Функция $\text{LayerNorm}(\mathbf{X})$ определяется для входной матрицы~$\mathbf{X} \in \mathbb{R}^{L \times d_V}$ следующим образом:
\begin{align}
    \text{LayerNorm}(\mathbf{X})_{i,j} = \mathbf{\gamma}_j \cdot \frac{\mathbf{X}_{i,j} - \mu_i}{\sqrt{\sigma_i^2}} + \mathbf{\beta}_j,
\end{align}
где параметры~$\mu_i, \sigma_i$ определяются следующим образом:
\[
    \mu_i = \frac{1}{d_V} \sum_{j=1}^{d_V} \mathbf{X}_{i,j}, \quad \sigma_i^2 = \frac{1}{d_V} \sum_{j=1}^{d_V} (\mathbf{X}_{i,j} - \mu_i)^2,
\]
а параметры~$\gamma_j, \beta_j$ являются настраиваемым в процессе оптимизации.

Итого, получаем полный список параметров полного слоя трансформера:
\[
    \mathbf{w} = \{\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V, \mathbf{W}_1, \mathbf{W}_2, \mathbf{b}_1, \mathbf{b}_2, \mathbf{\gamma}, \mathbf{\beta}\}
\], где $\mathbf{\gamma},\mathbf{\beta}$ являются параметрами LaterNorm, для простоты вычисления в некоторых случаях введем предположения, что параметры~$\mathbf{\gamma},\mathbf{\beta}$ являются постоянными и не меняются в процессе оптимизации.

\subsection{Матрица Гессе для LayerNorm слоя}

Для начала вычислим для функции~$\mathrm{LayerNorm}$ матрицу Якоби относительно параметров модели, для этого докажем теорему~\ref{thm:layernorm_derivative}.
\begin{theorem}\label{thm:layernorm_derivative}
    Пусть задана матрица $\mathbf{X} \in \mathbb{R}^{L \times d_V}$. Определим функцию~$\mathbf{M}(\mathbf{X})$ следующим образом:
    \begin{align}
        \mathbf{M}(\mathbf{X}) &= \mathbf{X} - \tfrac{1}{d_V}\mathbf{X}\mathbf{1}_{d_V}\mathbf{1}_{d_V}^\top, \\
        \sigma(\mathbf{X}) &= \tfrac{1}{\sqrt{d_V}}\big(\mathbf{M}(\mathbf{X})^{\circ 2}\mathbf{1}_{d_V}\big)^{\circ 1/2},\\
        \mathbf{P}(\mathbf{X}) &= \mathrm{diag}^{-1}(\sigma(\mathbf{X})).
    \end{align}
    Тогда для функции LayerNorm :
    \[
        \text{LayerNorm}(\mathbf{X}) = \mathbf{P}(\mathbf{X}) \mathbf{M}(\mathbf{X}),
    \]
    матрица Якоби относительно переменной~$\mathbf{X}$ определяется следующим образом:
    \begin{align}
        \frac{\partial \text{LayerNorm}(\mathbf{X})}{\partial \mathbf{X}} &= (\mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V}) \left(\mathbf{I}_{Ld_V} - \tfrac{1}{d_V}(\mathbf{I}_L \otimes \mathbf{1}_{d_V \times d_V})\right)+\\
    &\quad+\left(\mathbf{I}_L \otimes \mathbf{M}(\mathbf{X})^\top\right)\frac{\partial\mathbf{P}(\mathbf{X})}{\partial\mathbf{X}},
    \end{align}
    где
    \begin{align}
    \frac{\partial \mathbf{P}}{\partial \mathbf{X}}&=\frac{1}{\sqrt{d_V}}\Big( -\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \Big)\cdot\\
    &\quad\cdot\big( \mathbf{e}_1 \otimes \mathbf{e}_1, \dots, \mathbf{e}_L \otimes \mathbf{e}_L \big)\cdot\\
    &\quad\cdot\Big(\mathrm{diag}^{-1}\!\big(\mathrm{vec}_r^{1/2}(\mathbf{M}^{\circ 2}\mathbf{1}_{d_V})\big)(\mathbf{I}_L \otimes \mathbf{1}_{d_V}^\top)\mathrm{diag}\left(\mathrm{vec}_r(\mathbf{M})\right)\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\Big),
    \end{align}
    где $\mathbf{D} = \mathrm{diag}(\sigma(\mathbf{X})).$
\end{theorem}
\begin{proof}
Представим функцию LayerNorm в матричном виде:
\begin{equation}
    \text{LayerNorm}(\mathbf{X}) = \mathbf{P}(\mathbf{X})\mathbf{M}(\mathbf{X}),
\end{equation}
где матрица~$\mathbf{P}(\mathbf{X}) = \mathbf{D}^{-1}$, а матрица~$\mathbf{D} = \textit{diag}(\sigma(\mathbf{X})),$ в свою очередь согласно свойству~\ref{prop:elem_wise_division} матрица~$\mathbf{M}(\mathbf{X}) = (\mathbf{X} - \mu(\mathbf{X})\mathbf{1}^\top_{d_V}).$

Используя лемму~\ref{lemma:matrix_funcs_product_derivative} получаем выражение для произведения матричнозначных функций:
\begin{equation}
    \frac{\partial\text{LayerNorm}(\mathbf{X})}{\partial \mathbf{X}} = ( \mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V}) \frac{\partial \mathbf{M}}{\partial \mathbf{X}} + (\mathbf{I}_L\otimes \mathbf{M}^\top)\frac{\partial \mathbf{P}}{\partial \mathbf{X}}
\end{equation}

Вычислим значение производной~$\frac{\partial \mathbf{M}}{\partial \mathbf{X}},$ используя матричные вычисления~$\mathbf{M}(\mathbf{X}) = (\mathbf{X} - \mu(\mathbf{X}) \mathbf{1}^{\top}_{d_V}) = (\mathbf{X} - \frac{1}{d_V}\mathbf{X} \mathbf{1}_{d_V} \mathbf{1}^{\top}_{d_V}) = (\mathbf{X} - \frac{1}{d_V}\mathbf{X} \mathbf{1}_{d_V \times d_V})$. Получаем:
\begin{equation}
   \frac{\partial \mathbf{M}}{\partial \mathbf{X}} = \frac{\partial  (\mathbf{X} - \frac{1}{d_V}\mathbf{X} \mathbf{1}_{d_V \times d_V})}{\partial \mathbf{X}} = (\mathbf{I}_L \otimes \mathbf{I}_{d_V}) - \frac{1}{d_V}(\mathbf{I}_L \otimes \mathbf{1}_{d_V \times d_V})
\end{equation}

Далее вычислим значение производной~$\frac{\partial \mathbf{P}}{\partial \mathbf{X}}.$
Для начала получим выражение для нелинейного преобразования~$\sigma(\mathbf{X}).$
Данное выражение в матричном виде принимает вид:
\[
    \sigma(\mathbf{X}) =  \left(\frac{1}{d_V} (\mathbf{X} - \mu(X)\mathbf{1}_{d_V}^\top)^{\circ 2} \mathbf{1}_{d_V}\right)^{\circ \frac{1}{2}} = \frac{1}{\sqrt{d_V}} \left(\mathbf{M}(\mathbf{X})^{\circ 2} \mathbf{1}_{d_V}\right)^{\circ{\frac{1}{2}}},
\]
где~$\circ \alpha$ операция поэлементного взятия степени $\alpha$ описанного в определении~\ref{def:vec_elem_ops}.
Далее, применив цепное правило получаем:
\begin{align}
    \frac{\partial \mathbf{P}}{\partial \mathbf{X}} &= \frac{\partial \mathbf{D}^{-1}}{\partial \mathbf{D}} \frac{\partial\textit{diag}(\sigma(\mathbf{X}))}{\partial \sigma(\mathbf{X})} \frac{\partial \sigma(\mathbf{X})}{\partial \mathbf{X}},
\end{align}
причем используя свойства~\ref{lemma:hadamard_square_derivative}, \ref{lemma:hadamard_root_derivative} и~\ref{prop:matrix_product_derivative} получаем выражение для~$\frac{\partial \sigma(\mathbf{X})}{\partial \mathbf{X}}:$
\begin{equation}
    \frac{\partial \sigma(\mathbf{X})}{\partial \mathbf{X}} = \frac{1}{\sqrt{d_V}} \frac{\partial \tau^{\circ \frac{1}{2}}}{\partial \tau} \frac{\partial \tau}{\partial \mathbf{Q}} \frac{\partial \mathbf{Q}}{\partial {\mathbf{X}}},
\end{equation}
где~$\tau = \mathbf{Q}\cdot\mathbf{1}_L, \mathbf{Q} = \mathbf{M}^{\circ{2}},$ а следовательно подставляя получаем:
\begin{align}
    \frac{\partial \sigma(\mathbf{X})}{\partial \mathbf{X}} &= \frac{1}{\sqrt{d_V}} \frac{\partial \tau^{\circ \frac{1}{2}}}{\partial \tau} \frac{\partial \mathbf{Q}\cdot\mathbf{1}_{d_V}}{\partial \mathbf{Q}} \frac{\partial \mathbf{M}^{\circ{2}}}{\partial \mathbf{M}} \frac{\partial \mathbf{M}}{\partial \mathbf{X}} = \\
    \\ &= \frac{1}{\sqrt{d_V}} \frac{1}{2} \textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\tau)) (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V}) 2\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial \mathbf{X}} = \\
    &= \frac{1}{\sqrt{d_V}}\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\cdot (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M})) \frac{\partial \mathbf{M}}{\partial \mathbf{X}}.
\end{align}

Используя леммы~\ref{lemma:invert_derivative} и \ref{lemma:diag_derivative} получаем:
\begin{align}
    \frac{\partial \mathbf{P}}{\partial \mathbf{X}} &= \frac{1}{\sqrt{d_V}}\left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right) \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big) \cdot \\
    &\quad\cdot\left(\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\cdot (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\right).
\end{align}

И того мы получили все составляющие для вычисления матрицы Якобы для LayerNorm оператора:
\begin{align}
    &\frac{\partial\text{LayerNorm}(\mathbf{X})}{\partial \mathbf{X}} = ( \mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V}) \frac{\partial \mathbf{M}}{\partial \mathbf{X}} + (\mathbf{I}_L\otimes \mathbf{M}^\top)\frac{\partial \mathbf{P}}{\partial \mathbf{X}} = \\
    & = ( \mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V}) \frac{\partial \mathbf{M}}{\partial \mathbf{X}} +\\
    &\quad+ (\mathbf{I}_L\otimes \mathbf{M}^\top)\frac{1}{\sqrt{d_V}}\left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right) \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big) \cdot \\
    &\qquad\cdot \left(\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\cdot (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\right),
\end{align}
где
\begin{align}
    \mathbf{M}(\mathbf{X}) &= (\mathbf{X} - \frac{1}{d_V}\mathbf{X} \mathbf{1}_{d_V \times d_V})\\
    \mathbf{P}(\mathbf{X}) &= \textit{diag}^{-1}(\sigma(\mathbf{X}))\\
    \frac{\partial \mathbf{M}}{\partial \mathbf{X}} &= (\mathbf{I}_L \otimes \mathbf{I}_{d_V}) - \frac{1}{d_V}(\mathbf{I}_L \otimes \mathbf{1}_{d_V \times d_V}.
\end{align}
\end{proof}

Теперь же вычислим для функции матрицу Гессе относительно параметров модели, для этого докажем теорему~\ref{thm:layernorm_second_derivative}.
\begin{theorem}\label{thm:layernorm_second_derivative}
    Пусть задан оператор~$LayerNorm$ в виде аналогичном теореме~\ref{thm:layernorm_derivative}:
    \[
        \text{LayerNorm}(\mathbf{X}) = \mathbf{P}(\mathbf{X}) \mathbf{M}(\mathbf{X}),
    \]
    с матрицей Якоби, полученную в теореме~\ref{thm:layernorm_derivative}:
    \[
        \frac{\partial \text{LayerNorm}}{\partial \mathbf{X}} = (\mathbf{P} \otimes \mathbf{I}_{d_V}) \mathbf{G} + (\mathbf{I}_L \otimes \mathbf{M}^\top) \mathbf{H},
    \]
    где дополнительно введены обозначения константы:
    \[
        \mathbf{G} = \left(\mathbf{I}_{Ld_V} - \tfrac{1}{d_V}(\mathbf{I}_L \otimes \mathbf{1}_{d_V \times d_V})\right),
    \] а также оператор, аналогичный оператору в теореме~\ref{thm:layernorm_derivative}:
    \[
        \mathbf{H} = \frac{\partial \mathbf{P}}{\partial \mathbf{X}}.
    \]
    Тогда для функции~$\text{LayerNorm}$ матрица Гессе относительно параметров~$\mathbf{X}$ имеет вид:
    \begin{align}\label{chapter-2:theorem:layernorm_second_derivative:eqstemant}
        \frac{\partial^2 \text{LayerNorm}}{\partial \mathbf{X}^2} &= \left( (\mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V}) \otimes \mathbf{I}_{L d_V} \right)\frac{\partial^2 \mathbf{M}}{\partial \mathbf{X}^2} +\\
        &\quad+\left( \mathbf{I}_{L d_V} \otimes \mathbf{G}^\top \right) \frac{\partial (\mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V})}{\partial \mathbf{X}} + \\
        &\quad+ \left( (\mathbf{I}_L \otimes \mathbf{M}^\top ) \otimes \mathbf{I}_{L d_V} \right) \frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2} + \left( \mathbf{I}_{L d_V} \otimes \mathbf{H}^\top \right) \frac{\partial (\mathbf{I}_L \otimes \mathbf{M}^\top )}{\partial\mathbf{X}}.
    \end{align}
    причем все матрицы явно вычислимые и задаются формулами, которые указаны ниже в доказательстве.
\end{theorem}
\begin{proof}
Используя свойство матричного произведения~\ref{prop:matrix_product_derivative} получаем следующее выражение матрицы Гессе для оператора~\text{LayerNorm}:
\begin{align}
    \frac{\partial^2\text{LayerNorm}}{\partial\mathbf{X}^2} &= \left( (\mathbf{P}(\mathbf{X})  \otimes  \mathbf{I}_{d_V}) \otimes \mathbf{I}_{Ld_V} \right)\frac{\partial^2 \mathbf{M}}{\partial \mathbf{X}^2} +\\
    &\quad+\left( \mathbf{I}_{Ld_V} \otimes  \left(\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\right)^\top \right) \frac{\partial (\mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V})}{\partial \mathbf{X}} + \\
    &\quad+ \left( (\mathbf{I}_L \otimes \mathbf{M}^\top ) \otimes \mathbf{I}_{L d_V}  \right) \frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2} +\\
    &\quad+\left( \mathbf{I}_{L d_V} \otimes \left(\frac{\partial \mathbf{P}}{\partial \mathbf{X}}\right)^\top \right) \frac{\partial (\mathbf{I}_L \otimes \mathbf{M}^\top )}{\partial\mathbf{X}},
\end{align}
причем заметим, что~$\mathbf{P} \in \mathbb{R}^{L \times L}$, $\mathbf{M} \in \mathbb{R}^{L \times d_V}$, $\frac{\partial \mathbf{M}}{\partial \mathbf{X}} \in \mathbb{R}^{Ld_V \times Ld_V}$, $\frac{\partial \mathbf{P}}{\partial \mathbf{X}} \in \mathbb{R}^{L^2 \times Ld_V}.$ Используя свойства~\ref{prop:kronecker_product_derivative} и леммы~\ref{lemma:transposed_matrix_derivative} получаем следующие выражения первых и вторых производных: 
\begin{align}
    \frac{\partial^2 \mathbf{M}}{\partial \mathbf{X}^2} &= 0, \\
    \frac{\partial (\mathbf{P}(\mathbf{X}) \otimes \mathbf{I}_{d_V} )}{\partial \mathbf{X}} &= \frac{\partial (\mathbf{P} \otimes \mathbf{I}_L)}{\partial \mathbf{P}} \frac{\partial \mathbf{P}}{\partial\mathbf{X}} = \left(\mathbf{I}_L \otimes \mathbf{K}_{L, L} \otimes \mathbf{I}_L \right) \left(\mathbf{I}_{L^2} \otimes  \mathrm{vec}_r(\mathbf{I}_L)  \right) \frac{\partial \mathbf{P}}{\partial \mathbf{X}}, \\
    \frac{\partial (\mathbf{I}_L \otimes \mathbf{M}^\top )}{\partial\mathbf{X}} &= \frac{\partial ( \mathbf{I}_L \otimes \mathbf{M}^\top)}{\partial \mathbf{M}^\top} \frac{\partial \mathbf{M}^\top}{\partial \mathbf{M}} \frac{\partial \mathbf{M}}{\partial \mathbf{X}} =\\
    &=\left(\mathbf{I}_{L} \otimes \mathbf{K}_{d_V,L} \otimes \mathbf{I}_L \right) \left(\mathrm{vec}_r(\mathbf{I}_L) \otimes  \mathbf{I}_{L d_V} \right) \mathbf{K}_{d_V, L} \frac{\partial \mathbf{M}}{\partial \mathbf{X}}.
\end{align}

Перейдем к оценке вторых производных матрицы~$\mathbf{P}.$
Рассмотрим каждое слагаемое в матрице подробнее.
Матрица~$\mathbf{D}$ является диагональной матрицей~$\textit{diag}(\sigma(\mathbf{X})),$ причем размер вектор~$\sigma(\mathbf{X})$ имеет размерность  $L\times 1$, тогда матрица~$\mathbf{D} \in \mathbb{R}^{L\times L},$ а следовательно слагаемое~$\left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right) \in \mathbb{R}^{L^2 \times L^2}$.
Каждый базисный вектор~$\mathbf{e}_i$ имеет размерность~$L\times1$, а следовательно~$\mathbf{e}_i \otimes \mathbf{e}_i \in \mathbb{R}^{L^2 \times 1},$ и тогда $ \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big) \in \mathbb{R}^{L^2 \times L}$.
Ранее было доказано, что~$\mathbf{M}(\mathbf{X}) \in \mathbb{R}^{L \times d_V}$, тогда $M\cdot\mathbf{1}_{d_V} \in \mathbb{R}^{L \times 1}$, и следовательно слагаемое~$\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))$ имеет размерность $L \times L$.
Следующие слагаемые~$(\mathbf{I}_L \otimes \mathbf{1}^T_{d_V}) \in \mathbb{R}^{L \times Ld_V}$ и $\textit{diag}(\mathrm{vec}_r (M)) \in \mathbb{R}^{Ld_V \times Ld_V}$.
Последнее слагаемое~$\frac{\partial \mathbf{M}}{\partial \mathbf{X}}$ уже было посчитано ранее, причем его размерность~$Ld_V \times Ld_V$. Итого матрица~$\frac{\partial \mathbf{P}}{\partial \mathbf{X}}$ принадлежит пространству~$\mathbb{R}^{L^2 \times Ld_V}$.
 
Для удобства введем обозначение:
\[
    \frac{\partial \mathbf{P}}{\partial \mathbf{X}} = \frac{1}{\sqrt{d_V}} \mathbf{A}_1(\mathbf{X})\cdot \mathbf{B}_1(\mathbf{X}),
\]
где $\mathbf{A}_1 = \left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right),$ а $\mathbf{B}_1$ все остальное, обе матрицы были вычислены ранее.
Используя свойство произведения матричнозначных функций~\ref{lemma:matrix_funcs_product_derivative} вторая производная принимает вид:
\begin{align}
    \frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2} = \frac{1}{\sqrt{d_V}} \frac{\partial \mathbf{A}_1(\mathbf{X})\cdot \mathbf{B}_1(\mathbf{X})}{\partial \mathbf{X}} = \frac{1}{\sqrt{d_V}} \left(\mathbf{A}_1 \otimes  \mathbf{I}_{Ld_V}  \right) \frac{\partial \mathbf{B}_1}{\partial \mathbf{X}} + \left( \mathbf{I}_{L^2} \otimes  \mathbf{B}_1^\top \right) \frac{\partial \mathbf{A}_1}{\partial \mathbf{X}}.
\end{align}
Разберем вторую производную по частям. Сначала вычислим~$\frac{\partial \mathbf{A}_1}{\partial \mathbf{X}}.$ Используя лемму~\ref{lemma:matrix_funcs_kronecker_product_derivative} получаем следующее выражение:
\begin{align}
    \frac{\partial \mathbf{A}_1}{\partial \mathbf{X}} &= \frac{\partial \left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right)}{\partial \mathbf{X}} =\\
    &=\left(\mathbf{I}_L \otimes \mathbf{K}_{L, L} \otimes \mathbf{I}_L \right) \Big((\mathbf{I}_{L^2} \otimes \mathrm{vec}_r(\mathbf{\mathbf{D}^{-\top}})) \cdot \frac{\partial -\mathbf{\mathbf{D}^{-1}}}{\partial \mathbf{X}}+ \\
    &\quad+ (\mathrm{vec}_r(-\mathbf{\mathbf{D}^{-1}}) \otimes \mathbf{I}_{L^2}) \cdot \frac{\partial \mathbf{\mathbf{D}^{-\top}}}{\partial \mathbf{X}} \Big).
\end{align}
Далее используя леммы~\ref{lemma:transposed_matrix_derivative}, \ref{lemma:invert_derivative} получаем выражение на:
\begin{align}
    \frac{\partial -\mathbf{\mathbf{D}^{-1}}}{\partial \mathbf{X}} &= \frac{\partial -\mathbf{\mathbf{D}^{-1}}}{\partial \mathbf{D}}\frac{\partial \mathbf{D}}{\partial \mathbf{X}} = \left( \mathbf{D}^{-1} \otimes \mathbf{D}^{-\top}\right) \frac{\partial \mathbf{D}}{\partial \mathbf{X}},\\
    \frac{\partial \mathbf{\mathbf{D}^{-\top}}}{\partial \mathbf{X}} &= \frac{\partial \mathbf{\mathbf{D}^{-\top}}}{\partial \mathbf{D}^{-1}}\frac{\partial \mathbf{\mathbf{D}^{-1}}}{\partial \mathbf{D}} \frac{\partial \mathbf{\mathbf{D}}}{\partial \mathbf{X}} = \mathbf{K}_{L, L} \left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right) \frac{\partial \mathbf{\mathbf{D}}}{\partial \mathbf{X}},
\end{align}
где $\frac{\partial \mathbf{\mathbf{D}}}{\partial \mathbf{X}}$ вычисляется аналогично тому, как в теореме~\ref{thm:layernorm_derivative}:
\begin{align}
    \frac{\partial \mathbf{\mathbf{D}}}{\partial \mathbf{X}} &= \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \dots   \mathbf{e}_L \otimes \mathbf{e}_L\Big)\cdot\\
    &\quad\cdot \left(\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\cdot (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\right),
\end{align}
заканчивая вывод оценки~$\frac{\partial \mathbf{A}_1}{\partial \mathbf{X}}.$

Перейдем к оценке~$\frac{\partial \mathbf{B}_1}{\partial \mathbf{X}}.$
Для начала снова представим матрицу~$\mathbf{B}_1$ в виде произведения матриц:
\begin{align}
    \mathbf{B}_1 = \mathbf{E} \mathbf{A}_2 \mathbf{B}_2,
\end{align}
где введены следующие обозначения матриц:
\begin{align}
    \mathbf{A}_2 &= \textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\\
    \mathbf{B}_2 &= (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\\
    \mathbf{E} &= \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big).
\end{align}
Для начала, заметим, что $\mathbf{E}$ является константной матрицей относительно матрицы~$\mathbf{X},$ а следовательно используя результат леммы~\ref{lemma:matrix_funcs_product_derivative} получаем
\begin{align}
    \frac{\partial\mathbf{B}_1}{\partial\mathbf{X}} &= \frac{\partial \mathbf{E} \mathbf{A}_2 \mathbf{B}_2}{\partial (\mathbf{A}_2 \mathbf{B}_2)} \frac{\partial \mathbf{A}_2 \mathbf{B}_2}{\partial \mathbf{X}} = \left( \mathbf{E} \otimes \mathbf{I}_{L d_V}\right)\frac{\partial \mathbf{A}_2 \mathbf{B}_2}{\partial \mathbf{X}} \\
    &= \left( \mathbf{E} \otimes \mathbf{I}_{L d_V}\right) \left( (\mathbf{A}_2\otimes \mathbf{I}_{L d_V} )\frac{\partial \mathbf{B}_2}{\partial \mathbf{X}} + (\mathbf{I}_L \otimes \mathbf{B}_2^\top) \frac{\partial \mathbf{A}_2}{\partial \mathbf{X}}\right).
\end{align}

Далее осталось оценить матрицы~$\frac{\partial\mathbf{A}_2}{\partial \mathbf{X}}$ и~$\frac{\partial\mathbf{B}_2}{\partial \mathbf{X}}.$
Для оценки~$\frac{\partial\mathbf{B}_2}{\partial \mathbf{X}}$ разобьем на части:
\begin{align}
    \mathbf{B}_2 = \mathbf{J} \mathbf{A}_3 \mathbf{B}_3,
\end{align}
где~$\mathbf{J} = (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})$, $\mathbf{A}_3 = \textit{diag}(\mathrm{vec}_r (\mathbf{M})), \mathbf{B}_3 = \frac{\partial \mathbf{M}}{\partial \mathbf{X}}$. Аналогично, используя лемму~\ref{lemma:matrix_funcs_product_derivative} получаем:
\begin{align}
    \frac{\partial \mathbf{B}_2}{\partial \mathbf{X}} &= \frac{\partial \mathbf{J} \mathbf{A}_3 \mathbf{B}_3}{\partial (\mathbf{A}_3 \mathbf{B}_3)} \frac{\partial \mathbf{A}_3 \mathbf{B}_3}{\partial \mathbf{X}} = \left( \mathbf{J} \otimes \mathbf{I}_{L d_V}\right)\frac{\partial \mathbf{A}_3 \mathbf{B}_3}{\partial \mathbf{X}} \\
    &= \left( \mathbf{J} \otimes \mathbf{I}_{L d_V}\right) \left((\mathbf{A}_3 \otimes \mathbf{I}_{Ld_V} ) \frac{\partial \mathbf{B}_3}{\partial \mathbf{X}} + ( \mathbf{I}_{Ld_V} \otimes \mathbf{B}_3^\top)\frac{\partial\mathbf{A}_3}{\partial \mathbf{X}}\right),
\end{align}
где
\begin{align}
    \frac{\partial\mathbf{A}_3}{\partial \mathbf{X}} &= \frac{\partial \textit{diag}(\mathrm{vec}_r(\mathbf{M}))}{\partial \mathbf{X}} = \frac{\partial \textit{diag}(\mathbf{v})}{\partial (\mathbf{v})} \frac{\partial \mathrm{vec}_r(\mathbf{M})}{\partial \mathbf{M}} \frac{\partial \mathbf{M}}{\partial \mathbf{X}},\\
    \frac{\partial \mathbf{B}_3}{\partial \mathbf{X}} &= \frac{\partial^2 \mathbf{M}}{\partial\mathbf{X}^2} = 0,
\end{align}
причем, используя лемму~\ref{lemma:diag_derivative} получаем, что~$\frac{\partial \textit{diag}(\mathbf{v})}{\partial (\mathbf{v})} = \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big),$ где $\mathbf{e}_i \in \mathbb{R}^{Ld_V \times 1}$, а также $\frac{\partial \mathrm{vec}_r(\mathbf{M})}{\partial \mathbf{M}}=\mathbf{I}_{L d_V}.$
Для вычисления матрицы~$\frac{\partial\mathbf{A}_2}{\partial \mathbf{X}}$ воспользуемся леммами~\ref{lemma:invert_derivative}, \ref{lemma:diag_derivative},\ref{lemma:hadamard_root_derivative},\ref{lemma:identification_theorem_vec_r} получаем выражение:
\begin{align}
    \textcolor{red}{\frac{\partial\mathbf{A}_2}{\partial\mathbf{X}} = \frac{\partial\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))}{\partial \mathbf{X}}=}
\end{align}

Собирая все полученные выражения воедино, получаем выражение для~$\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}$:
\begin{align}
    \frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2} &= \frac{1}{\sqrt{d_V}} \left(\mathbf{A}_1 \otimes  \mathbf{I}_{Ld_V}  \right) \frac{\partial \mathbf{B}_1}{\partial \mathbf{X}} + \left( \mathbf{I}_{L^2} \otimes  \mathbf{B}_1^\top \right) \frac{\partial \mathbf{A}_1}{\partial \mathbf{X}},
\end{align}
где~$\frac{\partial \mathbf{B}_1}{\partial \mathbf{X}},\frac{\partial \mathbf{A}_1}{\partial \mathbf{X}}, \mathbf{B}_1, \mathbf{A}_1$ определены и получены выше.

Итого, все матрицы выражения~\eqref{chapter-2:theorem:layernorm_second_derivative:eqstemant} вычислены, что заканчивает доказательство.
\end{proof}

\subsection{Матрица Гессе для нелинейности ReLU}

\begin{theorem}\label{theorem:relu_derivative_hessian}
Пусть задана матрица~$\mathbf{X} \in \mathbb{R}^{m \times n},$ тогда для оператора~$\mathrm{ReLU}$ почти всюду верно следующее выражение:
\begin{align}
    \frac{\partial \mathrm{ReLU}(\mathbf{X})}{\partial \mathbf{X}} &= \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big), \\
    \frac{\partial^2 \mathrm{ReLU}(\mathbf{X})}{\partial \mathbf{X}^2} &= \mathbf{0}.
\end{align}
\end{theorem}
\begin{proof}
Оператор~$\mathrm{ReLU}$ принимает следующий вид:
\[
    \mathrm{ReLU}(x) = \max(0, x),
\]
то есть, для каждого элемента~$x_{ij}$ в матрице~$\mathbf{X} \in \mathbb{R}^{m \times n}$ получаем:
\[
    \frac{\partial \mathrm{ReLU}(x_{ij})}{\partial x_{ij}} =
    \begin{cases}
    1 & \text{если}~x_{ij} > 0, \\
    0 & \text{если}~x_{ij} < 0, \\
    \text{неопределенно (субградиент }\in[0,1]\text{)} & \text{если}~x_{ij} = 0.
    \end{cases}
\]
В случае скалярной величины~$x \in \mathbb{R},$ множеством с неопределенным градиентом является множество~$\{0\},$ которое является множеством меры 0.
Рассматривая же матрицу~$\mathbf{X} \in \mathbb{R}^{m \times n}$ как точку в~$\mathbb{R}^{m\times n},$ дифференцируемым множеством является множество:
\[
    \mathcal{N} = \bigcup_{i,j} \left\{ \mathbf{X} \in \mathbb{R}^{m \times n} : x_{ij} = 0 \right\}.
\]
Заметим, что каждое множество~$\{x_{ij} = 0\}$ является гиперплоскостью коразмерности~$1$ в пространстве~$\mathbb{R}^{m\times n},$ а следовательно является множеством меры~$0$.
Так как, множество~$\mathcal{N}$ является конечным объединением множеств меры~$0,$ то и множество~$\mathcal{N}$ также имеет меру~$0.$ Получили, что оператор~$\mathrm{ReLU}$ является почти всюду дифференцируем в пространстве~$\mathbb{R}^{m \times n}.$

Для каждой дифференцируемой точки~$\mathbf{X} \notin \mathcal{N},$ применим построчкую векторизацию и лемму~\ref{lemma:identification_theorem_vec_r}:
\begin{align}
    \mathrm{vec}_r(d\mathrm{ReLU}(\mathbf{X}))
    = \mathrm{diag}(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}}))  \mathrm{vec}_r(d\mathbf{X}),
\end{align}
причем, используя свойство~\ref{prop:vec_r_hadamard_product} и лемму~\ref{lemma:diag_derivative} для диагональной матрицы получаем:
\begin{align}
    \frac{\partial \mathrm{ReLU}(\mathbf{X})}{\partial \mathbf{X}}
= \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big).
\end{align}

В силу того, что матрица Якоби является кусочно-постоянной, то ее дифференциал равен нулю почти всюду:
\[
    d\left(\frac{\partial \mathrm{ReLU}(\mathbf{X})}{\partial \mathbf{X}}\right) = \mathbf{0}, \quad \mathbf{X} \notin \mathcal{N},
\]
а следовательно и матрица Гессе почти всюду равна нулевой матрице:
\[
    \frac{\partial^2 \mathrm{ReLU}(\mathbf{X})}{\partial \mathbf{X}^2} = \mathbf{0}, \quad \mathbf{X} \notin \mathcal{N}.
\]
\end{proof}

\subsection{Матрица Гессе для трансформера}

\begin{theorem}\label{thm:transformer_derivative}
Для модели глубокого обучения архитектуры трансформер~\ref{eq:transformer} матрица Якоби~$\frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i}$ вычисляется в следующем виде:
\begin{align}
    \frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i} &= \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{W}_i},\qquad i \in \{1,2\},
\end{align}
где
\begin{equation}
    \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{W}_i} = \begin{cases}
        \left(\mathbf{I}_L \otimes \mathbf{W}_2^\top \right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{Y} \otimes \mathbf{I}_{d_{ff}}\right), & \text{for } i = 1 \\
        \sigma(\mathbf{Y} \mathbf{W}_1) \otimes \mathbf{I}_{d_V}, & \text{for } i = 2
    \end{cases},
\end{equation}
причем~$\frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}$ вычисляется согласно теоремы~\ref{thm:layernorm_derivative}.
\begin{align}
    \frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i} &= \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{Y}} \frac{\partial \mathbf{Y}}{\partial \mathbf{W}_i}, \qquad i \in \{ K, Q, V\},
\end{align}
где
\begin{align}
    \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{Y}} &= \left( \mathbf{I}_L \otimes \mathbf{W}_2^\top\right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{I}_L \otimes \mathbf{W}_1^\top \right) + \left( \mathbf{I}_L \otimes \mathbf{I}_{d_V}\right),
\end{align}
причем~$\frac{\partial \mathbf{Y}}{\partial\mathbf{W}_i} = \frac{\partial \text{LayerNorm}(\mathbf{F} (\mathbf{X}) + \mathbf{X})}{\partial (\mathbf{F} (\mathbf{X}) + \mathbf{X})} \frac{\partial \mathbf{F}(\mathbf{X})}{\partial \mathbf{W}_i},$ где $\frac{\partial \mathbf{F}(\mathbf{X})}{\partial \mathbf{W}_i}$ вычисляется при помощи леммы~A.2 в работе~\cite{noci2022signalpropagationtransformerstheoretical}, а матрица~$\frac{\partial \text{LayerNorm}(\mathbf{F} (\mathbf{X}) + \mathbf{X})}{\partial (\mathbf{F} (\mathbf{X}) + \mathbf{X})}$ вычисляется согласно теоремы~\ref{thm:layernorm_derivative}.
\end{theorem}
\begin{proof}
В дальнейшем при доказательстве вводим следующие обозначения и предположения~$\mathbf{X} \in R^{L \times d_V}, \mathbf{Y} \in R^{L\times d_V}, \mathbf{W}_1 \in R^{d_V \times d_{ff}}, \text{ReLU}(\mathbf{Y\mathbf{W}_1}) \in R^{L \times d_{ff}}, \mathbf{W}_2 \in R^{d_{ff} \times d_V}.$
Трансформер блок определен в выражении~\eqref{eq:transformer}, а именно:
\begin{align}
    \mathbf{Y} &= \text{LayerNorm}(\mathbf{F}(\mathbf{X}) + \mathbf{X}),\\
    \mathbf{Z} &= \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y}).
\end{align}

Начнем вычисления матрицы Гессе для полного трансформера с матриц~$\frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i},$ тогда для~$i \in \{1,2\}$ получаем:
\begin{equation}
    \frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i} = \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{W}_i},
\end{equation}
где
\begin{equation}
    \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{W}_i} = \frac{\partial (\text{FFN}(\mathbf{Y}))}{\partial \mathbf{W}_i} = \frac{\partial \mathbf{I}_L \sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2 \mathbf{I}_{d_V}}{\partial \mathbf{W}_i},
\end{equation}
причем используя свойство~\ref{prop:matrix_product_derivative} об производной произведения матриц получаем:
\begin{align}
    \frac{\partial \mathbf{I}_L \sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2 \mathbf{I}_{d_V}}{\partial \mathbf{W}_2} &= \sigma(\mathbf{Y} \mathbf{W}_1) \otimes \mathbf{I}_{d_V}\\
    \frac{\partial \mathbf{I}_L \sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2 \mathbf{I}_{d_V}}{\partial \mathbf{W}_1} &= \frac{\partial \sigma(\mathbf{Y}\mathbf{W}_1) \mathbf{W}_2}{\partial \sigma(\mathbf{Y}\mathbf{W}_1)} \frac{\partial \sigma(\mathbf{Y}\mathbf{W}_1)}{\partial \mathbf{Y}\mathbf{W}_1} \frac{\partial \mathbf{Y}\mathbf{W}_1}{\partial \mathbf{W}_1} =\\
    &= \left(\mathbf{I}_L \otimes \mathbf{W}_2^\top \right) \frac{\partial \sigma(\mathbf{Y}\mathbf{W}_1)}{\partial \mathbf{Y}\mathbf{W}_1} \left( \mathbf{I}_L \otimes \mathbf{W}_1^\top\right).
\end{align}
Используя результаты теоремы~\ref{theorem:relu_derivative_hessian} для производной оператора ReLU для матрицы~$\frac{\partial \sigma(\mathbf{Y}\mathbf{W}_1)}{\partial \mathbf{Y}\mathbf{W}_1}$ получаем следующее выражение:
\begin{align}
    \frac{\partial \mathbf{I}_L \sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2 \mathbf{I}_{d_V}}{\partial \mathbf{W}_i} = \left(\mathbf{I}_L \otimes \mathbf{W}_2^\top \right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{Y} \otimes \mathbf{I}_{d_{ff}}\right).
\end{align}
Тогда в общем виде для~$i \in \{1, 2\}$ получаем следующее выражение: 
\begin{equation}
    \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{W}_i} = \begin{cases}
        \left(\mathbf{I}_L \otimes \mathbf{W}_2^\top \right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{Y} \otimes \mathbf{I}_{d_{ff}}\right), \text{если}~i = 1 \\
        \sigma(\mathbf{Y} \mathbf{W}_1) \otimes \mathbf{I}_{d_V}, \text{если}~i = 2
    \end{cases}.
\end{equation}
Тогда весь блок тронсформера имеет следующую производную:
\begin{equation}
    \frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i} = 
    \begin{cases}
        \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}\left(\mathbf{I}_L \otimes \mathbf{W}_2^\top \right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{Y} \otimes \mathbf{I}_{d_{ff}}\right), i = 1 \\
        \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}\sigma(\mathbf{Y} \mathbf{W}_1) \otimes \mathbf{I}_{d_V}, i = 2
    \end{cases}
\end{equation}
причем, согласно теореме~\ref{thm:layernorm_derivative} об производной LayerNorm получаем следующее выражение в нашем случае:
\begin{align}
    &\frac{\partial\text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} =\\
    &=( \mathbf{P}(\text{FFN}(\mathbf{Y}) + \mathbf{Y}) \otimes \mathbf{I}_{d_V}) \frac{\partial \mathbf{M}}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} +\\
    &\quad+ (\mathbf{I}_L\otimes \mathbf{M}^\top)\frac{1}{\sqrt{d_V}}\left(-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top} \right) \Big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots  \quad \mathbf{e}_L \otimes \mathbf{e}_L\Big) \cdot\\
    &\qquad\cdot \left(\textit{diag}^{-1}(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{M}^{\circ{2}}\cdot\mathbf{1}_{d_V}))\cdot (\mathbf{I}_L \otimes \mathbf{1}^T_{d_V})\cdot \textit{diag}(\mathrm{vec}_r (\mathbf{M}))\frac{\partial \mathbf{M}}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}\right),
\end{align}
где
\begin{align}
    \mathbf{M}(\text{FFN}(\mathbf{Y}) + \mathbf{Y}) &= ((\text{FFN}(\mathbf{Y}) + \mathbf{Y}) - \frac{1}{d_V}(\text{FFN}(\mathbf{Y}) + \mathbf{Y}) \mathbf{1}_{d_V \times d_V}),\\
    \mathbf{P}((\text{FFN}(\mathbf{Y}) + \mathbf{Y})) &= \textit{diag}^{-1}(\sigma(\text{FFN}(\mathbf{Y}) + \mathbf{Y}),\\
    \frac{\partial \mathbf{M}}{\partial(\text{FFN}(\mathbf{Y}) + \mathbf{Y})} &= (\mathbf{I}_L \otimes \mathbf{I}_{d_V}) - \frac{1}{d_V}(\mathbf{I}_L \otimes \mathbf{1}_{d_V \times d_V}),
\end{align}
где~$\sigma$ вычисляется согласно определению оператору LayerNorm.

Далее, перейдем к вычислению матриц Гессе~$\frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i}$ для~$i \in \{ K, Q, V\},$ где получаем:
\begin{align}
    \frac{\partial\mathbf{Z}}{\partial \mathbf{W}_i} = \frac{\partial \text{LayerNorm}(\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})} \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{Y}} \frac{\partial \mathbf{Y}}{\partial \mathbf{W}_i},
\end{align}
где используя свойство~\ref{prop:matrix_product_derivative} и результат теоремы~\ref{theorem:relu_derivative_hessian} получаем:
\begin{align}
    \frac{\partial (\text{FFN}(\mathbf{Y}) + \mathbf{Y})}{\partial \mathbf{Y}} &= \frac{\partial \text{FFN}(\mathbf{Y}) }{\partial \mathbf{Y}} + \frac{\partial \mathbf{Y}}{\partial \mathbf{Y}} =\\
    &= \frac{\partial \text{FFN}(\mathbf{Y}) }{\partial \mathbf{Y}} + \left( \mathbf{I}_L \otimes \mathbf{I}_{d_V}\right) =\\
    &=\frac{\partial \sigma(\mathbf{Y} \mathbf{W}_1) \mathbf{W}_2}{\partial \mathbf{Y}} +\left( \mathbf{I}_L \otimes \mathbf{I}_{d_V}\right)=\\ 
    &= \left( \mathbf{I}_L \otimes \mathbf{W}_2^\top\right) \frac{\partial \sigma (\mathbf{Y} \mathbf{W}_1)}{\partial \mathbf{Y}\mathbf{W}_1} \frac{\partial \mathbf{Y}\mathbf{W}_1}{\partial \mathbf{Y}} + \left( \mathbf{I}_L \otimes \mathbf{I}_{d_V}\right) = \\
    & = \left( \mathbf{I}_L \otimes \mathbf{W}_2^\top\right) \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{X}>0\}})\big) \left( \mathbf{I}_L \otimes \mathbf{W}_1^\top \right) + \left( \mathbf{I}_L \otimes \mathbf{I}_{d_V}\right).
\end{align}
Для вычисления матрицы~$\frac{\partial \mathbf{Y}}{\partial \mathbf{W}_i}$ используем результат леммы~A.2 с работы~\cite{noci2022signalpropagationtransformerstheoretical}:
\begin{align}
    \frac{\partial \mathbf{F}}{\partial \mathbf{W}_V} &= \text{softmax}\left(\frac{\mathbf{X}\mathbf{W}_Q\mathbf{W}_{K}^{\top}\mathbf{X}^\top}{\sqrt{d_K}}\right) \mathbf{X} \otimes \mathbf{I}_{d_V}\\
    \frac{\partial \mathbf{F}}{\partial \mathbf{W}_Q} &= \left(\mathbf{I}_L \otimes \mathbf{W}_{V}^{\top}\mathbf{X}^\top\right) \frac{\partial \mathbf{A}}{\partial \mathbf{M}} \left(\frac{\mathbf{X} \otimes \mathbf{X}\mathbf{W}_K}{\sqrt{d_K}}\right),
\end{align}
где
\begin{equation}
    \frac{\partial \mathbf{A}}{\partial \mathbf{M}} = \text{blockdiag}\left(\frac{\partial \mathbf{A}_i}{\partial \mathbf{M}_i^\top}\right),
\end{equation}
причем данное выражение сильно упрощается используя свойства матрицы~$\mathbf{A}$:
\begin{align}
    \frac{\partial \mathbf{A}_i}{\partial \mathbf{M}_i^\top} = \text{diag}(\mathbf{A}_i) - \mathbf{A}_i\mathbf{A}_i^\top,
\end{align}
где~$\mathbf{A}_i$ является $i$-й строкой матрицы~$\mathbf{A}$ в формате вектора. Итого в условия равномерного внимания (англ. uniform-attention) данное выражение упрощается до:
\begin{equation}
    \frac{\partial \mathbf{A}}{\partial \mathbf{M}} = \frac{1}{n}\mathbf{I}_L \otimes \left(\mathbf{I}_L - \frac{1}{L}\mathbf{1}_{L \times L}\right)
\end{equation}
Аналогично, используя лемму~\ref{lemma:transposed_matrix_derivative} вычисляем производну относительно матрицы~$\mathbf{W}_K$:  
\begin{align}
    \frac{\partial \mathbf{F}}{\partial \mathbf{W}_K} &= \left(\mathbf{I}_L \otimes \mathbf{W}_{V}^{\top}\mathbf{X}^\top\right) \frac{\partial \mathbf{A}}{\partial \mathbf{M}} \left(\frac{(\mathbf{X} \mathbf{W}_Q \otimes \mathbf{X})\mathbf{K}_{d_V d_K}}{\sqrt{d_k}}\right).
\end{align}
Получаем, что матрица~$\frac{\partial \mathbf{Y}}{\partial\mathbf{W}_i}$ для~$i \in \{ K, Q, V\}$ вычисляется следующим образом:
\begin{align}
    \frac{\partial\mathbf{Y}}{\partial\mathbf{W}_i} &= \frac{\partial \text{LayerNorm}(\mathbf{F} (\mathbf{X}) + \mathbf{X})}{\partial \mathbf{W}_i} =\\
    &=\frac{\partial\text{LayerNorm}(\mathbf{F} (\mathbf{X}) + \mathbf{X})}{\partial (\mathbf{F} (\mathbf{X}) + \mathbf{X})} \frac{\partial \mathbf{F}(\mathbf{X})}{\partial \mathbf{W}_i},
\end{align}
где~$\frac{\partial \mathbf{F}(\mathbf{X})}{\partial \mathbf{W}_i}$ вычисляется согласно леммы~A.2 с работы~\cite{noci2022signalpropagationtransformerstheoretical}, а матрица~$\frac{\partial \text{LayerNorm}(\mathbf{F} (\mathbf{X}) + \mathbf{X})}{\partial (\mathbf{F} (\mathbf{X}) + \mathbf{X})}$ вычисляется согласно теоремы~\ref{thm:layernorm_derivative}.
\end{proof}

В теореме~\ref{thm:transformer_derivative} получен вид матрицы Якоби для полного блока трансформера, теперь можно перейти к вычислению матрицы Гессе для, который получен в виде теоремы~\ref{thm:transformer_hessian}.

\begin{theorem}\label{thm:transformer_hessian}
Пусть заданы матрицы параметров модели трансформера~$\mathbf{X} \in \mathbb{R}^{L \times d_V}$, $\mathbf{Y} \in \mathbb{R}^{L \times d_V}$, $\mathbf{W}_1 \in \mathbb{R}^{d_V \times d_{ff}}$, $\mathbf{W}_2 \in \mathbb{R}^{d_{ff} \times d_V}$, $\mathbf{W}_Q, \mathbf{W}_K \in \mathbb{R}^{d_V \times d_K}$, $\mathbf{W}_V \in \mathbb{R}^{d_V \times d_V},$
где блок трансформатор описан в виде следующих матричнозначных функций:
\[
    \mathbf{S}(\mathbf{Y},\mathbf{W}_1,\mathbf{W}_2) = \sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2 + \mathbf{Y} \in \mathbb{R}^{L \times d_V}, \qquad\mathbf{Z} = \mathrm{LayerNorm}(\mathbf{S}) \in \mathbb{R}^{L \times d_V},
\]
для которых в условиях теорем~\ref{thm:layernorm_derivative} и \ref{thm:layernorm_second_derivative} вычислимые матрицы Якобы и Гессе вида:
\[
    \mathbf{J}_Z := \frac{\partial\mathrm{LayerNorm}(\mathbf{S})}{\partial \mathbf{S}} \in \mathbb{R}^{L d_V \times L d_V}, \quad\mathbf{H}_Z := \frac{\partial^2\mathrm{LayerNorm}(\mathbf{S})}{\partial \mathbf{S}^2} \in \mathbb{R}^{(L d_V)^2 \times L d_V}.
\]
Также в условиях теорем~\ref{thm:layernorm_derivative}, \ref{thm:layernorm_second_derivative} и \ref{theorem:relu_derivative_hessian} введем следующее:
\begin{align}
    \mathbf{D}_\sigma := \mathrm{diag}\big(\mathrm{vec}_r(\mathbf{1}_{\{\mathbf{Y}\mathbf{W}_1>0\}})\big) \in \mathbb{R}^{L d_{ff} \times L d_{ff}},\\
    \mathbf{J}_{SY} := \frac{\partial \mathbf{S}}{\partial \mathbf{Y}} = (\mathbf{I}_L \otimes \mathbf{W}_2^\top)\mathbf{D}_\sigma (\mathbf{I}_L \otimes \mathbf{W}_1^\top) + (\mathbf{I}_L \otimes \mathbf{I}_{d_V}) \in \mathbb{R}^{L d_V \times L d_V},
\end{align}
где для матрицы~$\mathbf{Y} = \mathrm{LayerNorm}(\mathbf{F}(\mathbf{X}) + \mathbf{X})$ в условиях теорем~~\ref{thm:layernorm_derivative}, \ref{thm:layernorm_second_derivative} определено:
\begin{align}
    \mathbf{J}_Y &:= \frac{\partial \mathrm{LayerNorm}(\mathbf{F}(\mathbf{X})+\mathbf{X})}{\partial (\mathbf{F}(\mathbf{X})+\mathbf{X})} \in \mathbb{R}^{L d_V \times L d_V}, \\
    \mathbf{H}_Y &:= \frac{\partial^2 \mathrm{LayerNorm}(\mathbf{F}(\mathbf{X})+\mathbf{X})}{\partial (\mathbf{F}(\mathbf{X})+\mathbf{X})^2} \in \mathbb{R}^{(L d_V)^2 \times L d_V},
\end{align}
где для удобства введем следующие обозначения:~ $n_1 = d_V d_{ff}, n_2 = d_{ff} d_V, n_Q = n_K = d_V d_K, n_V = d_V^2.$
Пусть матрицы Якоби вычислимы в условиях теоремы~\ref{thm:transformer_derivative} в следующем виде:
\begin{align}
     \mathbf{G}_V &:= \frac{\partial \mathbf{F}}{\partial \mathbf{W}_V} \in \mathbb{R}^{L d_V \times n_V},\\
     \mathbf{G}_Q &:= \frac{\partial \mathbf{F}}{\partial \mathbf{W}_Q} \in \mathbb{R}^{L d_V \times n_Q},\\
     \mathbf{G}_K &:= \frac{\partial \mathbf{F}}{\partial \mathbf{W}_K} \in \mathbb{R}^{L d_V \times n_K},\\
     \mathbf{B}_1 &:= \frac{\partial \mathbf{S}}{\partial \mathbf{W}_1} = (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \in \mathbb{R}^{L d_V \times n_1},\\
     \mathbf{B}_2 &:= \frac{\partial \mathbf{S}}{\partial \mathbf{W}_2} = \sigma(\mathbf{Y}\mathbf{W}_1) \otimes \mathbf{I}_{d_V} \in \mathbb{R}^{L d_V \times n_2},\\
     \mathbf{B}_k &:= \frac{\partial \mathbf{S}}{\partial \mathbf{W}_k} = \mathbf{J}_{SY} \mathbf{J}_Y \mathbf{G}_k \in \mathbb{R}^{L d_V \times n_k}, \quad k \in \{K,Q,V\}.
\end{align}

Тогда матрицы Гессе трансформера~$\mathbf{Z}$ по параметрам модели~$(\mathbf{W}_i,\mathbf{W}_j)$ задается в виде:
\begin{equation}\label{eq:block_hessian_transformer}
    \;\mathbf{H}_{\mathrm{tr}}^{(i,j)} := \frac{\partial^2 \mathbf{Z}}{\partial \mathbf{W}_i \partial \mathbf{W}_j}
    = \left( \mathbf{J}_Z \otimes \mathbf{I}_{n_i} \right) \boldsymbol{\xi}_{ij}
      + \left( \mathbf{I}_{L d_V} \otimes \mathbf{B}_i^\top \right) \mathbf{H}_Z \mathbf{B}_j,
\end{equation}
где размерность матрицы Гессе~$\mathbf{H}_{\mathrm{tr}}^{(i,j)} \in \mathbb{R}^{(L d_V \cdot n_i) \times n_j},$ также введены дополнительные матрицы для удобства:
\[
    \boldsymbol{\xi}_{ij} := \frac{\partial}{\partial \mathbf{W}_j} \left( \frac{\partial \mathbf{S}}{\partial \mathbf{W}_i} \right) \in \mathbb{R}^{(L d_V \cdot n_i) \times n_j}.
\]
Матрицы $\boldsymbol{\xi}_{ij}$ вычисляются для всех пар~$(i,j)$ почти всюду.

Для пар FFN:
\begin{align}
    \boldsymbol{\xi}_{11} &= \mathbf{0}_{(L d_V \cdot n_1) \times n_1}, \\
    \boldsymbol{\xi}_{22} &= \mathbf{0}_{(L d_V \cdot n_2) \times n_2}, \\
    \boldsymbol{\xi}_{12} &= \left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V} \right) \left( \mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V}) \right)\left( \mathbf{D}_\sigma  (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \right),\\
    \boldsymbol{\xi}_{21} &= \left( \mathbf{I}_{L d_V} \otimes \left( (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})^\top \mathbf{D}_\sigma^\top \right) \right)\left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, L} \otimes \mathbf{I}_{d_{ff}} \right) \left( \mathrm{vec}_r(\mathbf{I}_L) \otimes \mathbf{I}_{d_V d_{ff}} \right) \mathbf{K}_{d_{ff}, d_V},
\end{align}
где матрицы~$\boldsymbol{\xi}_{12},\boldsymbol{\xi}_{21}$ имеют размерности $(L d_V \cdot n_1) \times n_2$ и $(L d_V \cdot n_2) \times n_1$ соответственно.

Для пар FFN с параметрами слоев внимания для всех~$k \in \{K,Q,V\}$:
\begin{align}
    \boldsymbol{\xi}_{1k} &= \left( (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma \otimes \mathbf{I}_{n_k}\right)\left( \mathbf{I}_L \otimes \mathbf{K}_{d_{ff}, d_V} \otimes \mathbf{I}_{d_{ff}} \right)\left( \mathbf{I}_{L d_V} \otimes \mathrm{vec}_r(\mathbf{I}_{d_{ff}}) \right) \left( \mathbf{J}_Y \mathbf{G}_k \right),\\
    \boldsymbol{\xi}_{2k} &= \left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V} \right)\left( \mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V}) \right)\left( \mathbf{D}_\sigma (\mathbf{I}_L \otimes \mathbf{W}_1^\top)  \mathbf{J}_Y \mathbf{G}_k \right),
\end{align}
где размерности матрицы~$\boldsymbol{\xi}_{1k} \in \mathbb{R}^{(L d_V \cdot n_1) \times n_k}$ и матрицы~$\boldsymbol{\xi}_{2k} \in \mathbb{R}^{(L d_V \cdot n_2) \times n_k}$.

Для пар слоев внимания~$k,\ell \in \{K,Q,V\}$:
\begin{align}
    \boldsymbol{\xi}_{k\ell} 
    = \left( \mathbf{J}_{SY} \otimes \mathbf{I}_{n_k} \right)
    \left[\left( \mathbf{I}_{L d_V} \otimes \mathbf{G}_k^\top \right) \left( \mathbf{H}_Y \mathbf{G}_\ell \right)+ \left( \mathbf{J}_Y \otimes \mathbf{I}_{n_k} \right) \boldsymbol{\Phi}_{k\ell}\right],
\end{align}
где~$\boldsymbol{\Phi}_{k\ell} := \frac{\partial \mathbf{G}_k}{\partial \mathbf{W}_\ell} \in \mathbb{R}^{(L d_V \cdot n_k) \times n_\ell}$ является второй производной слоя внимания~$\mathbf{F}$ по ее параметрам, которые вычислены в рамках леммы~\ref{lemma:attention_phi_from_functional_hessian}. Все матрицы имеют следующие размерности~$\boldsymbol{\xi}_{k\ell} \in \mathbb{R}^{(L d_V \cdot n_k) \times n_\ell}$.

Также матрица Гессе удовлетворяет следующим свойствам почти везде:
\[
    \mathbf{H}_{\mathrm{tr}}^{(i,j)} = \mathbf{H}_{\mathrm{tr}}^{(j,i)},
\]
так как, во первых единственные нелинейности с потенциально ненулевым вторым дифференциалом является оператор LayerNorm, для которого получены матрицы~$\mathbf{H}_Z,\mathbf{H}_Y$ в рамках теоремы~\ref{thm:layernorm_second_derivative} и которые являются симметричными по построению и оператор ReLU, для которого матрица Гессе является нулевой согласно теоремы~\ref{theorem:relu_derivative_hessian}, а во вторых все другие отображения являются линейными, а следовательно согласно леммы~\ref{lemma:matrix_funcs_product_derivative} и свойства производной произведения Кронекера их частные производные являются коммутативными почти всюду.
\end{theorem}
\begin{proof}
Вычислим производной матрицы Якоби с теоремы~\ref{thm:transformer_derivative} используя лемму~\ref{lemma:matrix_funcs_product_derivative} об производной матричного умножения, также свойство производной произведения Кронекера~\ref{prop:kronecker_product_derivative}, также лемму~\ref{lemma:transposed_matrix_derivative} об производной транспонированной матрицы, лемму~\ref{lemma:identification_theorem_vec_r} и теорему~\ref{theorem:relu_derivative_hessian}. Для удобства доказательства разделим его на 4 шага.

На 1-м шаге для всех $i \in \{1,2,K,Q,V\}$ получаем:
\begin{align}
    \frac{\partial \mathbf{Z}}{\partial \mathbf{W}_i} \;=\; \mathbf{J}_Z  \mathbf{B}_i,\qquad\mathbf{J}_Z \in \mathbb{R}^{L d_V \times L d_V},
\end{align}
где $\mathbf{B}_i := \frac{\partial \mathbf{S}}{\partial \mathbf{W}_i}$ задается следующим образом:
\begin{align}
    \mathbf{B}_1 &= (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \in \mathbb{R}^{L d_V \times n_1}, \\
    \mathbf{B}_2 &= \sigma(\mathbf{Y}\mathbf{W}_1) \otimes \mathbf{I}_{d_V} \in \mathbb{R}^{L d_V \times n_2},\\
    \mathbf{B}_k &= \mathbf{J}_{SY} \mathbf{J}_Y \mathbf{G}_k \in \mathbb{R}^{L d_V \times n_k},\qquad k \in \{K,Q,V\},
\end{align}
где матрица~$\mathbf{J}_{SY}$ вычисляется следующим образом:
\begin{align}
    \mathbf{J}_{SY} = \frac{\partial \mathbf{S}}{\partial \mathbf{Y}} = (\mathbf{I}_L \otimes \mathbf{W}_2^\top)\mathbf{D}_\sigma(\mathbf{I}_L \otimes \mathbf{W}_1^\top) + (\mathbf{I}_L \otimes \mathbf{I}_{d_V}) \in \mathbb{R}^{L d_V \times L d_V},
\end{align}
матрица имеет следующую размерность~$\mathbf{J}_Y \in \mathbb{R}^{L d_V \times L d_V},$ а матрица~$\mathbf{G}_k$ описана в теореме~\ref{thm:transformer_derivative}.
Используя лемму~\ref{lemma:matrix_funcs_product_derivative} и теорему~\ref{thm:layernorm_second_derivative} получаем выражение для блока матрицы Гессе:
\begin{align}
     \frac{\partial^2 \mathbf{Z}}{\partial \mathbf{W}_i \partial \mathbf{W}_j} &= \left( \mathbf{J}_Z \otimes \mathbf{I}_{n_i} \right) \boldsymbol{\xi}_{ij}  + \left( \mathbf{I}_{L d_V} \otimes \mathbf{B}_i^\top \right) \mathbf{H}_Z \mathbf{B}_j,\\
     \boldsymbol{\xi}_{ij} &:= \frac{\partial \mathbf{B}_i}{\partial \mathbf{W}_j} \in \mathbb{R}^{(L d_V \cdot n_i) \times n_j}.
\end{align}


На 2-м шаге вычисляем размерности и вид матриц ~$\mathbf{B}_i.$ Используя результаты теорем~\ref{thm:transformer_derivative} и~\ref{theorem:relu_derivative_hessian} получаем следующие выражения:
\begin{align}
    \mathbf{B}_1 &= (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \in \mathbb{R}^{L d_V \times n_1}, \\
    \mathbf{B}_2 &= \sigma(\mathbf{Y}\mathbf{W}_1) \otimes \mathbf{I}_{d_V} \in \mathbb{R}^{L d_V \times n_2},
\end{align}
где матрица~$\mathbf{D}_\sigma \in \mathbb{R}^{L d_{ff} \times L d_{ff}}$, матрица~$(\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \in \mathbb{R}^{L d_{ff} \times d_V d_{ff}}$.
Тогда для всех матриц~$\mathbf{B}_k,$ где~$k \in \{K,Q,V\}$ получаем:
\begin{align}
    \mathbf{B}_k = \mathbf{J}_{SY} \mathbf{J}_Y  \mathbf{G}_k \in \mathbb{R}^{L d_V \times n_k}.
\end{align}

На 3-м шаге вычисляем вычисляем матрицы~$\boldsymbol{\xi}_{ij}$ для всех пар~$(i, j)$.

Начнем вычисления с пар FFN. Заметим, что матрица~$\mathbf{B}_1$ не зависит от матрицы~$\mathbf{W}_1,$ а следовательно~$\boldsymbol{\xi}_{11} = \mathbf{0}.$ Аналогично матрица~$\mathbf{B}_2$ не зависит от матрицы~$\mathbf{W}_2,$ а следовательно~$\boldsymbol{\xi}_{22} = \mathbf{0}.$ Вычислим~$\frac{\partial \mathbf{B}_2}{\partial \mathbf{W}_1}$ используя свойство~\ref{prop:kronecker_product_derivative} производной произведения Кронекера для $\frac{\partial (\mathbf{X} \otimes \mathbf{Y})}{\partial \mathbf{X}},$ где $\mathbf{X}=\sigma(\mathbf{Y}\mathbf{W}_1)$ и $\mathbf{Y}=\mathbf{I}_{d_V}$:
\begin{align}
    \frac{\partial \mathbf{B}_2}{\partial \mathbf{W}_1}=\left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V} \right)\left( \mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V}) \right)\frac{\partial \mathrm{vec}_r(\sigma(\mathbf{Y}\mathbf{W}_1))}{\partial \mathbf{W}_1},
\end{align}
далее используя то, что $\frac{\partial\mathrm{vec}_r(\sigma(\mathbf{Y}\mathbf{W}_1))}{\partial \mathbf{W}_1} = \mathbf{D}_\sigma(\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})$ получаем оценку на~$\boldsymbol{\xi}_{12}:$
\begin{align}
    \boldsymbol{\xi}_{12}= \left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V} \right) \left( \mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V}) \right)\left( \mathbf{D}_\sigma  (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}) \right).
\end{align}
Вычислим~$\frac{\partial \mathbf{B}_1}{\partial \mathbf{W}_2}$ используя лемму~\ref{lemma:matrix_funcs_product_derivative}, где в качестве левого множителя выступает~$(\mathbf{I}_L \otimes \mathbf{W}_2^\top)$:
\begin{align}
    \frac{\partial\mathbf{B}_1}{\partial \mathbf{W}_2}= \left( \mathbf{I}_{L d_V} \otimes \left( (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})^\top \mathbf{D}_\sigma^\top \right) \right) \frac{\partial(\mathbf{I}_L \otimes \mathbf{W}_2^\top)}{\partial \mathbf{W}_2},
\end{align}
далее используя свойство~\ref{prop:kronecker_product_derivative} об производной произведения Кронекера и лемму~\ref{lemma:transposed_matrix_derivative} об производной транспонированной матрицы получем:
\begin{align}
    \frac{\partial(\mathbf{I}_L \otimes \mathbf{W}_2^\top)}{\partial \mathbf{W}_2}= \left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, L} \otimes \mathbf{I}_{d_{ff}} \right) \left( \mathrm{vec}_r(\mathbf{I}_L) \otimes \mathbf{I}_{d_V d_{ff}} \right) \mathbf{K}_{d_{ff}, d_V}.
\end{align}
Далее собирая все полученные матрицы, получаем оценку на~$\boldsymbol{\xi}_{21}:$
\begin{align}
    \boldsymbol{\xi}_{21} = \left( \mathbf{I}_{L d_V} \otimes \left( (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})^\top \mathbf{D}_\sigma^\top \right) \right)\left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, L} \otimes \mathbf{I}_{d_{ff}} \right) \left( \mathrm{vec}_r(\mathbf{I}_L) \otimes \mathbf{I}_{d_V d_{ff}} \right) \mathbf{K}_{d_{ff}, d_V}.
\end{align}

Перейдем к оценке пар FFN с параметрами слоев внимания для всех $k \in \{K, Q, V\}.$ Для матрицы~$\mathbf{B}_1 = (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}),$ заметим, что почти всюду матрица~$\frac{\partial \mathbf{D}_\sigma}{\partial \mathbf{Y}}=\mathbf{0}$ равняется нулю согласно теоремы~\ref{theorem:relu_derivative_hessian}, а следовательно только последний множитель зависит от матрицы~$\mathbf{W}_k.$ Используя лемму~\ref{lemma:matrix_funcs_product_derivative}, где первый множитель является константой, а также цепное правило относительно переменной~$\mathbf{Y}$ получаем:
\begin{align}
    \frac{\partial(\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})}{\partial \mathbf{W}_k} = \left( \frac{\partial (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})}{\partial \mathbf{Y}} \right) \frac{\partial\mathbf{Y}}{\partial \mathbf{W}_k},
\end{align}
причем согласно свойства~\ref{prop:kronecker_product_derivative} об производной произведения Кронекера с матрицей~$\mathbf{X} = \mathbf{Y}$ и матрицей~$\mathbf{Y}=\mathbf{I}_{d_{ff}}$ получаем:
\begin{align}
    \frac{\partial (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}})}{\partial \mathbf{Y}} = \left( \mathbf{I}_L \otimes \mathbf{K}_{d_{ff}, d_V} \otimes \mathbf{I}_{d_{ff}} \right)\left( \mathbf{I}_{L d_V} \otimes \mathrm{vec}_r(\mathbf{I}_{d_{ff}}) \right),
\end{align}
а в свою очередь согласно теоремы~\ref{thm:transformer_derivative} матрица~$\frac{\partial\mathbf{Y}}{\partial \mathbf{W}_k} = \mathbf{J}_Y \mathbf{G}_k.$ Тогда получаем оценку на матрицу~$\boldsymbol{\xi}_{1k}$ следующего вида:
\begin{align}
    \boldsymbol{\xi}_{1k} = \left( (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma \otimes \mathbf{I}_{n_k}\right)\left( \mathbf{I}_L \otimes \mathbf{K}_{d_{ff}, d_V} \otimes \mathbf{I}_{d_{ff}} \right)\left( \mathbf{I}_{L d_V} \otimes \mathrm{vec}_r(\mathbf{I}_{d_{ff}}) \right) \left( \mathbf{J}_Y \mathbf{G}_k \right).
\end{align}
Перейдем к матрице~$\mathbf{B}_2 = \sigma(\mathbf{Y}\mathbf{W}_1) \otimes \mathbf{I}_{d_V},$ заметим, что только первый множитель произведения Кронекера зависит от матриц~$\mathbf{W}_k,$ а следовательно используя свойство~\ref{prop:kronecker_product_derivative} производной произведения Кронекера и цепное правило получим следующее выражение:
\[
    \boldsymbol{\xi}_{2k} = \left( \mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V} \right)\left( \mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V}) \right)\left( \mathbf{D}_\sigma (\mathbf{I}_L \otimes \mathbf{W}_1^\top)  \mathbf{J}_Y \mathbf{G}_k \right),
\]
где использовано свойство~\ref{prop:matrix_product_derivative} для преобразования~$\frac{\partial (\mathbf{Y}\mathbf{W}_1)}{\partial \mathbf{Y}} = \mathbf{I}_L \otimes \mathbf{W}_1^\top,$ а также теорема~\ref{theorem:relu_derivative_hessian} для выражения $\frac{\partial \sigma(\cdot)}{\partial (\cdot)} = \mathbf{D}_\sigma,$ а также согласно теореме~\ref{thm:transformer_derivative} матрица~$\frac{\partial\mathbf{Y}}{\partial \mathbf{W}_k} = \mathbf{J}_Y \mathbf{G}_k.$

Перейдем к оценке пар слоев внимания~$(k,\ell)$ with $k,\ell \in \{K,Q,V\}$.
Рассмотрим матрицу~$\mathbf{B}_k = \mathbf{J}_{SY} \mathbf{J}_Y  \mathbf{G}_k,$ заметим, что почти всюду~$\frac{\partial \mathbf{J}_{SY}}{\partial \mathbf{Y}} = \mathbf{0},$ так как матричнозначная функция~$\mathbf{D}_\sigma$ является кусочно-постоянной, согласно теоремы~\ref{theorem:relu_derivative_hessian}, а следовательно используя лемму~\ref{lemma:matrix_funcs_product_derivative} с матрицей~$\mathbf{A}(\cdot)=\mathbf{J}_Y,$ и с матрицей~$\mathbf{B}(\cdot)=\mathbf{G}_k$ получаем:
\begin{align}
    \frac{\partial\mathbf{B}_k}{\partial \mathbf{W}_\ell}= (\mathbf{J}_{SY} \otimes \mathbf{I}_{n_k}) \frac{\partial(\mathbf{J}_Y \mathbf{G}_k)}{\partial \mathbf{W}_\ell},
\end{align}
далее вычислим матрицу~$\frac{\partial(\mathbf{J}_Y \mathbf{G}_k)}{\partial \mathbf{W}_\ell}$ используя свойство производной матричного произведения:
\begin{align}
    \frac{\partial(\mathbf{J}_Y \mathbf{G}_k)}{\partial \mathbf{W}_\ell} = (\mathbf{J}_Y \otimes \mathbf{I}_{n_k}) \boldsymbol{\Phi}_{k\ell}+ \left( \mathbf{I}_{L d_V} \otimes \mathbf{G}_k^\top \right) \frac{\partial\mathbf{J}_Y}{\partial \mathbf{W}_\ell}.
\end{align}
В условия теоремы~\ref{thm:layernorm_second_derivative} легко получить следующее выражение~$\frac{\partial\mathbf{J}_Y}{\partial \mathbf{W}_\ell} = \mathbf{H}_Y \mathbf{G}_\ell,$ а следовательно получаем оценки:
\begin{align}
    \boldsymbol{\xi}_{k\ell} = \left( \mathbf{J}_{SY} \otimes \mathbf{I}_{n_k} \right)\left[\left( \mathbf{I}_{L d_V} \otimes \mathbf{G}_k^\top \right) \left( \mathbf{H}_Y \mathbf{G}_\ell \right)+ \left( \mathbf{J}_Y \otimes \mathbf{I}_{n_k} \right) \boldsymbol{\Phi}_{k\ell}\right].
\end{align}


На 4-м шаге проверим симметричность полученных выражений.
Во первых единственные нелинейности с потенциально ненулевым вторым дифференциалом является оператор LayerNorm, для которого получены матрицы~$\mathbf{H}_Z,\mathbf{H}_Y$ в рамках теоремы~\ref{thm:layernorm_second_derivative} и которые являются симметричными по построению и оператор ReLU, для которого матрица Гессе является нулевой согласно теоремы~\ref{theorem:relu_derivative_hessian}, а во вторых все другие отображения являются линейными, а следовательно согласно леммы~\ref{lemma:matrix_funcs_product_derivative} и свойства производной произведения Кронекера их частные производные являются коммутативными почти всюду.
\end{proof}

\subsection{Спектральные оценки матрицы Гессе для трансформера}

\begin{theorem}\label{thm:transformer_hessian_estimate}
Пусть матрица Гессе~$\mathbf{H}_{\mathrm{tr}}^{(i,j)}$ описывает матрицу Гессе между~$(i,j)$-м блоком трансформер модели~\eqref{eq:block_hessian_transformer}, где~$i,j\in\{1,2,K,Q,V\}, n_i=\dim(\mathbf{W}_i).$
Тогда для каждой пары~$(i,j)$ получаем оценку нормы:
\begin{equation}\label{eq:block_bound_transformer}
    \big\|\mathbf{H}_{\mathrm{tr}}^{(i,j)}\big\|_2 \le \|\mathbf{J}_Z\|_2 \|\boldsymbol{\xi}_{ij}\|_2 + \|\mathbf{B}_i\|_2 \|\mathbf{H}_Z\|_2 \|\mathbf{B}_j\|_2,
\end{equation}
где~$\boldsymbol{\xi}_{ij}=\frac{\partial}{\partial \mathbf{W}_j}\!\left(\frac{\partial \mathbf{S}}{\partial \mathbf{W}_i}\right), \mathbf{B}_i=\frac{\partial \mathbf{S}}{\partial \mathbf{W}_i}$. 

Пусть матрица~$\mathbf{H}_{\mathrm{tr}}$ является полной матрицей Гессе размера~$m_b \times n_b$ состоящей из блоков матрицы~$\mathbf{H}_{\mathrm{tr}}^{(i,j)},$ где $m_b=n_b=5,i\in\{1,2,K,Q,V\}, j\in\{1,2,K,Q,V\}$).
Тогда
\begin{equation}\label{eq:full_bound_transformer}
    \|\mathbf{H}_{\mathrm{tr}}\|_2 \le
    \sqrt{m_b n_b} \max \limits_{i,j} \left(\frac{2}{L d_V}\|\frac{\partial \mathbf{Z}}{\partial \mathbf{W}_i}\|_2 \|\frac{\partial \mathbf{Z}}{\partial \mathbf{W}_j}\|_2 + \|\mathbf{R}^{\text{tr}}_m\|_2 \|\mathbf{H}_{\text{tr}}^{(i,j)}\|_2 \right).
\end{equation}
\end{theorem}
\begin{proof}
Рассмотрим блоки матрицы Гессе~\eqref{eq:block_hessian_transformer}:
\begin{align}
    \mathbf{H}_{\mathrm{tr}}^{(i,j)} = \big( \mathbf{J}_Z \otimes \mathbf{I}_{n_i} \big) \boldsymbol{\xi}_{ij} + \big( \mathbf{I}_{L d_V} \otimes \mathbf{B}_i^\top \big) \mathbf{H}_Z  \mathbf{B}_j.
\end{align}
Используя свойства норм матриц~\ref{prop:matrix_sum_norm},~\ref{prop:matrix_product_norm} и~\ref{prop:kronecker_product_norm} получаем оценку
\begin{align}
    \big\|\mathbf{H}_{\mathrm{tr}}^{(i,j)}\big\|_2 &\le\big\|\mathbf{J}_Z \otimes \mathbf{I}_{n_i}\big\|_2 \|\boldsymbol{\xi}_{ij}\|_2+\big\|\mathbf{I}_{L d_V} \otimes \mathbf{B}_i^\top\big\|_2 \|\mathbf{H}_Z\|_2 \|\mathbf{B}_j\|_2 = \\
    &= \|\mathbf{J}_Z\|_2 \|\boldsymbol{\xi}_{ij}\|_2+\|\mathbf{B}_i\|_2 \|\mathbf{H}_Z\|_2 \|\mathbf{B}_j\|_2,
\end{align}
описанной в условиях теоремы~\eqref{eq:block_bound_transformer}.

Оценим все слагаемые операторных норм в полученной оценке, а именно норму~$\|\mathbf{B}_i\|_2,$ и норму~$\|\boldsymbol{\xi}_{ij}\|_2,$ которые используются в формуле~\eqref{eq:block_bound_transformer}.
Используя свойствы матричных норм~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, \ref{prop:matrix_sum_norm}, \ref{prop:matrix_norm_inequalities}, \ref{prop:transposed_matrix_norm}, а также определение~\ref{def:commutation_matrix} коммутативных матрицы, получаем, что $\|\mathbf{K}_{m,n}\|_2=1,$ а также согласно свойству~\ref{prop:matrix_norm_inequalities} получаем нормы~$\|\mathrm{vec}_r(\mathbf{I}_{d})\|_2=\|\mathbf{I}_{d}\|_F=\sqrt{d}$ и~$\|\mathbf{I}_p\|_2=1$.
В доказательстве теоремы~\ref{thm:self_attention_hessian_estimation} было доказано, что :
\begin{align}
    \Big\|\frac{\partial \mathbf{A}}{\partial \mathbf{T}}\Big\|_2 &\le \frac{1}{L},\\
    \|\mathbf{Z}_1\|_2 &= \|(\mathbf{I}_L \otimes \mathbf{X}^\top)(\partial \mathbf{A}/\partial \mathbf{T})(\mathbf{X}\otimes \mathbf{X})\|_2\le \|\mathbf{X}\|_2 \frac{1}{L} \|\mathbf{X}\|_2^2= \frac{1}{L}\|\mathbf{X}\|_2^3, \\
    \Big\|\frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2}\Big\|_2 &\le 6, \\
    \|\mathbf{Z}_2\|_2 &\le \|\mathbf{X}\|_2^5 \Big\|\frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2}\Big\|_2 \le 6 \|\mathbf{X}\|_2^5,\\
    \|\mathbf{A}\|_2 &\le \sqrt{L L}\|\mathbf{A}\|_{\max} = L,
\end{align}
а следовательно согласно свойству~\ref{prop:matrix_product_norm} получаем оценку $\|\mathbf{A}\mathbf{X}\|_2 \le \|\mathbf{A}\|_2 \|\mathbf{X}\|_2 \le L \|\mathbf{X}\|_2.$
Оценим матрицы~$\boldsymbol{\Phi}_{k\ell}$ полученные в рамках леммы~\ref{lemma:attention_phi_from_functional_hessian}.
Используя свойства матричных норм~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, а также верхние оценки на матрицы~$\|\mathbf{Z}_1\|_2$, $\|\mathbf{Z}_2\|_2$ получаем:
\begin{align}
    \|\boldsymbol{\Phi}_{VV}\|_2 &= 0,\\
    \|\boldsymbol{\Phi}_{QQ}\|_2 &\le \frac{2}{L d_V d_K}\|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2 \|\mathbf{Z}_2\|_2 \|\mathbf{W}_K\|_2 \le\\
    &\le\frac{12}{L d_V d_K} \|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2^2 \|\mathbf{X}\|_2^5,\\
    \|\boldsymbol{\Phi}_{VQ}\|_2 &\le \frac{2}{L d_V \sqrt{d_K}}\|\mathbf{I}_L \otimes \mathbf{S}\|_2  \|\mathbf{Z}_1\|_2  \|\mathbf{I}_{d_V} \otimes \mathbf{W}_K\|_2 \le\\
    &\le\frac{2}{L^2 \sqrt{d_V d_K}} \|\mathbf{W}_K\|_2 \|\mathbf{X}\|_2^3,\\
    \|\boldsymbol{\Phi}_{QK}\|_2 &\le \frac{2}{L d_V d_K} \|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2 \|\mathbf{Z}_2\|_2 \|\mathbf{W}_Q\|_2
    + \frac{2}{L d_V \sqrt{d_K}} \|\mathbf{W}_V\|_2  \|\mathbf{Z}_1\|_2  \|\mathbf{S}\|_2 \le \\
    &\le \frac{12}{L d_V d_K} \|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2 \|\mathbf{W}_Q\|_2 \|\mathbf{X}\|_2^5
    + \frac{2}{L^2 \sqrt{d_V d_K}} \|\mathbf{W}_V\|_2 \|\mathbf{X}\|_2^3.
\end{align}
Для оценки матричных норм~$\|\mathbf{B}_i\|_2$ рассмотрим чему они равны при разных~$i$ из определения в теоремах~\ref{thm:transformer_hessian}, \ref{theorem:relu_derivative_hessian}. Для матрицы~$\mathbf{B}_1 = (\mathbf{I}_L \otimes \mathbf{W}_2^\top) \mathbf{D}_\sigma (\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}),$ тогда используя свойства матричных норм~\ref{prop:kronecker_product_norm}, \ref{prop:matrix_product_norm}, \ref{prop:transposed_matrix_norm}, а также оценку~$\|\mathbf{D}_\sigma\|_2 \le 1$ получаем:
\begin{equation}\label{eq:B1_norm}
    \|\mathbf{B}_1\|_2 \le \|\mathbf{I}_L \otimes \mathbf{W}_2^\top\|_2 \|\mathbf{D}_\sigma\|_2 \|\mathbf{Y} \otimes \mathbf{I}_{d_{ff}}\|_2
    = \|\mathbf{W}_2\|_2 \|\mathbf{Y}\|_2.
\end{equation}
Для матрицы~$\mathbf{B}_2 = \sigma(\mathbf{Y}\mathbf{W}_1) \otimes \mathbf{I}_{d_V}$ воспользовавшись свойством~\ref{prop:kronecker_product_norm} получаем:
\begin{equation}\label{eq:B2_norm}
    \|\mathbf{B}_2\|_2 = \|\sigma(\mathbf{Y}\mathbf{W}_1)\|_2.
\end{equation}
Для матриц~$\mathbf{B}_k = \mathbf{J}_{SY}\mathbf{J}_Y\mathbf{G}_k,$ где~$k\in\{K,Q,V\},$ используя свойство~\ref{prop:matrix_product_norm}:
\begin{equation}\label{eq:Bk_norm}
    \|\mathbf{B}_k\|_2 \le \|\mathbf{J}_{SY}\|_2  \|\mathbf{J}_Y\|_2  \|\mathbf{G}_k\|_2.
\end{equation}
Для матрицы~$\mathbf{J}_{SY} = (\mathbf{I}_L \otimes \mathbf{W}_2^\top)\mathbf{D}_\sigma(\mathbf{I}_L \otimes \mathbf{W}_1^\top) + (\mathbf{I}_L \otimes \mathbf{I}_{d_V})$ используя свойства~\ref{prop:matrix_sum_norm}, \ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, \ref{prop:transposed_matrix_norm}, а также оценку~$\|\mathbf{D}_\sigma\|_2 \le 1$ получаем оценку матричной нормы:
\begin{align}\label{eq:JSY_norm}
    \|\mathbf{J}_{SY}\|_2 &\le \|\mathbf{I}_L \otimes \mathbf{W}_2^\top\|_2 \|\mathbf{D}_\sigma\|_2 \|\mathbf{I}_L \otimes \mathbf{W}_1^\top\|_2 + \|\mathbf{I}_L \otimes \mathbf{I}_{d_V}\|_2 =\\
    &=\|\mathbf{W}_2\|_2 \|\mathbf{W}_1\|_2 + 1.
\end{align}
Для матриц~$\|\mathbf{G}_V\|_2,\|\mathbf{G}_Q\|_2,\|\mathbf{G}_K\|_2,$ используя свойства~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm} получаем оценки на нормы:
\begin{align}\label{eq:Gk_bounds}
    \|\mathbf{G}_V\|_2 &\le L \|\mathbf{X}\|_2,\\
    \|\mathbf{G}_Q\|_2 &\le \frac{1}{L\sqrt{d_K}} \|\mathbf{W}_V\|_2 \|\mathbf{W}_K\|_2 \|\mathbf{X}\|_2^3, \\
    \|\mathbf{G}_K\|_2 &\le \frac{1}{L\sqrt{d_K}} \|\mathbf{W}_V\|_2 \|\mathbf{W}_Q\|_2 \|\mathbf{X}\|_2^3.
\end{align}
Для оценки матричных норм~$\|\boldsymbol{\xi}_{ij}\|_2,$ рассмотрим чему они равны из определения в теореме~\ref{thm:transformer_hessian}.
В случае пар FFN для матриц~$\|\boldsymbol{\xi}_{11}\|_2,\|\boldsymbol{\xi}_{12}\|_2,\|\boldsymbol{\xi}_{21}\|_2,\|\boldsymbol{\xi}_{22}\|_2$ используя свойства~\ref{prop:kronecker_product_norm}, \ref{prop:matrix_product_norm}, \ref{prop:matrix_norm_inequalities} матричных норм, а также свойство коммутативных матриц~$\|\mathbf{K}_{m,n}\|_2=1$ получаем оценки:
\begin{align}\label{eq:xiffn}
    \|\boldsymbol{\xi}_{11}\|_2 &= 0,\\
    \|\boldsymbol{\xi}_{22}\|_2 &= 0,\\
    \|\boldsymbol{\xi}_{12}\|_2 &\le \|\mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V}\|_2  \|\mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V})\|_2  \|\mathbf{D}_\sigma\|_2  \|\mathbf{Y}\otimes \mathbf{I}_{d_{ff}}\|_2 \nonumber\\
    &= 1 \cdot \|\mathrm{vec}_r(\mathbf{I}_{d_V})\|_2 \cdot 1 \cdot \|\mathbf{Y}\|_2
    = \sqrt{d_V} \|\mathbf{Y}\|_2, \\
    \|\boldsymbol{\xi}_{21}\|_2 &\le \|\mathbf{I}_L \otimes \mathbf{W}_2^\top\|_2  \|\mathbf{D}_\sigma\|_2  \|\mathbf{I}_L \otimes \mathbf{K}_{d_{ff}, d_V} \otimes \mathbf{I}_{d_{ff}}\|_2  \|\mathbf{I}_{L d_V} \otimes \mathrm{vec}_r(\mathbf{I}_{d_{ff}})\|_2 \nonumber\\
    &= \|\mathbf{W}_2\|_2 \cdot 1 \cdot 1 \cdot \|\mathrm{vec}_r(\mathbf{I}_{d_{ff}})\|_2 = \sqrt{d_{ff}} \|\mathbf{W}_2\|_2.
\end{align}
В случае пар FFN с параметрами слоев внимания для всех~$k\in\{K,Q,V\}$ получаем оценки:
\begin{align}\label{eq:xiffnk}
    \|\boldsymbol{\xi}_{1k}\|_2 &\le \|(\mathbf{I}_L \otimes \mathbf{W}_2^\top)\mathbf{D}_\sigma \otimes \mathbf{I}_{n_k}\|_2 \|\mathbf{I}_L \otimes \mathbf{K}_{d_{ff}, d_V} \otimes \mathbf{I}_{d_{ff}}\|_2 \cdot\\
    &\quad\cdot\|\mathbf{I}_{L d_V} \otimes \mathrm{vec}_r(\mathbf{I}_{d_{ff}})\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2 \nonumber\\
    &\le \|\mathbf{W}_2\|_2 \cdot 1 \cdot 1 \cdot \sqrt{d_{ff}} \cdot\|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2 =\\
    &= \sqrt{d_{ff}} \|\mathbf{W}_2\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2, \\
    \|\boldsymbol{\xi}_{2k}\|_2 &\le \|\mathbf{I}_L \otimes \mathbf{K}_{d_V, d_{ff}} \otimes \mathbf{I}_{d_V}\|_2 \|\mathbf{I}_{L d_{ff}} \otimes \mathrm{vec}_r(\mathbf{I}_{d_V})\|_2 \|\mathbf{D}_\sigma\|_2  \|\mathbf{I}_L \otimes \mathbf{W}_1^\top\|_2 \cdot\\
    &\quad\cdot \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2 \nonumber\\
    &\le 1 \cdot \sqrt{d_V} \cdot 1 \cdot \|\mathbf{W}_1\|_2 \cdot \|\mathbf{J}_Y\|_2 \cdot \|\mathbf{G}_k\|_2 =\\
    &= \sqrt{d_V} \|\mathbf{W}_1\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2.
\end{align}
Для пар слоев внимания~$k,\ell\in\{K,Q,V\}$
\begin{align}
    \boldsymbol{\xi}_{k\ell}
    = \big(\mathbf{J}_{SY} \otimes \mathbf{I}_{n_k}\big)
    \Big[ \big(\mathbf{I}_{L d_V} \otimes \mathbf{G}_k^\top\big) (\mathbf{H}_Y \mathbf{G}_\ell)+ \big(\mathbf{J}_Y \otimes \mathbf{I}_{n_k}\big) \boldsymbol{\Phi}_{k\ell}\Big],
\end{align}
используя свойства~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm} получаем следующие оценки:
\begin{align}\label{eq:xikell}
    \|\boldsymbol{\xi}_{k\ell}\|_2 &\le \|\mathbf{J}_{SY}\|_2 \Big( \|\mathbf{I}_{L d_V} \otimes \mathbf{G}_k^\top\|_2 \|\mathbf{H}_Y\|_2 \|\mathbf{G}_\ell\|_2
    + \|\mathbf{J}_Y\|_2 \|\boldsymbol{\Phi}_{k\ell}\|_2 \Big) =\\
    &=\|\mathbf{J}_{SY}\|_2 \Big( \|\mathbf{G}_k\|_2 \|\mathbf{H}_Y\|_2 \|\mathbf{G}_\ell\|_2
    + \|\mathbf{J}_Y\|_2 \|\boldsymbol{\Phi}_{k\ell}\|_2 \Big).
\end{align}
Итого собирая все части выражения~\eqref{eq:block_bound_transformer}, используя для каждой пары~$(i,j)$ оценки норм матриц~$\|\boldsymbol{\xi}_{ij}\|_2$ с выражений~\eqref{eq:xiffn},\eqref{eq:xiffnk},\eqref{eq:xikell}, а также оценки норм матриц~$\|\mathbf{B}_i\|_2$ с выражений \eqref{eq:B1_norm},\eqref{eq:Bk_norm},\eqref{eq:JSY_norm},\eqref{eq:Gk_bounds} и подставляя в выражение~\eqref{eq:block_bound_transformer} получаем следующие оценки норм на все блоки матрицы Гесее:
\begin{align}
    \big\|\mathbf{H}_{\mathrm{tr}}^{(1,1)}\big\|_2
    &\le \|\mathbf{J}_Z\|_2 \cdot 0 + \|\mathbf{B}_1\|_2^2 \|\mathbf{H}_Z\|_2 \le\\
    &\le \|\mathbf{H}_Z\|_2 (\|\mathbf{W}_2\|_2 \|\mathbf{Y}\|_2)^2,\\
    \big\|\mathbf{H}_{\mathrm{tr}}^{(1,2)}\big\|_2 &\le \|\mathbf{J}_Z\|_2  \sqrt{d_V}\|\mathbf{Y}\|_2 + \|\mathbf{H}_Z\|_2  (\|\mathbf{W}_2\|_2 \|\mathbf{Y}\|_2) \|\sigma(\mathbf{Y}\mathbf{W}_1)\|_2,\\
    \big\|\mathbf{H}_{\mathrm{tr}}^{(1,k)}\big\|_2 &\le \|\mathbf{J}_Z\|_2 \sqrt{d_{ff}}\|\mathbf{W}_2\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2 +\\
    &\quad+\|\mathbf{H}_Z\|_2  (\|\mathbf{W}_2\|_2 \|\mathbf{Y}\|_2)  (\|\mathbf{J}_{SY}\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2),\\
    \big\|\mathbf{H}_{\mathrm{tr}}^{(k,\ell)}\big\|_2
    &\le \|\mathbf{J}_Z\|_2 \|\mathbf{J}_{SY}\|_2 \Big( \|\mathbf{G}_k\|_2 \|\mathbf{H}_Y\|_2 \|\mathbf{G}_\ell\|_2 + \|\mathbf{J}_Y\|_2 \|\boldsymbol{\Phi}_{k\ell}\|_2 \Big) +\\
    &\quad + \|\mathbf{H}_Z\|_2  (\|\mathbf{J}_{SY}\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_k\|_2)  (\|\mathbf{J}_{SY}\|_2 \|\mathbf{J}_Y\|_2 \|\mathbf{G}_\ell\|_2),
\end{align}

В оценке матричной норм~$\|\mathbf{Y} \|_2$ and $\| \mathbf{S}\|_2$ были использованы результаты леммы~\ref{lemma:Y_S_norm_bounds}. Нормы матриц~$\mathbf{H}_Z,\mathbf{H}_Y$ оцениваются в рамках леммы~\ref{lemma:layernorm_deriv_hessian_norm}.
\end{proof}