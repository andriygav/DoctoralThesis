\paragraph{Актуальность темы.}
Ключевым фактором развития нейросетевых моделей с начала XXI века является прогресс в вычислительной технике, характеризующийся экспоненциальным ростом производительности суперкомпьютеров, измеряемой в операциях с плавающей запятой~(англ. FLOPS): от значений на уровне терафлопсов в начале столетия до достижения экзафлопсов в настоящий момент.
Параллельно наблюдается сопоставимый рост сложности моделей глубокого обучения, выраженный в увеличении количества обучаемых параметров на несколько порядков~--- от тысяч в начале нулевых годов до миллиардов и сотен миллиардов в двадцатых, с прогнозируемым переходом к триллионам параметров в ближайшем десятилетии.
Современные исследования, посвященные анализу сложности таких моделей, в значительной степени опираются на эмпирические корреляции, связывающие их эффективность с количественными метриками~--- числом параметров и вычислительными затратами на обучение либо применение.
Отсутствие же строгой теоретической основы для предсказания поведения моделей при масштабировании делает этот процесс экономически и энергетически нерациональным и зачастую приводит к получению необоснованных и противоречивых результатов.

Развитие больших языковых моделей~(англ. LLM) сопряжено с высокими затратами на их обучение, выражающимися в большом потреблении вычислительных, энергетических и финансовых ресурсов.
Существенной проблемой в этом процессе является присущая ему непредсказуемость, для нивелирования которой на предварительной стадии обучения~(англ. pretrain) эмпирически подбираются оптимальные соотношения между размером модели в параметрах и объемом обучающих данных в токенах при заданной допустимой вычислительном ресурсе в операциях с плавающей запятой.
Однако данные, полученные для одной архитектуры, не являются переносимыми на другие модели, что делает подобные оценки зачастую несостоятельными и приводит к непредвиденным результатам при полномасштабном обучении.
В этой связи разработка теоретических оценок сложности моделей представляется критически важной, поскольку она позволила бы получать асимптотические оценки сложности моделей, которые связаны со сложностью выборки еще на этапе проектирования архитектуры, чувствительной к любым модификациям своей структуры.

Оценка сложности моделей машинного обучения является глубоко изученной областью для классических методов и находит ограниченное применение при анализе моделей глубокого обучения.
Фундаментальный вклад в теорию оценивания сложности моделей и в теорию машинного обучения в целом внесли работы Владимира Наумовича Вапника и Алексея Яковлевича Червоненкиса, заложившие основы статистической теории обучения~\cite{vapnik1974StatTheory}.
Альтернативный математический аппарат для анализа алгоритмов обучения был разработан Лесли Вэлиантом, предложившим приближенно правильного обучения~(англ. PAC-Learning)~\cite{valiant1984LearnableTheory}.
Современный подход к определению сложности основан на Радемахеровской сложности, предложенная Владимиром Кольчинским и Дмитрием Панченко~\cite{koltchinskii2000RadamakherTheory}. Современные исследования в области теории машинного обучения редко рассматривают современные нейросетевые модели в виду их сложности и невозможности получения ``адекватных'' оценок на сложность таких моделей. Значимая часть современных исследованиях на ведущих конференция посвящена классическим методам машинного анализа и улучшения оценок для известных методов, так как текущие оценки сложности даже для классических моделей машинного обучения являются сильно завышенными.

В работе же проводится теоретический анализ нейросетевых моделей, опирающийся на анализ их матриц Гессе.
Матрица Гессе используется как для анализа важности параметров в задачах прореживания нейросетевых моделей, так и для анализа ландшафта функции потерь, который используется для анализа сложности модели.

\paragraph{Степень разработанности темы диссертационного исследования.}
Современное состояние анализа сложности моделей машинного обучения описаны в~\cite{macKay2003}. С другой стороны сложность нейросетевых моделей является слабо изученной темой на текущий момент. В свою очередь рассмотрения сложности выборки сводится только к анализу размера выборки для обучения~\cite{motrenko2022numerical613055643, demidenko2007, joseph1997, joseph1995, kloek1975, lindley1997, motrenko2014, qumsiyeh2013, rubin1998, self1988, self1992, shieh2000, shieh2005, wang2002}, что является не совсем корректным, так как сложность выборки также обусловлено и сложностью каждого объекта. Современные методы оценки сложность объектов выборки опираются исследования сложности многообразий, которые аппроксимируют данный элемент выборки. Одним из способов оценки объема выборки для обучения больших языковых моделей являются эмперические оценки вместе с методологией их получения для новых моделей~\cite{hoffmann2022Chinchila,kaplan2020ScalingLaws} полученные Джаредом Капланом и Джорданом Хофманом в рамках исследования законов масштабирования нейросетевых моделей~\cite{rae2022scalinglanguagemodelsmethods}. Что касается задачи снижения сложности моделей, на текущий момент существует несколько направлений. Первое направление направлено на квантизацию параметров моделей, второе же, более теоретическое, направлено на дистиляцию и привилегированное обучение~\cite{hinton2015, lopez2016, grabovoi2021bayesian609999432}.

Однако, на текущий момент, в исследованиях не существует теоретического апарата, для описания сложности моделей и данных, чтобы получить асимптотические оценки сложности моделей и данных. Вместе с тем полученный математический аппарат позволит проводить сравнительный анализ различных нейросетевых архитектур, для выбора лучшего решения для заданной задачи, которая описывается выборкой заданной сложности.

В данной работе проведен комплексный подход к оценке сложности моделей и данных. Предложен альтернативный математический аппарат для анализа сложности на основе анализа ландшафта функции потерь нейросетевых моделей. Получены теоретические оценки для матриц Гессе различных моделей глубокого обучения, которые требуются для оценки сложности модели. Отдельной частью данной работы является анализ связи объема и сложности выборки со сложностью модели. Вводятся частные случае общей теории сложности, которые позволяют получать практические оценки для прикладных задач.

\paragraph{Объектом исследования} в работе являются параметрические семейства функций, которые представимы в виде суперпозиции линейных и нелинейных преобразований; данные, которые используются для настройки параметров параметрических семейств.

\paragraph{Предмет исследования:} разработка моделей, методов и теоретического аппарата работы со сложностью моделей глубокого обучения и данными, которые используются для обучения.

\paragraph{Цель и задачи исследования.}
Целью исследования является получения оценок сложности моделей глубокого обучения, а также получения оценок сложности данных для обучения моделей.

Для достижения цели были поставлены и решены следующие задачи:
\begin{itemize}
    \item[--] разработка методов оценки сложности моделей глубокого обучения;~\textcolor{red}{Теоретическое введение теории сложности, с понятием условной сложности.}
    \item[--] вычисление оценок матриц Гессе для различных архитектур моделей глубокого обучения;~\textcolor{red}{Все работы связанные с оценкой матриц Гессе: Киселев/Мешков/Петров.}
    \item[--] вычисление оценок размера выборки и их связь со сложностью моделей и данных;~\textcolor{red}{Все работы связанные с оценкой размера выборки: Киселев.}
    \item[--] вычисление оценок важности параметров глубоких нейросетевых моделей при обучении и предсказании;~\textcolor{red}{Работы с прореживанием нейросетевых моделей, дистиляция моделей. Требуюется работа связана с использованием новых оценок матрицы Гессе для вычисления важности параметров.}
    \item[--] оценка влияния регуляризаторов параметров моделей глубокого обучения на оценку сложности моделей глубокого обучения;~\textcolor{red}{Требуются дополнительные, новые исследования: Мешков.}
    \item[--] практическое применение анализа сложности моделей и данных при обучении моделей глубокого обучения;~\textcolor{red}{Тут все эксперименты, которые есть: Киселев/Мешков.}
    \item[--] неявная регуляризация сложности модели при мультизадачном обучении.~\textcolor{red}{Все работы связанные с детекцией машингена в мультизадачном подходе: Грицай/Ремизова.}
\end{itemize}

\paragraph{Методы исследования.}
Для решения поставленных задач в диссертации используются методы: машинного обучения, статистического анализа данных, статистической теории машинного обучения, линейной алгебры, дискретной математики, теории вероятности, глубокого обучения.

\paragraph{Научная новизна.}
Научной новизной проведенного исследования являются теоретические оценки сложности моделей глубокого обучения и их связи со сложностью данных для обучения.
Диссертация представляет новый подход к рассмотрению сложности моделей глубокого обучения на стыке строго терроризированных оценок и их практической применимостью в реальных задачах.
В диссертации предлагается новый подход к оценке сложности модели на основе анализа ландшафта оптимизационной задачи.
В рамках диссертационной работы получены оценки матриц Гессе для некоторых моделей глубокого обучения.
На основе полученных оценок матриц Гессе в рамках диссертационной работы рассмотрены новые подходы для построения методов оценки важности параметров нейросетевых моделей, которые ранее были слабо исследованы из-за невычислимости данной матрицы для больших нейросетевых моделей.


\paragraph{Теоретическая значимость работы.}
Диссертационная работа в значительной степени представляет из себя именно теоретический результат. В диссертационной работе рассматриваются фундаментальные вопросы о сложности моделей машинного обучения. Полученные оценки на матрицы Гессе открывают большой спектр задач в теории выбора моделей машинного обучения. 

\paragraph{Практическая значимость работы.}
Предложенные теоретические оценки в работе также рассматривались и из практической стороны. В диссертационной работе предложены адаптации различных полученных оценок, для их применимости в различных прикладных задачах. Отдельно проведены работы с сравнением теоретических оценок и эмпирических оценок закона масштабирования моделей глубокого обучения.

\paragraph{Положения, выносимые на защиту:}
\begin{enumerate}
    \item Оценки сложности моделей глубокого обучения.
    \item Оценки сложности данных.
    \item Ландшафтная мера сложности моделей глубокого обучения и ее связь со сложностью данных.
    \item Оценки матриц Гессе для некоторых классов нейросетевых архитектур и их связь с ландшафтной мерой сложности моделей глубокого обучения.
    \item Оценки достаточного размера выборки и их связь со сложностью моделей и данных.
    \item Методы снижения размерности пространства параметров моделей глубокого обучения на основе анализа матриц Гессе.
\end{enumerate}

\paragraph{Степень достоверности результатов.}
Достоверность научных результатов работы подтверждается непротиворечивостью и согласованностью с известными фактами и исследованиями в рассматриваемой области, высокой степенью сходимости теоретических результатов с данными экспериментов и определяется применением теоретических и методологических основ разработок ведущих ученых в области обработки естественного языка, корректным и обоснованным использованием математического аппарата, экспериментальными исследованиями разработанных моделей и методов.

\paragraph{Соответствие диссертации паспорту специальности.}
Тема и основные результаты диссертации соответствуют следующим областям исследований паспорта специальности 1.2.1 --- Искусственный интеллект и машинное обучение.

2 Исследования в области оценки качества и эффективности алгоритмических и программных решений для систем искусственного интеллекта и машинного обучения. Методики сравнения и выбора алгоритмических и программных решений при многих критериях.

4 Разработка методов, алгоритмов и создание систем искусственного интеллекта и машинного обучения для обработки и анализа текстов на естественном языке, для изображений, речи, биомедицины и других специальных видов данных.

16 Исследования в области специальных методов оптимизации, проблем сложность и элиминации перебора, снижения размерности.

17 Исследования в области многослойных алгоритмических конструкций, в том числе – многослойных нейросетей.

\paragraph{Апробация результатов диссертации.}
Основные результаты работы докладывались и обсуждались на Всероссийской конференции с международным участием «Математические методы распознавания образов»~(Москва, 2019, Москва, 2021, Муром, 2025), Международной конференции «Интеллектуализация обработки информации»~(Гаэта, 2018, Москва, 2020, Москва, 2022), Всероссийской научной конференции МФТИ~(Москва, 2018, 2019, 2020, 2021, 2023, 2024, 2025), Ivannikov Ispras Open Conference~(Москва, 2021, 2022, 2023, 2024), Ivannikov Memorial Workshop~(Казань, 2022), Iberian Languages Evaluation Forum co-located with the Conference of the Spanish Society for Natural Language Processing~(Андалусия, 2023, 2024), 35th Conference of Open Innovations Association~(Тампере, 2024), Fourth Workshop on Scholarly Document Processing~(Бангкок, 2024), 1st Workshop on GenAI Content Detection (GenAIDetect)~(Абу-Даби 2025), 19th International Workshop on Semantic Evaluation~(Вена, 2025).

\paragraph{Публикации.}
По теме диссертации опубликовано 56 научных работ, из
которых 17 статей в научно-технических журналах, входящих в перечень ВАК, 32 – в изданиях, входящих в международные наукометрические базы Scopus и Web of Science. В трудах российских и международных конференций опубликовано 39 работ.

\paragraph{Личный вклад соискателя.}
Все выносимые на защиту результаты и положения, составляющие основное содержание диссертационного исследования, разработаны и получены лично автором или при его непосредственном участии вместе с учениками. В работах, опубликованных в соавторстве, соискателю принадлежит определяющая роль в построении теоретических методов и направлении. В работе ...

\paragraph{Структура и объем работы.}
Диссертация состоит из оглавления, введения, шести разделов, заключения, списка иллюстраций, списка таблиц, перечня основных обозначений и списка литературы из~\total{citnum} наименований. Основной текст занимает~\pageref{LastPage} страницы.