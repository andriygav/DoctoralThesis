Диссертационная работа решает проблему отсутствия строгого теоретического аппарата для оценки сложности моделей и данных в моделях глубокого обучения, что препятствует рациональному проектированию архитектур и эффективному использованию вычислительных ресурсов при масштабировании моделей.
В работе разработан новый теоретический подход, основанный на анализе ландшафта оптимизационной задачи, который обеспечивает получение асимптотических оценок сложности различных семейств моделей глубокого обучения и устанавливает формальную связь между сложностью модели и сложностью данных, необходимых для ее обучения.

\paragraph{Актуальность темы.}
С начала XXI века наблюдается экспоненциальный рост производительности вычислительных систем, измеряемой в операциях с плавающей запятой~(англ. FLOPS): от терафлопсов в начале столетия до экзафлопсов в настоящий момент.
Параллельно происходит сопоставимый рост сложности моделей глубокого обучения, выраженный в увеличении количества обучаемых параметров на несколько порядков~--- от тысяч в начале нулевых годов до миллиардов и сотен миллиардов в двадцатых, с прогнозируемым переходом к триллионам параметров в ближайшем десятилетии.
Современные исследования анализа сложности таких моделей преимущественно опираются на эмпирические корреляции, связывающие эффективность моделей с количественными метриками~--- числом параметров и вычислительными затратами на обучение или применение.
Отсутствие строгой теоретической основы для предсказания поведения моделей при масштабировании делает процесс разработки экономически и энергетически неэффективным и зачастую приводит к получению необоснованных и противоречивых результатов.

Особую остроту проблема приобретает при разработке больших языковых моделей~(англ. LLM), обучение которых требует значительных вычислительных, энергетических и финансовых ресурсов.
Для снижения непредсказуемости процесса обучения на предварительной стадии~(англ. pretrain) эмпирически подбираются оптимальные соотношения между размером модели в параметрах и объемом обучающих данных в токенах при заданном вычислительном ресурсе.
Однако эмпирические оценки, полученные для одной архитектуры, не переносятся на другие модели, что делает подобные подходы несостоятельными и приводит к непредвиденным результатам при полномасштабном обучении.
В связи с этим разработка теоретических оценок сложности моделей становится критически важной, поскольку такие оценки позволяют связать сложность модели со сложностью выборки еще на этапе проектирования архитектуры, обеспечивая рациональный выбор параметров модели и объема данных.
Отметим, что глубоким теоретическим анализом выбора и порождения моделей на этапе проектирования архитектур моделей глубокого обучения занимается Вадим Викторович Стрижов~\cite{strijov2014doctoral}.

Классические подходы к оценке сложности моделей машинного обучения, разработанные для традиционных методов, находят ограниченное применение при анализе моделей глубокого обучения.
Фундаментальный вклад в теорию оценивания сложности моделей внесли работы Владимира Наумовича Вапника и Алексея Яковлевича Червоненкиса, заложившие основы статистической теории обучения~\cite{vapnik1974StatTheory}, а также Лесли Вэлианта, предложившего подход приближенно правильного обучения~(англ. PAC-Learning)~\cite{valiant1984LearnableTheory}.
Современные подходы к определению сложности основаны на Радемахеровской сложности, предложенной Владимиром Кольчинским и Дмитрием Панченко~\cite{koltchinskii2000RadamakherTheory}, и комбинаторном подходе к оценке обучаемости алгоритмов Константина Вячеславовича Воронцова~\cite{vorontsov04qualdan}, в котором разработан математический аппарат на основе комбинаторных оценок вероятности переобучения моделей.
Однако современные исследования в области теории машинного обучения редко рассматривают нейросетевые модели глубокого обучения ввиду их высокой сложности и трудности получения адекватных оценок сложности таких моделей.
Значительная часть современных исследований на ведущих конференциях посвящена классическим методам машинного обучения и улучшению оценок для известных методов, так как текущие оценки сложности даже для классических моделей являются сильно завышенными.

Отдельным направлением в исследованиях являются оценки репрезентативной способности моделей глубокого обучения к аппроксимации по прецедентам.
Основополагающий результат принадлежит Джорджу Цибенко~\cite{cybenko1989ApproximationBS}, который доказал, что нейронные сети аппроксимируют непрерывные функции с некоторыми ограничениями сколь угодно большим качеством.
Йохану Хестад~\cite{hastad1987phd} принадлежат первые оценки, указывающие на рост аппроксимирующей способности моделей глубокого обучения с глубиной модели.
В более современных работах Яна Лекуна~\cite{bengioLecun2007:scaling}, Йошуа Бенджио~\cite{begio2011exponetialdepth}, Надава Коэна~\cite{cohen2015OnTE} эти оценки получены с более мягкими ограничениями на класс рассматриваемых функций.
В целом все эти оценки наряду с работой Охада Шамира~\cite{eldan2016exponentialcomplexity} указывают на экспоненциальное увеличение аппроксимирующей способности моделей глубокого обучения с ростом числа слоев.

В настоящей работе предлагается теоретический анализ нейросетевых моделей, опирающийся на анализ их матриц Гессе.
Матрица Гессе функции потерь по параметрам модели содержит информацию о локальной кривизне оптимизационного ландшафта и используется для анализа сложности моделей и данных, а также для оценки важности параметров в задачах прореживания нейросетевых моделей.

\paragraph{Степень разработанности темы диссертационного исследования.}
Классические подходы к оценке сложности моделей машинного обучения, основанные на VC-размерности, PAC-обучаемости и радемахеровской сложности, разработаны для моделей с ограниченным числом параметров и ориентированы на worst-case анализ, что делает их оценки слишком консервативными для перепараметризованных нейронных сетей~\cite{macKay2003}.
Эти меры не учитывают специфику архитектур глубокого обучения, такие как сверточные фильтры, остаточные связи и механизмы внимания, а также не отражают влияние регуляризации, ранней остановки и особенностей процесса оптимизации на обобщающую способность моделей.
Комбинаторный подход К.В. Воронцова~\cite{vorontsov04qualdan} позволяет снизить оценки Вапника--Червоненкиса, но остается применимым лишь к ограниченным классам моделей.

Что касается оценки сложности данных, существующие подходы преимущественно сводятся к анализу размера выборки для обучения~\cite{motrenko2022numerical613055643, demidenko2007, joseph1997, joseph1995, kloek1975, lindley1997, motrenko2014, qumsiyeh2013, rubin1998, self1988, self1992, shieh2000, shieh2005, wang2002}, что является неполным, так как сложность выборки также определяется сложностью каждого объекта.
Современные методы оценки сложности объектов выборки опираются на исследования сложности многообразий, аппроксимирующих элементы выборки.
Для больших языковых моделей используются эмпирические законы масштабирования~\cite{hoffmann2022Chinchila,kaplan2020ScalingLaws}, полученные Джаредом Капланом и Джорданом Хофманом~\cite{rae2022scalinglanguagemodelsmethods}, однако эти законы не имеют строгого теоретического обоснования и не объясняют механизмы, лежащие в основе зависимостей между параметрами, данными и качеством модели.

В области снижения сложности моделей существуют два основных направления: квантизация параметров моделей и дистилляция знаний~\cite{hinton2015, lopez2016, grabovoi2021bayesian609999432}.
Однако существующие методы не опираются на строгие теоретические оценки сложности моделей и данных, что ограничивает их эффективность и предсказуемость.

Таким образом, на текущий момент в исследованиях отсутствует единый теоретический аппарат для описания сложности моделей и данных, позволяющий получить асимптотические оценки и установить формальные критерии соответствия между сложностью модели и сложностью данных.
Разработка такого аппарата позволит проводить сравнительный анализ различных нейросетевых архитектур для выбора оптимального решения для заданной задачи, характеризуемой выборкой определенной сложности.

В настоящей работе разработан комплексный теоретический подход к оценке сложности моделей и данных.
Предложен новый математический аппарат для анализа сложности на основе анализа ландшафта функции потерь нейросетевых моделей через матрицы Гессе.
Получены теоретические оценки матриц Гессе для различных архитектур моделей глубокого обучения, необходимые для оценки сложности модели.
Разработан анализ связи объема и сложности выборки со сложностью модели, введены частные случаи общей теории сложности, позволяющие получать практические оценки для прикладных задач.

\paragraph{Объектом исследования} являются параметрические семейства функций, задаваемые суперпозициями линейных и нелинейных преобразований, а также конечные выборки данных, применяемые для оценки параметров указанных семейств в рамках задачи минимизации эмпирического риска.

\paragraph{Предмет исследования:} разработка теоретического аппарата для оценки и анализа сложности моделей глубокого обучения и сложности данных, а также установление формальных критериев соответствия между сложностью модели и сложностью выборки, необходимой для ее обучения.

\paragraph{Цель и задачи исследования.}
Целью исследования является построение единого теоретического аппарата для оценки сложности моделей глубокого обучения и сложности данных, а также установление формальных критериев соответствия между сложностью модели и сложностью выборки, необходимой для ее обучения.

Для достижения цели были поставлены и решены следующие задачи:
\begin{itemize}
    \item[--] введение формальных определений мер сложности моделей и данных в рамках теории мер и установление критерия обучаемости модели на выборке;
    \item[--] получение теоретических оценок ландшафтных мер на основе матриц Гессе для полносвязных, сверточных и трансформерных архитектур моделей глубокого обучения;
    \item[--] построение ландшафтной меры сложности модели на основе анализа матриц Гессе и установление ее связи с условной сложностью выборки;
    \item[--] построение методов оценки достаточного объема выборки на основе анализа стабильности функции потерь и близости апостериорных распределений параметров;
    \item[--] построение методов снижения сложности моделей глубокого обучения на основе анализа матриц Гессе и методов дистилляции знаний;
    \item[--] демонстрация практического применения построенного теоретического аппарата в задачах многозадачного обучения, нейровизуализации и детекции машинно-генерированного контента.
\end{itemize}

\paragraph{Методы исследования.}
Для решения поставленных задач в диссертации используются методы теории мер, линейной алгебры, матричного анализа, включая матричное дифференцирование и спектральный анализ матриц, методы теории вероятностей, математической статистики, включая байесовский анализ, анализ сходимости случайных процессов, методы теории оптимизации, анализа функций многих переменных, методы статистической теории обучения.

\paragraph{Научная новизна.}
Научной новизной проведенного исследования является построение единого теоретического аппарата для оценки сложности моделей глубокого обучения и сложности данных на основе теории мер и анализа ландшафта оптимизационной задачи.
Впервые введены формальные определения меры сложности выборки и меры сложности модели в рамках теории мер, установлен критерий обучаемости модели на выборке, определяющий необходимое условие предотвращения переобучения.
Введена ландшафтная мера сложности модели, определяемая через спектральные свойства матриц Гессе функции потерь, и установлена ее связь с условной сложностью выборки.
Получены строгие теоретические оценки ландшафтных мер на основе оценок матриц Гессе для полносвязных, сверточных и трансформерных архитектур моделей глубокого обучения.
Построены методы оценки достаточного объема выборки на основе анализа стабильности функции потерь и близости апостериорных распределений параметров.

\paragraph{Теоретическая значимость работы.}
Диссертационная работа представляет собой фундаментальный теоретический вклад в теорию статистического обучения, расширяющий классические подходы к оценке сложности моделей на случай перепараметризованных нейронных сетей.
В работе рассматриваются фундаментальные вопросы о сложности моделей машинного обучения, для которых получены строгие теоретические результаты, связывающие сложность модели со сложностью данных в рамках единого математического аппарата.
Полученные оценки матриц Гессе и ландшафтной меры сложности открывают новые направления исследований в теории выбора моделей машинного обучения, теории оптимизации нейронных сетей и анализе обобщающей способности моделей глубокого обучения.

\paragraph{Практическая значимость работы.}
Разработанный теоретический аппарат применен к решению прикладных задач машинного обучения: многозадачного обучения, декодирования фМРТ-изображений и детекции машинно-генерированного контента.
Полученные методы оценки и управления сложностью моделей и данных экспериментально подтверждены на реальных задачах компьютерного зрения, обработки естественного языка и классификации.

\paragraph{Положения, выносимые на защиту:}
\begin{enumerate}
    \item Разработанный единый теоретический аппарат оценки сложности моделей глубокого обучения и сложности данных на основе теории мер и анализа ландшафта оптимизационной задачи, включающий формальные определения мер сложности и критерий обучаемости модели на выборке.
    \item Введенная ландшафтная мера сложности модели, определяемая через спектральные свойства матриц Гессе функции потерь, задает количественную связь между архитектурными характеристиками модели и условной сложностью выборки.
    \item Полученные теоретические оценки ландшафтной меры сложности на основе анализа спектральных норм матриц Гессе для полносвязных, сверточных раскрывают характер зависимости сложности моделей от их глубины, ширины и иных структурных параметров.
    \item Получены теоретические оценки сходимости методов определения достаточного объема выборки, для которых установлена связь со сложностью моделей
    \item Предложенные методы снижения сложности моделей глубокого обучения на основе анализа ковариационной матрицы градиентов, дистилляции на многодоменных данных и анти-дистилляции обеспечивают эффективное сокращение числа параметров и передачу знаний между моделями различной сложности и доменами данных.
\end{enumerate}

\paragraph{Степень достоверности результатов.}
Достоверность научных результатов работы подтверждается непротиворечивостью и согласованностью с известными фактами и исследованиями в рассматриваемой области, высокой степенью сходимости теоретических результатов с данными экспериментов и определяется применением теоретических и методологических основ разработок ведущих ученых в области обработки естественного языка, корректным и обоснованным использованием математического аппарата, экспериментальными исследованиями разработанных моделей и методов.

\paragraph{Соответствие диссертации паспорту специальности.}
Тема и основные результаты диссертации соответствуют следующим областям исследований паспорта специальности 1.2.1~--- Искусственный интеллект и машинное обучение.

2 Исследования в области оценки качества и эффективности алгоритмических и программных решений для систем искусственного интеллекта и машинного обучения. Методики сравнения и выбора алгоритмических и программных решений при многих критериях.

4 Разработка методов, алгоритмов и создание систем искусственного интеллекта и машинного обучения для обработки и анализа текстов на естественном языке, для изображений, речи, биомедицины и других специальных видов данных.

16 Исследования в области специальных методов оптимизации, проблем сложность и элиминации перебора, снижения размерности.

17 Исследования в области многослойных алгоритмических конструкций, в том числе – многослойных нейросетей.

\paragraph{Апробация результатов диссертации.}
Основные результаты работы докладывались и обсуждались на Всероссийской конференции с международным участием «Математические методы распознавания образов»~(Москва, 2019, Москва, 2021, Муром, 2025), Международной конференции «Интеллектуализация обработки информации»~(Гаэта, 2018, Москва, 2020, Москва, 2022), Всероссийской научной конференции МФТИ~(Москва, 2018, 2019, 2020, 2021, 2023, 2024, 2025), Ivannikov Ispras Open Conference~(Москва, 2021, 2022, 2023, 2024), Ivannikov Memorial Workshop~(Казань, 2022), Iberian Languages Evaluation Forum co-located with the Conference of the Spanish Society for Natural Language Processing~(Андалусия, 2023, 2024), 35th Conference of Open Innovations Association~(Тампере, 2024), Fourth Workshop on Scholarly Document Processing~(Бангкок, 2024), 1st Workshop on GenAI Content Detection (GenAIDetect)~(Абу-Даби 2025), 19th International Workshop on Semantic Evaluation~(Вена, 2025).

\paragraph{Публикации.}
По теме диссертации опубликовано 56 научных работ, из
которых 17 статей в научно-технических журналах, входящих в перечень ВАК, 32~--- в изданиях, входящих в международные наукометрические базы Scopus и Web of Science.
В трудах российских и международных конференций опубликовано 39 работ. Также на основе работ автора зарегистрировано 13 программ для ЭВМ.

\paragraph{Личный вклад соискателя.}
Все выносимые на защиту результаты и положения, составляющие основное содержание диссертационного исследования, разработаны и получены лично автором или при его непосредственном участии вместе с учениками.
В работах, опубликованных в соавторстве, соискателю принадлежит определяющая роль в построении теоретических методов и направлении исследований.

\paragraph{Структура и объем работы.}
Диссертация состоит из оглавления, введения, шести разделов, заключения, списка иллюстраций, списка таблиц, перечня основных обозначений и списка литературы из~\total{citnum} наименований. Основной текст занимает~\pageref{LastPage} страницы.

\paragraph{Краткое содержание работы по главам.}

В главе~\ref{chapter:curent-learnability-theory} рассматривается текущее состояние теории обучаемости моделей глубокого обучения.

В главе~\ref{chapter:complexity} вводятся общие определения меры сложности моделей и данных, а также понятие обучаемости модели, основанное на сравнении меры сложности модели и меры сложности данных.
Рассматривается условная сложность выборки, частным случаем которой при простой генеральной совокупности является достаточный объем выборки для обучения модели глубокого обучения; методы оценки достаточного объема выборки рассматриваются в главе~\ref{chapter:samplesize}.
Вводится понятие условной сложности моделей глубокого обучения, частным случаем которой является ландшафтная мера, анализирующая изменение ландшафта функции потерь при вариации обучающей выборки.
Ландшафтная мера основывается на исследовании матриц Гессе в окрестности локального минимума; подробные результаты по оценке матриц Гессе для специальных семейств моделей глубокого обучения рассматриваются в главе~\ref{chapter:gesian}.
В главе получена связь между достаточным объемом выборки, являющимся частным случаем условной сложности выборки, и ландшафтной мерой модели глубокого обучения.
Из полученных общих теоретических оценок получены оценки для полносвязных, сверточных и трансформерных архитектур моделей глубокого обучения.
Полученные оценки имеют асимптотический характер и указывают на экспоненциальный рост ландшафтной меры при увеличении числа слоев сети и полиномиальный рост при увеличении числа параметров внутри слоя.

В главе~\ref{chapter:gesian} описываются ключевые теоремы об оценках матриц Гессе для различных семейств моделей глубокого обучения.
Матрица Гессе функции ошибки по параметрам модели глубокого обучения содержит информацию о поведении функции вокруг некоторой точки и служит основой для введенной в главе~\ref{chapter:complexity} ландшафтной меры.
В рамках данной главы получены выражения для матриц Гессе и оценки их спектральных норм для полносвязных, сверточных и трансформерных архитектур моделей глубокого обучения.
Несмотря на технический характер главы, используемый в других главах диссертации, полученные оценки представляют самостоятельный научный результат, применимый в других направлениях исследования моделей глубокого обучения, в частности в теории оптимизации.

В главе~\ref{chapter:samplesize} предлагаются методы оценки достаточного объема выборки для линейных и нейросетевых моделей глубокого обучения.
В главе сравниваются классические и байесовские методы оценки достаточного объема выборки.
Предложен новый метод оценки достаточного объема выборки на основе семплирования эмпирической функции ошибки, для которого указана сходимость для линейной модели и эмпирически подтверждена работа для моделей глубокого обучения.
Также предложены методы определения достаточного размера выборки, основанные на близости апостериорных распределений близких выборок, с теоретическими оценками сходимости для любых параметрических семейств с нормальным апостериорным распределением параметров.

В главе~\ref{chapter:pruning} рассматриваются методы снижения сложности моделей глубокого обучения.
Предложены методы удаления параметров на основе анализа ковариационной матрицы градиентов функции ошибки по параметрам модели.
Рассматриваются методы дистилляции моделей глубокого обучения, в частности предложен метод дистилляции моделей на многодоменных данных.
Также предложен метод анти-дистилляции для наращивания сложности модели, при котором использование информации из обученной небольшой модели учителя при обучении большой модели ученика обеспечивает повышенную устойчивость к шуму и более высокую точность аппроксимации.

В главе~\ref{chapter:other-aplication} рассматриваются прикладные применения теории сложности моделей глубокого обучения.
Рассматривается оценка радемахеровской сложности в задаче многозадачного обучения; установлено, что использование LoRA-адаптеров снижает радемахеровскую сложность.
Приведен пример снижения размерности признакового описания без снижения сложности данных и качества аппроксимации модели на примере высокоразмерных фМРТ-снимков.
Рассмотрено качество данных в задаче детекции машинно-генерированного контента; с использованием введенных метрик продемонстрировано, что качество данных непосредственно влияет на оценку детекторов.

\paragraph{Благодарности.}
Автор благодарен научной школе академиков РАН Константина Владимировича Рудакова и Юрия Ивановича Журавлева, в частности своим учителям доктору физико-математических наук Вадиму Викторовичу Стрижову и профессору РАН Константину Вячеславовичу Воронцову за интерес к работе, советам и поддержку в исследованиях, связанных с данной докторской диссертацией.

Также автор благодарен компании АО <<Антиплагиат>>, где под руководством Юрия Викторовича Чеховича и Александра Сергеевича Кильдякова в отделе исследований проведено большое количество прикладных экспериментов, которые снизили общую сложность моделей глубокого обучения в сервисах компании.

Отдельно, автор признателен своим ученикам, аспирантам и студентам Герману Грицаю, Никите Киселеву, Данилу Дорину, Ильдару Хабутдинову, Владиславу Мешкову, Игорю Игнашину, Анастасии Вознюк, Анне Зверевой, Камилу Баязитову, Анне Ремизовой и Егору Петрову за обсуждение результатов, экспериментальную работу и за общее развитие теории моделей глубокого обучения и в частности их теорию сложности.