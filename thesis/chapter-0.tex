\section{Основные понятия статистического обучения}

В теории машинного обучения базовой является стохастическая постановка, в рамках которой данные рассматриваются как выборка из неизвестного распределения. Пусть задано вероятностное пространство $(\Omega, \mathcal{F}, \mathbb{P})$ и случайная пара $(\mathbf{X}, \mathbf{Y})$, принимающая значения в декартовом произведении пространств объектов и ответов $\mathcal{X} \times \mathcal{Y}$, с неизвестным совместным распределением $\mathbb{P}_{\mathbf{X},\mathbf{Y}}$. Генеральная совокупность $\Gamma$ задает множество всех возможных объектов, а кольцо выборок
\[
    \mathfrak{D} = \{D_\Gamma^i : D_\Gamma^i \subset \Gamma\}
\]
состоит из всех конечных подмножеств, доступных для анализа. В дальнейшем под выборкой будем понимать произвольный элемент $D \in \mathfrak{D}$ вида
\[
    D = \{(\mathbf{x}_1, \mathbf{y}_1), \ldots, (\mathbf{x}_m, \mathbf{y}_m)\},
\]
который рассматривается как реализация независимых одинаково распределенных случайных величин $(\mathbf{X}_i, \mathbf{Y}_i) \sim \mathbb{P}_{\mathbf{X},\mathbf{Y}}$.

Моделью (или гипотезой) называется отображение $f \colon \mathcal{X} \to \mathcal{Y}$, выбранное из фиксированного класса параметрических функций $\mathfrak{F}$. Для формализации качества модели вводится \textit{функция потерь} $\ell \colon \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}_{+}$, измеряющая несоответствие предсказания $f(\mathbf{x})$ и истинного ответа $\mathbf{y}$. В задачах классификации часто используют 0--1-потерю
\[
    \ell_{\mathrm{0\text{-}1}}(f(\mathbf{x}), \mathbf{y}) = \mathbb{I}\{f(\mathbf{x}) \neq \mathbf{y}\},
\]
а в задачах регрессии~--- квадратичную или абсолютную потери.

\begin{definition}[Риск модели]
    Риском модели $f \in \mathfrak{F}$ называют математическое ожидание потерь относительно истинного распределения данных:
    \[
        R(f) = \mathbb{E}_{(\mathbf{X},\mathbf{Y}) \sim \mathbb{P}_{\mathbf{X},\mathbf{Y}}}\left[\ell(f(\mathbf{X}), \mathbf{Y})\right].
    \]
\end{definition}

Так как распределение $\mathbb{P}_{\mathbf{X},\mathbf{Y}}$ неизвестно, прямой расчет $R(f)$ обычно невозможен. Вместо этого вводят \textit{эмпирический риск} по выборке $D$:
\[
    \hat{R}_m(f) = \frac{1}{m} \sum_{i=1}^{m} \ell(f(\mathbf{x}_i), \mathbf{y}_i),
\]
который рассматривается как статистическая оценка истинного риска. Связь между $R(f)$ и $\hat{R}_m(f)$ является центральным объектом исследования в теории обучаемости.

\begin{remark}
    В главах~\ref{chapter:complexity}, \ref{chapter:gesian} и далее для эмпирического риска параметризованных моделей используется обозначение $\mathcal{L}_m(\boldsymbol{\theta})$, которое совпадает с $\hat{R}_m(f)$ при параметризации $f_{\boldsymbol{\theta}}$. В настоящей главе оба обозначения рассматриваются как взаимозаменяемые.
\end{remark}

\begin{definition}[Алгоритм минимизации эмпирического риска]
    Алгоритмом минимизации эмпирического риска называют отображение, которое по выборке $D \in \mathfrak{D}$ возвращает модель
    \[
        \hat{f}_{\mathrm{ERM}} \in \arg\min_{f \in \mathfrak{F}} \hat{R}_m(f).
    \]
\end{definition}

Даже если класс моделей достаточно богат, чтобы аппроксимировать зависимость между $\mathbf{X}$ и $\mathbf{Y}$, для конечной выборки $D$ может возникать \textit{переобучение}: алгоритм подстраивается под случайные шумы в данных, достигая малого эмпирического риска при высоком истинном риске. Поэтому важной задачей является построение оценок отклонения $|R(f) - \hat{R}_m(f)|$ для различных классов $\mathfrak{F}$ и схем выбора моделей.

\begin{definition}[Наилучшая модель в классе и избыточный риск]
    Модель $f^{*}$, минимизирующая риск в фиксированном классе $\mathfrak{F}$,
    \[
        f^{*} \in \arg\min_{f \in \mathfrak{F}} R(f),
    \]
    называется \textit{наилучшей в классе}. Величина
    \[
        R(f) - R(f^{*})
    \]
    называется \textit{избыточным риском} модели $f$.
\end{definition}

В идеальном случае теоретические оценки должны гарантировать, что избыточный риск $\bigl(R(\hat{f}_{\mathrm{ERM}}) - R(f^{*})\bigr)$ мал с высокой вероятностью при разумном объеме выборки $m$. При этом ключевую роль играет мера сложности класса моделей $\mathfrak{F}$, определяющая скорость сходимости эмпирического риска к истинному.

\begin{definition}[Обучаемость класса моделей]
    Пусть $\mathfrak{D}_m = \{D \subset \Gamma : |D| = m\}$ обозначает множество всех выборок размера $m$. \textit{Класс моделей} $\mathfrak{F}$ называется \textit{обучаемым} в заданной постановке, если существует алгоритм $A \colon \bigcup_{m=1}^{\infty} \mathfrak{D}_m \to \mathfrak{F}$, такой что для любого распределения $\mathbb{P}_{\mathbf{X},\mathbf{Y}}$ и любых $\varepsilon > 0$, $\delta \in (0,1)$ найдется такое число наблюдений $m(\varepsilon,\delta) \in \mathbb{N}$, что для любой выборки $D$ размера $m \geq m(\varepsilon,\delta)$ выполняется
    \[
        \mathbb{P}\left\{ R(A(D)) - R(f^{*}) \leq \varepsilon \right\} \geq 1 - \delta,
    \]
    где $f^{*} = \arg\min_{f \in \mathfrak{F}} R(f)$~--- наилучшая модель в классе $\mathfrak{F}$.
\end{definition}

Дальнейший анализ в рамках классических подходов концентрируется на поиске верхних оценок $m(\varepsilon,\delta)$ через различные характеристики сложности класса $\mathfrak{F}$: VC-размерность, ростовую функцию, радемахеровскую сложность и другие. Эти меры играют фундаментальную роль в теории обучаемости и подробно рассматриваются в следующем разделе.

\section{Классические меры сложности и их ограничения}

Первые строгие представления о сложности моделей сформировались в рамках статистической теории распознавания~\cite[глава~5]{vapnik1974StatTheory}, где ключевую роль сыграли понятия VC-размерности и структурного риска. Пусть задано пространство объектов $\mathcal{X}$ и класс булевых моделей $\mathfrak{F} \subset \{0,1\}^{\mathcal{X}}$.

\begin{definition}[Разделимость выборки]
    Набор точек $D = \{\mathbf{x}_1, \ldots, \mathbf{x}_m\} \subset \mathcal{X}$ называется \textit{разделимым} классом $\mathfrak{F}$, если для любого разметочного вектора $(\mathbf{y}_1,\ldots,\mathbf{y}_m) \in \{0,1\}^m$ существует модель $f \in \mathfrak{F}$ такая, что $f(\mathbf{x}_i) = \mathbf{y}_i$ для всех $i = 1,\ldots,m$.
\end{definition}

\begin{definition}[VC-размерность]
    VC-размерностью класса $\mathfrak{F}$ называется величина
    \[
        \mathrm{VC}(\mathfrak{F}) = \sup \left\{ m \in \mathbb{N} : \exists D \subset \mathcal{X},\ |D| = m,\ \text{$D$ разделим классом $\mathfrak{F}$} \right\}.
    \]
    Если множество в правой части пусто, полагаем $\mathrm{VC}(\mathfrak{F}) = 0$. Если супремум не достигается, полагаем $\mathrm{VC}(\mathfrak{F}) = +\infty$.
\end{definition}
На основе введенных понятий VC-размерности и разделимости удалось связать обобщающую способность алгоритмов с ростовой функцией $\Pi_{\mathfrak{F}}(m)$ и получить верхние оценки разности между эмпирическим риском $\hat{R}_m(f)$ и истинным риском $R(f)$ вида
\[
    R(f) \leq \hat{R}_m(f) + C \sqrt{\frac{\mathrm{VC}(\mathfrak{F}) \log m}{m}},
\]
где $C > 0$ некоторая универсальная константа. Однако такие оценки используют грубые верхние границы, мало чувствительные к структуре конкретной задачи и распределения данных.

Развитие PAC-подхода дополнило картину формальными критериями обучаемости~\cite[раздел~2]{valiant1984LearnableTheory}.

\begin{definition}[PAC-обучаемость]
    Класс моделей $\mathfrak{F}$ называется \textit{PAC-обучаемым} (англ. Probably Approximately Correct learnable), если существует алгоритм $A \colon \bigcup_{m=1}^{\infty} \mathfrak{D}_m \to \mathfrak{F}$ и функция $m \colon (0,1) \times (0,1) \to \mathbb{N}$, такие что для любого распределения $\mathbb{P}_{\mathbf{X},\mathbf{Y}}$ на $\mathcal{X} \times \{0,1\}$ и любых $\varepsilon > 0$, $\delta \in (0,1)$ для любой выборки $D$ размера $m \geq m(\varepsilon,\delta)$ выполняется
    \[
        \mathbb{P}\left\{ R(A(D)) - R(f^{*}) \leq \varepsilon \right\} \geq 1 - \delta,
    \]
    где $f^{*} = \arg\min_{f \in \mathfrak{F}} R(f)$. При этом функция $m(\varepsilon,\delta)$ должна быть полиномиальной по $1/\varepsilon$ и $\log(1/\delta)$.
\end{definition}

Для класса $\mathfrak{F}$ с конечной VC-размерностью $d = \mathrm{VC}(\mathfrak{F}) < \infty$ известна оценка~\cite[раздел~2]{valiant1984LearnableTheory}
\[
    m(\varepsilon,\delta) \geq C \frac{1}{\varepsilon^2}\left(d \log \frac{1}{\varepsilon} + \log \frac{1}{\delta}\right),
\]
где $C > 0$~--- универсальная константа. Для реальных данных высокой размерности классические PAC-оценки остаются слабыми: они требуют огромных выборок даже для относительно простых архитектур и не учитывают внутреннюю структуру параметров, регуляризацию и специфику процесса оптимизации.

Более тонким инструментом анализа является радемахеровская сложность, связывающая способность класса функций к переобучению с эмпирическими оценками~\cite[раздел~2]{koltchinskii2000RadamakherTheory}.

\begin{definition}[Эмпирическая радемахеровская сложность]
    Пусть $\mathcal{F}$~--- класс вещественных функций на $\mathcal{X}$, $D = \{\mathbf{x}_1,\ldots,\mathbf{x}_m\} \subset \mathcal{X}$~--- конечная выборка, и $\sigma_1,\ldots,\sigma_m$~--- независимые случайные переменные Радемахера, то есть $\mathbb{P}\{\sigma_i = +1\} = \mathbb{P}\{\sigma_i = -1\} = 1/2$ для всех $i = 1,\ldots,m$. \textit{Эмпирическая радемахеровская сложность} класса $\mathcal{F}$ на выборке $D$ определяется как
    \[
        \hat{\mathfrak{R}}_D(\mathcal{F}) = \mathbb{E}_{\sigma}\left[\sup_{f \in \mathcal{F}} \frac{1}{m} \sum_{i=1}^{m} \sigma_i f(\mathbf{x}_i)\right],
    \]
    где математическое ожидание берется по распределению случайных переменных Радемахера.
\end{definition}

\begin{definition}[Ожидаемая радемахеровская сложность]
    \textit{Ожидаемая радемахеровская сложность} класса $\mathcal{F}$ для выборок размера $m$ определяется как
    \[
        \mathfrak{R}_m(\mathcal{F}) = \mathbb{E}_{D \sim \mathbb{P}^m}\left[\hat{\mathfrak{R}}_D(\mathcal{F})\right],
    \]
    где математическое ожидание берется по распределению выборок $D$ размера $m$, порожденных распределением $\mathbb{P}$ на $\mathcal{X}$.
\end{definition}

Соответствующие неравенства связи с обобщающей способностью имеют вид
\[
    R(f) \leq \hat{R}_m(f) + 2 \hat{\mathfrak{R}}_D(\mathcal{F}) + 3 \sqrt{\frac{\log (2/\delta)}{2m}},
\]
что позволяет учитывать адаптивность класса к конкретной выборке. Однако даже эти оценки остаются в основном асимптотическими и практически не отражают поведение современных нейронных сетей, имеющих миллионы параметров и сложные регуляризационные схемы~\cite{macKay2003}. Недостаток адаптивности классических мер к глубине, нелинейностям и особенностям оптимизации создает разрыв между теорией и практикой, который необходимо закрыть в дальнейших разделах обзора.

\subsection{VC-размерность и ростовая функция}

Одним из центральных понятий классической теории обучаемости является \textit{ростовая функция} (англ. growth function) класса моделей.

\begin{definition}[Индуцированные разметки]
    Для конечной выборки $D = \{\mathbf{x}_1,\ldots,\mathbf{x}_m\} \subset \mathcal{X}$ множество \textit{индуцированных разметок} класса $\mathfrak{F}$ на выборке $D$ определяется как
    \[
        \mathfrak{F}|_D = \{(f(\mathbf{x}_1),\ldots,f(\mathbf{x}_m)) : f \in \mathfrak{F}\} \subseteq \{0,1\}^m.
    \]
\end{definition}

\begin{definition}[Ростовая функция]
    \textit{Ростовая функция} класса $\mathfrak{F}$ определяется как
    \[
        \Pi_{\mathfrak{F}}(m) = \max_{D \subset \mathcal{X},\, |D| = m} |\mathfrak{F}|_D|,
    \]
    где максимум берется по всем возможным выборкам $D$ размера $m$ из пространства $\mathcal{X}$.
\end{definition}
Тривиальная верхняя оценка имеет вид $\Pi_{\mathfrak{F}}(m) \leq 2^m$. Если VC-размерность класса конечна и равна $d = \mathrm{VC}(\mathfrak{F}) < \infty$, то лемма Сауэра~\cite[глава~5, лемма~5.1]{vapnik1974StatTheory} обобщает эту грубую оценку и дает полиномиальный рост числа реализаций:
\[
    \Pi_{\mathfrak{F}}(m) \leq \sum_{k=0}^{d} \binom{m}{k} \leq \left(\frac{em}{d}\right)^{d}, \quad \text{для всех } m \geq d.
\]
В сочетании с неравенствами концентрации это приводит к классическим оценкам равномерной сходимости эмпирического риска к истинному риску, которые лежат в основе принципа минимизации структурного риска.

Ключевым следствием конечности VC-размерности является возможность построения иерархии вложенных классов $\mathfrak{F}_1 \subset \mathfrak{F}_2 \subset \cdots$ с возрастающей сложностью, что позволяет формализовать баланс между смещением и дисперсией модели. Однако на практике вычисление или даже оценка VC-размерности для сложных архитектур оказывается нетривиальной задачей: для линейных классификаторов в $\mathbb{R}^d$ верно $\mathrm{VC}(\mathfrak{F}) = d+1$, но для многослойных нейронных сетей VC-размерность оценивается как степенная функция от числа параметров и может быть астрономически велика, что делает полученные границы очень слабыми.

Кроме того, классические оценки через $\mathrm{VC}(\mathfrak{F})$ не отражают влияние таких практических аспектов, как ранняя остановка, стохастическая оптимизация, нормализация и другие регуляризаторы, которые существенно улучшают обобщающую способность, но никак не уменьшают формальную VC-размерность класса.

\subsection{PAC-обучаемость: реализуемый и агностический случаи}

В рамках PAC-подхода (англ. Probably Approximately Correct) различают два основных случая обучаемости: реализуемый и агностический. Для 0--1-потерь в бинарной классификации PAC-обучаемость сводится к поиску гипотезы с малой ошибкой обобщения при ограниченном числе прецедентов.

В реализуемой постановке предполагается, что существует модель $f^{*} \in \mathfrak{F}$, для которой $R(f^{*}) = 0$, то есть данные порождаются без шума в рамках выбранного класса. В этой ситуации удается получить более сильные гарантии сходимости алгоритмов минимизации эмпирического риска:
\[
    \Pr\left\{\sup_{f \in \mathfrak{F}} |R(f) - \hat{R}_m(f)| > \varepsilon \right\} \leq \delta,
\]
при числе наблюдений $m(\varepsilon,\delta)$, логарифмически зависящем от $1/\delta$ и линейно зависящем от $\mathrm{VC}(\mathfrak{F})$.

Примером служат линейные классификаторы в $\mathbb{R}^d$: для гиперплоскостей VC-размерность равна $d+1$, и PAC-обучаемость достигается при $m = O((d/\varepsilon) \log(1/\varepsilon) + (1/\varepsilon) \log(1/\delta))$, что согласуется с персептроном Розенблатта. Связь с VC следует из леммы Vapnik--Chervonenkis~\cite[глава~5, теорема~5.1]{vapnik1974StatTheory}: если $\mathrm{VC}(\mathfrak{F}) = d < \infty$, то класс PAC-обучаем, с оценкой $m \geq (1/\varepsilon)(4d \log(13/\varepsilon) + \log(2/\delta))$.

В агностической постановке шум допускается, и оптимальная модель $f^{*}$ достигает ненулевого риска $R(f^{*}) > 0$. В этом случае стандартной целью является достижение неравенства вида
\[
    R(f) \leq R(f^{*}) + \varepsilon
\]
с вероятностью не менее $1 - \delta$. Соответствующие оценки на число наблюдений имеют тот же порядок, что и в реализуемом случае, но константы и зависимости от параметров модели оказываются менее благоприятными. Следует подчеркнуть, что классические PAC-оценки делают акцент на наихудшем распределении данных и не используют информацию о структуре реальных задач.

Следует отметить, что PAC-анализ, как правило, не учитывает вычислительную сторону задачи: существование алгоритма, удовлетворяющего PAC-критериям, не гарантирует его практическую реализуемость с полиномиальной сложностью. Для глубоких нейронных сетей это особенно важно, так как реальное обучение основано на приближенных численных процедурах, которые не обязательно находят глобальный минимум риска.

\subsection{Радемахеровская сложность и локальные оценки}

Переход к радемахеровской сложности был мотивирован стремлением получить более точные, зависящие от данных оценки сложности класса функций. В отличие от VC-размерности, которая является глобальной характеристикой $\mathfrak{F}$, величина $\hat{\mathfrak{R}}_D(\mathcal{F})$ зависит от конкретной выборки $D$ и потому позволяет учитывать геометрию данных. Через технику симметризации и цепочечные неравенства могут быть получены оценки общего вида
\[
    \mathbb{E}\left[\sup_{f \in \mathcal{F}} |R(f) - \hat{R}_m(f)|\right] \leq C \, \mathfrak{R}_m(\mathcal{F}),
\]
где $\mathfrak{R}_m(\mathcal{F})$~--- ожидаемая радемахеровская сложность, определенная выше.

Для типичных классов гладких функций могут быть получены явные верхние границы на $\mathfrak{R}_m(\mathcal{F})$ через нормы параметров и радиус области определения. Например, для линейных классификаторов
\[
    \mathcal{F} = \{x \mapsto \langle w, x \rangle : \|w\|_2 \leq B\},
\]
при ограничении $\|x_i\|_2 \leq R$ для всех $i$ верна оценка
\[
    \hat{\mathfrak{R}}_D(\mathcal{F}) \leq \frac{BR}{\sqrt{m}}.
\]
Для нейронных сетей аналогичные оценки строятся через произведения норм весов по слоям, что уже лучше согласуется с эмпирическими наблюдениями о роли регуляризации весов.

Однако и здесь сохраняется ряд принципиальных ограничений. Во-первых, получение достаточно точных верхних границ для глубоких архитектур приводит к сильно завышенным оценкам, поскольку приходится применять грубые неравенства при переходе от нелинейных слоев к линейным аппроксимациям. Во-вторых, стандартные радемахеровские оценки учитывают только статический класс функций и не отражают динамику обучения, в частности, влияние траектории градиентного спуска и стохастичности минибатчей. В-третьих, даже будучи зависящими от данных, эти оценки по-прежнему нацелены на наихудший случай и плохо согласуются с наблюдаемой в практике устойчивостью крупных моделей к шуму в данных.

\subsection{Комбинаторный подход Воронцова к снижению оценок Вапника--Червоненкиса}

Значительный вклад в развитие комбинаторной теории надежности обучения по прецедентам внес К.В. Воронцов, развивший идеи В.Н. Вапника и А.Я. Червоненкиса в русле комбинаторных границ для статистического обучения~\cite{vorontsov04qualdan}. В его докторской диссертации~\cite[раздел~2]{vorontsov2010doctoral} предложен комбинаторный подход к снижению оценок Вапника--Червоненкиса, основанный на учете структуры данных и специфики класса гипотез. Ключевая идея заключается в том, что классические VC-оценки используют worst-case анализ и не учитывают комбинаторные свойства конкретной выборки, что приводит к завышенным границам на вероятность переобучения.

В рамках комбинаторного подхода Воронцов вводит меры, такие как $\alpha$-расширение класса гипотез и энтропийные характеристики, которые позволяют получить более точные оценки вероятности переобучения. Для класса $\mathfrak{F}$ с конечным VC-размером $d$ комбинаторные оценки дают вероятность ошибки обобщения вида~\cite[раздел~1.5]{vorontsov2010doctoral}
\[
\Pr\{R(f) - \hat{R}_m(f) > \varepsilon\} \leq 8 \Pi_{\mathfrak{F}}(2m) e^{-m \varepsilon^2 / 32},
\]
где $\Pi_{\mathfrak{F}}(2m)$ контролируется комбинаторными свойствами прецедентов, а не только глобальной VC-размерностью. В отличие от классических оценок, которые зависят только от $\mathrm{VC}(\mathfrak{F})$ и размера выборки $m$, комбинаторный подход учитывает структуру данных через энтропийные характеристики, что позволяет получить более точные границы для конкретных задач.

Ключевым преимуществом комбинаторного подхода является возможность существенного снижения оценок по сравнению со стандартными границами Вапника--Червоненкиса. Это достигается за счет учета специфических свойств выборки, таких как коррелированность объектов, наличие кластерной структуры или дискретная природа данных. Воронцов также предложил алгоритмы для эмпирического расчета таких границ, что делает подход применимым к реальным задачам прецедентного обучения, в особенности в задачах распознавания образов, где данные имеют дискретную структуру.

Отдельно стоит подчеркнуть, что в диссертации Воронцова~\cite{vorontsov2010doctoral} установлена формальная связь комбинаторного подхода с радемахеровской сложностью~\cite{koltchinskii2000RadamakherTheory}. В работе~\cite[раздел~2.2.1]{vorontsov2010doctoral} показано, что комбинаторные оценки могут быть выражены через эмпирическую радемахеровскую сложность. Для класса функций $\mathfrak{F}$ и выборки $D$ верна оценка
\[
\hat{\mathfrak{R}}_D(\mathfrak{F}) \leq \sqrt{\frac{2 \log \Pi_{\mathfrak{F}}(m)}{m}},
\]
где $\Pi_{\mathfrak{F}}(m)$~--- ростовая функция класса $\mathfrak{F}$, контролируемая комбинаторными свойствами прецедентов. Обратно, комбинаторные границы на вероятность переобучения могут быть переформулированы через радемахеровскую сложность~\cite[раздел~2.2]{vorontsov2010doctoral}:
\[
\Pr\{R(f) - \hat{R}_m(f) > \varepsilon\} \leq \exp\left(-\frac{m \varepsilon^2}{8 \hat{\mathfrak{R}}_D^2(\mathfrak{F}) + 2\varepsilon}\right),
\]
что позволяет объединить преимущества обоих подходов: адаптивность к данным, характерную для радемахеровской сложности, и комбинаторную структуру, учитывающую специфику прецедентного обучения. Эта формальная связь демонстрирует, что комбинаторный подход не является изолированным методом, а естественным образом дополняет и развивает классические меры сложности, включая VC-размерность и радемахеровскую сложность.

Комбинаторный подход дополняет классический VC-анализ более адаптивными метриками, которые могут быть существенно ниже стандартных оценок, что особенно важно для практических приложений, где классические worst-case границы оказываются слишком консервативными. Этот подход подчеркивает роль комбинаторной оптимизации в контроле сложности и демонстрирует, что учет структуры данных позволяет получать более реалистичные оценки обобщающей способности моделей.

\subsection{Общие ограничения классических мер сложности}

Суммируя рассмотренные подходы, выделим следующие ключевые ограничения классических мер сложности в контексте современных моделей глубокого обучения.

Во-первых, большинство оценок строится в предположении фиксированного, конечномерного пространства признаков и не учитывает появление дополнительных структур, таких как сверточные фильтры, остаточные связи и механизмы внимания, которые существенно меняют эффективную сложность класса моделей.

Во-вторых, как VC-размерность, так и радемахеровская сложность ориентированы на worst-case анализ и не используют специфические свойства реальных распределений данных, такие как низкая размерность многообразия, коррелированность признаков или наличие сильных инвариантностей. В результате получаемые границы оказываются слишком консервативными и часто на несколько порядков хуже эмпирически наблюдаемых характеристик.

В-третьих, классические меры, как правило, рассматривают модель и данные раздельно: сложность гипотезы оценивается независимо от сложностей выборки, а влияние распределения на обучение учитывается лишь через общие вероятностные предположения. Для глубоких нейросетевых архитектур, где наблюдается сложное взаимодействие между параметрами, структурой данных и алгоритмом оптимизации, такое раздельное рассмотрение оказывается недостаточным для объяснения эмпирических фактов.

Четвертое ограничение заключается в том, что классические результаты в основном фокусируются на асимптотических режимах $m \to \infty$ и не описывают поведение моделей при конечных, но больших размерах выборок, характерных для реальных задач. Это особенно заметно при анализе крупных языковых и мультимодальных моделей, для которых наблюдаются устойчивые законы масштабирования качества от размера данных и модели, выходящие за рамки традиционных теоретических предсказаний.

\section{Современные подходы к анализу сложности нейросетей}

Развитие теории сложности нейросетей началось с изучения их аппроксимирующих свойств. В фундаментальной теореме Дж.\,Цибенко (англ. Cybenko theorem) показано, что многослойный перцептрон с регулируемым числом нейронов в скрытом слое способен аппроксимировать любую непрерывную функцию на компактном подмножестве $\mathbb{R}^n$ с произвольной точностью~\cite[теорема~1]{cybenko1989ApproximationBS}. Этот результат формально выражается следующим образом: для любой непрерывной функции $g \colon [0,1]^n \to \mathbb{R}$ и любого $\varepsilon > 0$ существует сеть вида
\[
    f(\mathbf{x}) = \sum_{j=1}^{N} \alpha_j \sigma(\mathbf{w}_j^\top \mathbf{x} + b_j),
\]
где $\sigma$~--- сигмоидальная нелинейность, такая что $\|f - g\|_{\infty} < \varepsilon$. Теорема Цибенко предоставила первое строгое обоснование репрезентативной способности нейронных сетей, однако не дала конструктивных оценок на число нейронов $N$ и не объяснила, как глубина влияет на эффективность представления функций.

Вскоре после этого Й.\,Хестад показал, что переход к многоуровневым архитектурам позволяет экспоненциально сократить число нейронов, необходимое для аппроксимации некоторых классов функций~\cite[глава~4]{hastad1987phd}. Формально, для любой фиксированной глубины $L$ существует семейство булевых функций $F_L$, для которого любая сеть глубины $L$ требует $\Omega\!\left(\exp(n^{1/(L-1)})\right)$ нейронов, тогда как сеть глубины $L+1$ реализует те же функции полиномиально малым числом параметров. Этот результат свидетельствует о принципиальной роли глубины: существуют функции, которые требуют экспоненциального числа нейронов в сети фиксированной глубины, но допускают компактное представление при добавлении дополнительных слоев. Дальнейшее развитие этих идей представлено в работах Й.\,Бенджио и Я.\,Лекуна~\cite{bengioLecun2007:scaling,begio2011exponetialdepth}, где обсуждалась экспоненциальная эффективность глубоких архитектур.

В более современных исследованиях Н.\,Коэна, А.\,Эльдана и др. показано, что глубина определяет не только число параметров, но и тип функций, которые допускают эффективное представление~\cite[теорема~3.1]{cohen2015OnTE}~\cite[теорема~1]{eldan2016exponentialcomplexity}. В частности, теорема Коэна устанавливает, что для тензорных разложений полиномов степени $d$ в сети с ReLU-нелинейностями минимальное число параметров $p_L$ на глубине $L$ удовлетворяет $p_L = \tilde{\Theta}(d^{L/2})$, что подчеркивает экспоненциальный рост выразительной силы с глубиной. Эльдан и Шамир в своей теореме доказывают существование функций, аппроксимируемых трехслойными сетями с $O(d^2)$ нейронами, но требующих $\Omega(2^d)$ нейронов в двухслойных сетях: формально, для класса полиномов $P_d$ на $[0,1]^n$ верна оценка
\[
\inf \{ \|f - g\|_{\infty} : f \text{ двухслойная сеть} \} \geq c \cdot 2^{-d},
\]
в то время как трехслойная сеть достигает $\|f - g\|_{\infty} \leq C \cdot d^{-1/2}$. Эти результаты не только подтверждают преимущество глубины, но и связывают ее с геометрией параметрического пространства: в узких сетях оптимизация сталкивается с ``проклятием размерности'', тогда как широкие сети позволяют эффективно захватывать нелинейные зависимости. Такие теоремы мотивируют анализ Hessian-спектра для оценки ``гладкости'' ландшафта и роли глубины в контроле сложности.

Работа Н.\,Коэна и А.\,Эльдана также указывает на экспоненциальную зависимость сложности сети от числа слоев: для определенного класса полиномов степени $d$ минимальное число скрытых блоков $N_L$ в сети глубины $L$ удовлетворяет условию
\[
    N_L \geq c\cdot \exp\left(\gamma \frac{d}{L}\right),
\]
где $c, \gamma > 0$ зависят от нормировки весов и выбора нелинейностей. При увеличении глубины до $L+1$ тот же класс функций достигается при
\[
    N_{L+1} \leq C \cdot d^{\alpha},
\]
то есть сложность падает до полиномиальной. Это означает, что для фиксированного уровня точности гибрид ``ширина/глубина'' должен подчиняться экспоненциальной зависимости: слишком малое число слоев приводит к взрывному росту числа параметров, тогда как рост глубины позволяет удерживать сложность в полиномиальных границах.

Параллельно исследовались подходы к количественной оценке сложности обучаемых представлений. Теоремы PAC-Bayes~\cite{mcallester2013book} связывают обобщающую способность моделей с дивергенциями между апостериорным и априорным распределениями параметров. Для нейросетей эти подходы применялись в контексте анализа плотности спектров Гессиана и флатнеса локальных минимумов. Несмотря на то, что PAC-Bayes оценки предоставляют гибкие вероятностные ограничения, на практике получение информативных границ требует сложного подбора априорного распределения и зачастую приводит к слабым числам.

Альтернативным направлением исследования является изучение динамики обучения через линейные аппроксимации, такие как нейронно-тангентное ядро (англ. NTK). Работы Артура Жако и коллег~\cite{NEURIPS2018_5a4be1fa} показали, что в пределе бесконечной ширины динамика градиентного спуска описывается линейным ядровым методом, что позволяет получить строгие результаты по обучаемости и обобщению в этом режиме. Подходы на основе NTK демонстрируют, что некоторые архитектуры обладают явным «линейным» режимом, однако не объясняют поведение конечных сетей и не учитывают ограниченные вычислительные ресурсы.

\subsection{Анализ ландшафта функции потерь и матрицы Гессе}

Активное развитие получило направление анализа ландшафта функции потерь (англ. loss landscape) и спектральных свойств матриц Гессе как меры сложности нейросетевых моделей, начиная с работ 2017 года. Фундаментальные работы Л. Сагуна~\cite{sagun2018empiricalanalysishessianoverparametrized} установили, что матрицы Гессе перепараметризованных нейронных сетей обладают характерной структурой: большинство собственных значений сосредоточены около нуля, а небольшое число больших собственных значений определяет кривизну ландшафта. Это наблюдение привело к пониманию, что эффективная размерность пространства параметров может быть существенно меньше формального числа параметров.

Ключевым результатом является декомпозиция матрицы Гессе на G-компоненту и H-компоненту, предложенная в работах~\cite{sagun2018empiricalanalysishessianoverparametrized,skorski2019chainruleshessianhigher}. G-компонента отражает кривизну функции потерь относительно выходов сети, тогда как H-компонента описывает кривизну самой нейронной сети. Эта декомпозиция позволяет анализировать вклад различных факторов в общую сложность оптимизационного ландшафта и связывает свойства Гессиана с обобщающей способностью моделей.

Эмпирические исследования показали, что минимумы с малым спектральным радиусом Гессиана коррелируют с лучшей обобщающей способностью~\cite{keskar2016large,dinh2017sharp}. Работа Н. Кескара~\cite{keskar2016large} установила связь между ``острыми'' минимумами и худшей обобщающей способностью при обучении большими батчами. Л. Динь~\cite{dinh2017sharp} показали, что ``плоские'' минимумы могут обобщаться лучше, хотя эта связь не является универсальной и зависит от архитектуры и данных.

Визуализация ландшафта функции потерь, предложенная Х. Ли~\cite{li2018visualizing}, позволила эмпирически исследовать геометрию оптимизационного пространства. Методы визуализации через фильтрацию случайных направлений и анализ одномерных сечений показали, что успешно обученные сети находятся в ``широких долинах'' с плавным ландшафтом, тогда как плохо обученные модели характеризуются ``острыми'' минимумами с высокой кривизной.

Структурный анализ матриц Гессе на больших масштабах был проведен в работах В. Папяна~\cite{papyan2019spectrumdeepnethessiansscale} и Б. Горбани~\cite{pmlr-v97-ghorbani19b}, которые исследовали динамику спектра Гессиана в процессе обучения и его зависимость от размера выборки. Было установлено, что распределение собственных значений Гессиана следует степенным законам, причем ``хвост'' распределения содержит информацию о критических направлениях в пространстве параметров.

Для сверточных нейронных сетей С.П. Сингх и коллеги~\cite{singh2023hessianperspectivenatureconvolutional} получили теоретические оценки структуры матриц Гессе, показав, что сверточная структура приводит к блочно-циркулянтным свойствам Гессиана. Для трансформеров В. Орманиец и коллеги~\cite{ormaniec2024attentionhessian} провели теоретический анализ Гессиана, связав его структуру с механизмами внимания и показав, как архитектурные особенности отражаются в спектральных свойствах.

Вычислительные методы для анализа Гессиана включают быстрый алгоритм умножения на Гессиан, предложенный Б. Перлмуттером~\cite{pearlmutter1994fast}, и библиотеку PyHessian~\cite{yao2020pyhessian}, позволяющую эффективно вычислять собственные значения и след матрицы Гессе для больших моделей. Эти инструменты сделали возможным эмпирический анализ ландшафта функции потерь для современных архитектур.

Теоретические результаты о структуре ландшафта были получены в работах Дж. Пеннингтона~\cite{pennington2017spectrum,pennington2018emergence}, который исследовал спектр матрицы Фишера и показал возникновение спектральной универсальности в глубоких сетях. Г. Гур-Ари и коллеги~\cite{gurari2018gradient} доказали, что градиентный спуск находит глобальные минимумы для достаточно широких сетей, что связано с ``плоскостью'' ландшафта в пределе бесконечной ширины.

Методы регуляризации, основанные на анализе Гессиана, включают Entropy-SGD~\cite{chaudhari2016entropy}, который смещает градиентный спуск в ``широкие долины'' ландшафта, и методы, использующие информацию о кривизне для адаптивной настройки шага обучения. С. Фор и С. Ястжебски~\cite{fort2019large,fort2019energy} исследовали крупномасштабную структуру ландшафта, показав наличие иерархии минимумов и связь между геометрией ландшафта и обобщающей способностью.

Несмотря на значительный прогресс, большинство результатов остаются эмпирическими и зависят от режима оптимизации, архитектуры и данных. Строгие теоретические связи между спектральными свойствами Гессиана и обобщающей способностью пока установлены лишь для ограниченных классов моделей, что мотивирует разработку новых теоретических подходов, способных объединить анализ ландшафта с мерами сложности данных и моделей.

\section{Эмпирические оценки и методы снижения сложности моделей}

\subsection{Эмпирические законы масштабирования}

С обоснованием потребности в больших данных связан анализ эмпирических законов масштабирования. Фундаментальная работа Дж.\,Каплана и коллег~\cite{kaplan2020ScalingLaws} установила, что функция потерь языковых моделей подчиняется степенным законам относительно числа параметров $N$, объема обучающих данных $D$ и вычислительного бюджета $C$. Эмпирически наблюдается зависимость вида
\[
L(N, D) = \left(\frac{N_c}{N}\right)^{\alpha_N} + \left(\frac{D_c}{D}\right)^{\alpha_D} + L_0,
\]
где $N_c$, $D_c$~--- критические значения параметров и данных, $\alpha_N \approx 0.076$, $\alpha_D \approx 0.095$~--- показатели степенных законов, а $L_0$~--- минимально достижимая потеря. Эти зависимости показывают, что для достижения заданного уровня качества необходимо синхронно масштабировать как модель, так и данные, причем влияние данных оказывается несколько сильнее.

Ключевым следствием является то, что при фиксированном вычислительном бюджете $C$ существует оптимальное распределение ресурсов между числом параметров и объемом данных. Работа Дж.\,Хоффмана и коллег~\cite{hoffmann2022Chinchila} формализовала эту задачу оптимизации. В рамках модели, где вычислительная стоимость обучения пропорциональна $C \propto N \cdot D$, авторы показали, что оптимальное соотношение между числом параметров $N^*$ и числом токенов $D^*$ подчиняется закону
\[
D^* = 20 \cdot N^*,
\]
то есть для оптимального использования вычислительных ресурсов объем данных должен быть примерно в 20 раз больше числа параметров. Это противоречит распространенной практике обучения очень больших моделей на относительно малых датасетах: например, модель GPT-3 с $N = 175 \times 10^9$ параметров обучалась на $D \approx 300 \times 10^9$ токенах, тогда как согласно методу Chinchilla оптимальным было бы $D^* \approx 3.5 \times 10^{12}$ токенов.

Экспериментальная валидация метода Chinchilla на моделях размером от 70 миллионов до 16 миллиардов параметров подтвердила, что при соблюдении соотношения $D = 20N$ достигается лучшее качество при том же вычислительном бюджете. В частности, модель Chinchilla-70B~\cite{hoffmann2022Chinchila} (70 миллиардов параметров, обученная на $1.4 \times 10^{12}$ токенах) превзошла модель Gopher-280B~\cite{rae2022scalinglanguagemodelsmethods} (280 миллиардов параметров, обученную на меньшем объеме данных) по большинству метрик, несмотря на в 4 раза меньшее число параметров. Это демонстрирует критическую важность баланса между сложностью модели и объемом данных, что напрямую связано с понятиями условной сложности данных и достаточного размера выборки, развиваемыми в настоящей диссертации.

Несмотря на практическую значимость, эти законы масштабирования остаются эмпирическими и не имеют строгого теоретического обоснования. Они подстраиваются под конкретную архитектуру, функцию потерь и метрики качества, и их применимость к другим типам моделей требует дополнительной валидации. Кроме того, метод Chinchilla предполагает фиксированную архитектуру и не учитывает адаптивные стратегии обучения, такие как обучение по учебному плану (англ. curriculum learning) или динамическое изменение размера модели в процессе обучения.

\subsection{Методы снижения сложности моделей: дистилляция и регуляризация}

Влияние регуляризации и структурных ограничений параметров анализируется через теории дистилляции, привилегированного обучения и методов снижения размерности. Классический подход к снижению сложности моделей~--- дистилляция знаний (англ. knowledge distillation), предложенная Дж. Хинтоном и коллегами~\cite{hinton2015}. В рамках данного подхода ``учитель'' передает знания ``ученику'' через ``мягкие'' вероятностные распределения на выходе, а не только через жесткие метки классов. Дистилляция интерпретируется как способ переноса сложности от сложной модели к компактной, позволяя сохранить качество при существенном уменьшении числа параметров.

Формально, задача дистилляции формулируется как минимизация функции потерь вида
\[
\mathcal{L}_{\mathrm{distill}} = \alpha \mathcal{L}_{\mathrm{hard}}(f_{\mathrm{student}}, \mathbf{y}) + (1-\alpha) \mathcal{L}_{\mathrm{soft}}(f_{\mathrm{student}}, f_{\mathrm{teacher}}),
\]
где $\mathcal{L}_{\mathrm{hard}}$~--- стандартная функция потерь между предсказаниями ученика и истинными метками, $\mathcal{L}_{\mathrm{soft}}$~--- функция потерь между ``мягкими'' выходами ученика и учителя, а $\alpha \in [0,1]$~--- параметр баланса. Температурный параметр $T > 1$ используется для ``размягчения'' распределений вероятностей, что позволяет ученику лучше усваивать структуру знаний учителя.

Д. Лопес-Пас и коллеги~\cite{lopez2016} предложили унифицированную теорию дистилляции и привилегированного обучения, показав, что дистилляция может быть интерпретирована как частный случай обучения с дополнительной информацией. Байесовский подход к дистилляции, развитый в работах~\cite{grabovoi2021bayesian609999432}, позволяет формализовать перенос знаний через дивергенции между апостериорными распределениями параметров учителя и ученика, что дает более строгие теоретические гарантии.

Строгие границы на избыточный риск для дистилляции пока существуют лишь для простых семей моделей, таких как линейные классификаторы или мелкие нейронные сети. Для глубоких архитектур большинство результатов остаются эмпирическими, хотя наблюдается устойчивая закономерность: правильно выполненная дистилляция позволяет достичь качества, близкого к учителю, при существенно меньшем числе параметров.

Помимо дистилляции, к методам снижения сложности относятся квантизация параметров, вырезание несущественных параметров на основе анализа важности, и методы структурной регуляризации, ограничивающие выразительную силу модели через архитектурные ограничения. Эти методы часто комбинируются с дистилляцией для достижения максимального сжатия моделей при сохранении качества.

\section{Проблемы существующих подходов и вклад диссертации}

Современная теория сложности нейросетей развивается в нескольких направлениях: (1) аппроксимативные теоремы, описывающие роль глубины и ширины; (2) вероятностные оценки обобщения (англ. PAC-Bayes~\cite{mcallester2013book}, англ. NTK~\cite{NEURIPS2018_5a4be1fa}), связывающие динамику оптимизации с геометрией параметрического пространства; (3) ландшафтные методы, изучающие спектральные свойства матриц Гессе и устойчивость к возмущениям; (4) эмпирические законы масштабирования~\cite{kaplan2020ScalingLaws,hoffmann2022Chinchila}, связывающие качество с ресурсами; (5) методы снижения сложности, включая дистилляцию и регуляризацию. Несмотря на существенный прогресс, эти подходы по-прежнему дают неполную картину и мотивируют разработку новых мер сложности, способных совместить свойства данных и модели в единой теории.

Анализ представленных в настоящей главе направлений исследования сложности моделей глубокого обучения выявляет ряд фундаментальных проблем, ограничивающих их практическую применимость и теоретическую строгость, решение которых является целью настоящей диссертации. 

Во-первых, классические меры сложности, основанные на VC-размерности, PAC-обучаемости и радемахеровской сложности, разработаны для моделей с ограниченным числом параметров и не учитывают специфику перепараметризованных нейронных сетей. Эти меры дают слишком пессимистичные оценки для глубоких архитектур, не отражая их реальную обобщающую способность. Комбинаторный подход Воронцова~\cite{vorontsov2010doctoral} позволяет снизить оценки Вапника--Червоненкиса, но остается применимым лишь к ограниченным классам моделей.

Во-вторых, существующие подходы рассматривают сложность модели и сложность данных изолированно, не устанавливая формальных критериев их соответствия. Эмпирические законы масштабирования (англ. scaling laws)~\cite{kaplan2020ScalingLaws,hoffmann2022Chinchila} выявляют корреляции между числом параметров, объемом данных и качеством модели, но не имеют строгого теоретического обоснования и не объясняют механизмы, лежащие в основе этих зависимостей. Отсутствует формальный критерий обучаемости, связывающий свойства модели с характеристиками данных.

В-третьих, анализ матриц Гессе и ландшафта функции потерь~\cite{sagun2018empiricalanalysishessianoverparametrized,keskar2016large,dinh2017sharp} выявил важные эмпирические закономерности, однако большинство результатов остаются эмпирическими и зависят от режима оптимизации, архитектуры и данных. Строгие теоретические связи между спектральными свойствами Гессиана и обобщающей способностью установлены лишь для ограниченных классов моделей. Существующие теоретические результаты либо слишком общие, либо применимы только к специфическим режимам. Отсутствуют точные оценки сложности для конкретных архитектур, учитывающие их структурные особенности и связывающие гиперпараметры архитектуры с мерой сложности.

В-четвертых, для моделей с миллионами и миллиардами параметров прямое вычисление и анализ матриц Гессе становится непрактичным из-за квадратичной сложности по памяти и вычислениям. Существующие методы аппроксимации~\cite{yao2020pyhessian} не дают теоретических гарантий точности и не позволяют получить аналитические оценки сложности. Законы масштабирования~\cite{kaplan2020ScalingLaws,hoffmann2022Chinchila} подстраиваются под конкретную архитектуру, функцию потерь и метрики качества, их применимость к другим типам моделей требует дополнительной валидации. Кроме того, они не учитывают адаптивные стратегии обучения и не объясняют, почему определенные соотношения между параметрами и данными являются оптимальными.

Таким образом, несмотря на существенный прогресс в различных направлениях исследования сложности моделей глубокого обучения, отсутствует единый формальный аппарат, способный связать сложность модели и данных в рамках строгой теоретической основы, применимой к широкому классу архитектур нейронных сетей и обеспечивающей вычислительно осуществимые методы оценки сложности.