Как было продемонстрировано в предыдущей главе, существующие подходы к анализу сложности моделей глубокого обучения характеризуются существенными ограничениями. Во-первых, классические меры сложности не учитывают специфику перепараметризованных нейронных сетей. Во-вторых, сложность модели и данных рассматриваются изолированно без формальных критериев их соответствия. В-третьих, анализ матриц Гессе остается в основном эмпирическим и не обеспечивает строгих теоретических связей для конкретных архитектур. В результате отсутствует единый формальный аппарат, способный связать сложность модели и данных в рамках строгой теоретической основы, применимой к широкому классу архитектур нейронных сетей.

Для преодоления указанных ограничений в настоящей главе разрабатывается единый теоретический формализм, устанавливающий формальное соотношение между сложностью модели и сложностью данных. Предлагаемый подход основан на введении мер сложности в рамках теории мер, что позволяет формализовать критерий обучаемости модели на выборке и установить строгие теоретические связи между свойствами архитектуры модели и характеристиками данных.

Ключевая идея настоящей главы заключается в установлении формального соотношения между мерой сложности модели $\mu_f(f)$ и мерой сложности данных $\mu_D(D)$, определяемого через условие обучаемости:
\begin{equation}
    \mu_f(f) \leq \mu_D(D),
\end{equation}
а также получение частных случаев, которые имеют более подробный практический и теоретический анализ.

Основным инструментом анализа в предложенном подходе служит изучение изменения функции потерь при непрерывном изменении выборки. Данный подход обеспечивает установление строгих теоретических связей между спектральными свойствами матриц Гессе и обобщающей способностью моделей, что восполняет пробел, выявленный в обзоре литературы. В разделе~\ref{chapter:complexity:loss} показывается, как абсолютное изменение функции потерь оценивается через спектральную норму матрицы Гессе:
\begin{equation}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{M_{\ell}}{k+1} + \frac{1}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2
\end{equation}

Результат, основанный на теоретических выкладках из главы~\ref{chapter:gesian}, обеспечивает формализацию понятия~\textit{условной сложности выборки}~$\mu_D(D_i|f)$ и установление критериев достаточности объема данных для обучения конкретной модели. Предлагаемый формализм обеспечивает вычислительно осуществимые методы оценки сложности, основанные на аналитических оценках спектральных свойств матриц Гессе. Это решает проблему непрактичности прямого вычисления матриц Гессе для крупных моделей.

Предлагаемый формализм не только углубляет теоретическое понимание процессов обучения глубоких нейронных сетей, но и обладает практической значимостью для разработки эффективных стратегий обучения, выбора архитектур моделей и планирования экспериментов. Результаты настоящей главы создают мост между теоретическим анализом оптимизационных свойств моделей и практическими аспектами их применения к реальным данным, открывая новые возможности для систематического подхода к проектированию и обучению сложных нейросетевых архитектур.

Структура главы организована следующим образом. В разделе~\ref{chapter:complexity:models-data} вводятся формальные определения меры сложности выборки и меры сложности модели в рамках теории мер, а также устанавливается критерий обучаемости модели на выборке. В разделе~\ref{chapter:complexity:sample-size} рассматривается частный случай меры сложности данных --- достаточный объем выборки, и вводится понятие условной сложности выборки. В разделе~\ref{chapter:complexity:loss} разрабатывается ландшафтная мера сложности модели на основе анализа сходимости функции потерь, и получены теоретические оценки для полносвязных, сверточных и трансформерных моделей. В разделе~\ref{chapter:complexity:experiments} представлены результаты вычислительных экспериментов, подтверждающие теоретические оценки.

\section{Оценка сложности моделей и данных}\label{chapter:complexity:models-data}

Как было указано во введении, существующие подходы рассматривают сложность модели и сложность данных изолированно, не устанавливая формальных критериев их соответствия. В настоящем разделе производится формализация мер сложности выборки и модели в рамках теории мер, что позволяет установить строгий критерий обучаемости, связывающий свойства модели с характеристиками данных.

\begin{definition}[Генеральная совокупность данных]\label{chapter:complex:def-gamma}
    Генеральной совокупностью данных~$\Gamma$ назовем произвольное множество объектов, которые исследуются в рамках той или иной задачи. В общем случае нет никаких ограничений на счетность множества генеральной совокупности.
\end{definition}

Определение~\ref{chapter:complex:def-gamma} позволяет работать как с однородными, так и с многородовыми генеральными совокупностями.

\begin{definition}[Однородная и многородная генеральная совокупность]\label{chapter:complex:def-gamma-modality}
    Генеральную совокупность~$\Gamma$ назовем однородной, если все объекты генеральной совокупности порождаются из одного распределения. В противном случае генеральную совокупность назовем~$k$-родной, где~$k$ является числом распределений, на основе которых была сгенерирована генеральная совокупность.
\end{definition}

В определении~\ref{chapter:complex:def-gamma-modality} примером двуродной генеральной совокупности служит выборка, состоящая из текстов и изображений в качестве объектов исследования. Современные большие языковые модели, которые одновременно обрабатывают тексты и изображения и называются многомодальными моделями, представляют собой пример работы с многородовыми генеральными совокупностями.

Пусть задана генеральная совокупность данных~$\Gamma$. Множество всех подмножеств объектов, образующих кольцо выборок, обозначим как:
\[
    \mathfrak{D} = \{D_\Gamma^i\}, \quad D_\Gamma^i \subset \Gamma.
\]

\begin{definition}[Мера сложности выборки]\label{chapter:complex:def-data-complexity}
    Мерой сложности выборки назовем отображение~$\mu_D,$ такое, что: 
    \[
        \mu_D(D_i) : \mathfrak{D}_\Gamma \to \mathbb{R}_+,
    \]
    удовлетворяющее свойству:
    \begin{align}
        \mu_D(D_i\cup D_j) \leq \mu_D(D_i)+\mu_D(D_j),
    \end{align}
    где равенство достигается при условии~$D_i\cap D_j=\emptyset.$
\end{definition}

Определение~\ref{chapter:complex:def-data-complexity} является классическим определением из теории меры, удовлетворяющим свойству конечной аддитивности. Конечной аддитивности достаточно, так как в исследованиях предполагается конечное число объектов при обучении моделей глубокого обучения.
Предполагается сравнение выборок только из одной генеральной совокупности, однако этим никак не ограничивается сама генеральная совокупность, и в определении возможны мультимодальные генеральные совокупности.

\begin{lemma}[Монотонность меры сложности выборки]\label{chapter:complex:lemma-monotonicity}
    Мера сложности выборки $\mu_D$, определенная в определении~\ref{chapter:complex:def-data-complexity}, обладает свойством монотонности: для любых выборок $D_1, D_2 \in \mathfrak{D}$ таких, что $D_1 \subseteq D_2$, выполняется неравенство
    \[
        \mu_D(D_1) \leq \mu_D(D_2).
    \]
\end{lemma}
\begin{proof}
    Пусть $D_1 \subseteq D_2$. Представим $D_2$ в виде объединения $D_2 = D_1 \cup (D_2 \setminus D_1)$, где $D_1 \cap (D_2 \setminus D_1) = \emptyset$. По свойству субаддитивности меры сложности выборки из определения~\ref{chapter:complex:def-data-complexity} имеем:
    \[
        \mu_D(D_2) = \mu_D(D_1 \cup (D_2 \setminus D_1)) \leq \mu_D(D_1) + \mu_D(D_2 \setminus D_1).
    \]
    Учитывая, что $D_1 \cap (D_2 \setminus D_1) = \emptyset$, по определению~\ref{chapter:complex:def-data-complexity} равенство в свойстве субаддитивности достигается:
    \[
        \mu_D(D_2) = \mu_D(D_1 \cup (D_2 \setminus D_1)) = \mu_D(D_1) + \mu_D(D_2 \setminus D_1).
    \]
    Поскольку мера сложности выборки принимает неотрицательные значения ($\mu_D : \mathfrak{D}_\Gamma \to \mathbb{R}_+$), имеем $\mu_D(D_2 \setminus D_1) \geq 0$. Следовательно,
    \[
        \mu_D(D_2) = \mu_D(D_1) + \mu_D(D_2 \setminus D_1) \geq \mu_D(D_1) + 0 = \mu_D(D_1),
    \]
    что и требовалось доказать.
\end{proof}

\begin{remark}
    Лемма~\ref{chapter:complex:lemma-monotonicity} устанавливает монотонность меры сложности выборки как следствие субаддитивности. Данное свойство является общим свойством мер конечных множеств в теории меры. Однако в случае бесконечных выборок монотонность может не выполняться автоматически и требует более строгих формулировок в определении меры выборок, таких как счетная аддитивность или непрерывность меры снизу. В рамках настоящей работы рассматриваются только конечные выборки, что соответствует практическим задачам обучения моделей глубокого обучения.
\end{remark}

Пусть задано множество параметрических аппроксимирующих моделей
\[
    \mathfrak{F} = \left\{f_i\right\},
\]
где каждое~$f_i$ является некоторым множеством параметрических функций. В определении~\ref{chapter:complex:def-model} вводится характеристика параметрического семейства функций~$f$, которые в дальнейшем рассматриваются в качестве моделей глубокого обучения.
\begin{definition}[Мера сложности модели]\label{chapter:complex:def-model}
    Мерой сложности модели~$f$ назовем отображение~$\mu_f(f_i)$:
    \[
        \mu_f(f_i) : \mathfrak{F} \to \mathbb{R}_+.
    \]
\end{definition}

Определение меры сложности модели~$f$ не является определением меры в общем случае, так как множество~$\mathfrak{F}$ не является кольцом. Поэтому данная мера представляет собой некоторое отображение, которое является характеристикой сложности.
В качестве примера можно указать, что число параметров модели удовлетворяет определению~\ref{chapter:complex:def-model}.

После введения определений меры сложности как для выборки, так и для модели, переходим к определению обучаемости модели на выборке, которое сформулировано в определении~\ref{chapter:complex:def-model-bound-data}.

\begin{definition}[Обучаемость модели на выборке]\label{chapter:complex:def-model-bound-data}
    Назовем модель $f\in\mathfrak{F}$ \textit{обучаемой} на выборке $D\in\mathfrak{D},$ если 
    \[
        \mu_f(f)\leq \mu_D(D).
    \]
\end{definition}

В определении~\ref{chapter:complex:def-model-bound-data} не вводится никакого ограничения на качество аппроксимации модели после обучения. Более подробно это будет определено для частных случаев мер в следующих разделах. 

Определение~\ref{chapter:complex:def-model-bound-data} имеет важную эмпирическую интерпретацию: сложность модели не должна превышать сложности данных, на которых она обучается. В противном случае возникает проблема переобучения, когда модель запоминает шум в данных вместо выявления значимых закономерностей. Указанный критерий формализует интуитивное представление о балансе между выразительной способностью модели и информационной емкостью данных.

В диссертационной работе предполагается исследовать частный случай меры сложности модели на основе оценок ландшафта оптимизационных задач, используя матрицы Гессе для различных нейросетевых архитектур, которые описаны в главе~\ref{chapter:gesian}. Подробнее о частном случае меры сложности см. раздел~\ref{chapter:complexity:loss}.

\begin{theorem}[Необходимое условие дообучаемости модели]\label{chapter:complex:theorem-finetunning}
    Если для исходной выборки~$D\in\mathfrak{D}$ выполняется условие~$\mu_f(f) \leq \mu_D(D)$, тогда для новой выборки~$D'\in\mathfrak{D}$ необходимое условие дообучаемости модели на объединенной выборке $D \cup D'$ имеет вид:
    \[
        \mu_f(f) - \mu_D(D) \leq \mu_D(D').
    \]
\end{theorem}
\begin{proof}
Доказательство основано на свойствах мер сложности и условии обучаемости модели. 

По определению обучаемости модели на выборке $D$ (определение~\ref{chapter:complex:def-model-bound-data}) имеем:
\[
    \mu_f(f) \leq \mu_D(D).
\]
При добавлении новых данных $D'$ к исходной выборке $D$ по лемме~\ref{chapter:complex:lemma-monotonicity} о монотонности меры сложности выборки получаем:
\[
    \mu_D(D) \leq \mu_D(D\cup D').
\]
Из свойства субаддитивности меры сложности выборки (определение~\ref{chapter:complex:def-data-complexity}) получаем:
\[
    \mu_D(D\cup D') \leq \mu_D(D)+\mu_D(D'),
\]
где равенство достигается при условии $D \cap D' = \emptyset$. Объединяя эти три неравенства, получаем цепочку:
\[
    \mu_f(f) \leq \mu_D(D) \leq \mu_D(D\cup D') \leq \mu_D(D)+\mu_D(D'),
\]
откуда, перенося $\mu_D(D)$ в левую часть, получаем окончательное неравенство:
\[
    \mu_f(f) - \mu_D(D) \leq \mu_D(D').
\]
Для того чтобы модель была обучаемой на объединенной выборке $D \cup D'$, необходимо выполнение условия $\mu_f(f) \leq \mu_D(D \cup D')$. Полученное неравенство $\mu_f(f) - \mu_D(D) \leq \mu_D(D')$ является необходимым для выполнения этого условия, что завершает доказательство.
\end{proof}

Данное неравенство демонстрирует, что оставшаяся емкость модели, определяемая как разность $\mu_f(f) - \mu_D(D)$ между сложностью модели и сложностью исходных данных, не превосходит сложности новых данных~$D'$. Указанное условие является необходимым для успешного дообучения модели на новых данных: если оставшаяся емкость превышает сложность новых данных, модель не сможет эффективно адаптироваться к расширенной выборке. Достаточность данного условия зависит от конкретного выбора мер сложности и может требовать дополнительных предположений о структуре данных и модели.

Введение формальных мер сложности моделей и данных создает теоретическую основу для решения практических задач проектирования архитектур нейронных сетей, планирования экспериментов и оптимизации процессов обучения.


\section{Достаточный объем выборки, как мера сложности данных}\label{chapter:complexity:sample-size}

В предыдущем разделе было введено определение обучаемости модели на выборке через условие $\mu_f(f) \leq \mu_D(D)$. Однако данное условие не учитывает важный аспект: одна и та же выборка данных может представлять различную сложность для разных архитектур моделей. 

Для учета этой зависимости в настоящем разделе вводится понятие условной сложности выборки, которое позволяет формализовать зависимость сложности данных от выбранной модели.

Ключевым понятием становится \textit{условная сложность выборки}:
\begin{equation}\label{chapter:complex:eq-data-submessure}
    \mu_D(D|f) : \mathfrak{D} \to \mathbb{R}_+,
\end{equation}
которая характеризует сложность данных $D\in\mathfrak{D}$ относительно заданной параметрической модели~$f$ и степень трудности выборки~$D$ для обучения модели $f$.

Мера сложности модели~$\mu_f(f)$ индуцирует меру сложности выборки следующим образом:
\begin{equation}\label{chapter:complex:eq-data-submessure-infinum}
    \mu_D(D|f) = \inf \{ \mu_D(D') : D' \subseteq D, \quad \mu_f(f) \leq \mu_D(D') \},
\end{equation}
то есть условная сложность выборки может быть задана как минимальная сложность данных, при которой модель $f$ остается обучаемой.

\begin{definition}[Условная сложность выборки]
    Условной сложностью выборки~$D$ относительно заданной параметрической модели~$f$ назовем отображение~\eqref{chapter:complex:eq-data-submessure}, определяемое выражением~\eqref{chapter:complex:eq-data-submessure-infinum}.
\end{definition}

Рассмотрим частный случай меры сложности данных~$\mu_D$, соответствующий определению достаточного объема выборки.
Предположим, что генеральная совокупность~$\Gamma_C$ состоит из объектов одинаковой сложности~$C$, то есть для каждого объекта $\gamma \in \Gamma_C$ выполняется:
\[
    \mu_D(\gamma) = C,
\]
где~$C\in\mathbb{R}_+$ --- некоторая агрегированная сложность одного объекта выборки. Указанное предположение представляет собой сильное ограничение и может не выполняться на практике, поскольку в реальных задачах различные объекты могут обладать разной сложностью.

\begin{remark}
Константа $C$ представляет собой стоимость одного объекта выборки в единицах сложности.
На практике $C$ может зависеть от характеристик генеральной совокупности~$\Gamma$ и должна калиброваться экспериментально.
\end{remark}

\begin{definition}[Простая генеральная совокупность]
    Однородную генеральную совокупность~$\Gamma_C$ назовем простой, если она состоит из объектов одинаковой сложности~$C.$
\end{definition}

\begin{theorem}[Мера сложности выборки из простой генеральной совокупности]
    Для простой генеральной совокупности~$\Gamma_C$ мера сложности любой выборки $D \subset \Gamma_C$ равна ее объему, умноженному на константу сложности:
    \[
        \mu_D(D) = C\cdot|D|.
    \]
\end{theorem}
\begin{proof}
    Доказательство состоит из двух частей: сначала покажем, что функция $\mu_D(D) = C \cdot |D|$ является мерой сложности выборки в смысле определения~\ref{chapter:complex:def-data-complexity}, затем покажем, что она единственным образом определяется условием $\mu_D(\gamma) = C$ для каждого объекта $\gamma \in \Gamma_C$.
    
    Проверим, что функция $\mu_D(D) = C \cdot |D|$ удовлетворяет определению меры сложности выборки~\ref{chapter:complex:def-data-complexity}.
    
    Поскольку $|D| \geq 0$ для любой выборки $D \subset \Gamma_C$, и константа $C \in \mathbb{R}_+$ по определению простой генеральной совокупности, имеем $\mu_D(D) = C|D| \geq 0$.
    
    Для любых выборок $D_1, D_2 \subset \Gamma_C$ имеем:
    \[
        \mu_D(D_1 \cup D_2) = C|D_1 \cup D_2|.
    \]
    По свойству мощности множеств $|D_1 \cup D_2| \leq |D_1| + |D_2|$, причем равенство достигается при $D_1 \cap D_2 = \emptyset$. Следовательно,
    \[
        \mu_D(D_1 \cup D_2) = C|D_1 \cup D_2| \leq C(|D_1| + |D_2|) = \mu_D(D_1) + \mu_D(D_2),
    \]
    причем равенство достигается при $D_1 \cap D_2 = \emptyset$, что соответствует определению~\ref{chapter:complex:def-data-complexity}.
    
    Покажем, что мера $\mu_D(D) = C \cdot |D|$ единственным образом определяется условием $\mu_D(\gamma) = C$ для каждого объекта $\gamma \in \Gamma_C$.
    
    Для произвольной выборки $D = \{\gamma_1, \gamma_2, \ldots, \gamma_n\} \subset \Gamma_C$ рассмотрим последовательность одноэлементных множеств $D_i = \{\gamma_i\}$ для $i = 1, \ldots, n$. По условию простой генеральной совокупности для каждого объекта $\gamma_i$ выполняется $\mu_D(\gamma_i) = C$, то есть $\mu_D(D_i) = C$ для всех $i = 1, \ldots, n$.
    
    Поскольку одноэлементные множества $D_i$ попарно не пересекаются ($D_i \cap D_j = \emptyset$ при $i \neq j$), по свойству конечной аддитивности меры сложности выборки (равенство в определении~\ref{chapter:complex:def-data-complexity} при непересекающихся множествах) имеем:
    \[
        \mu_D(D) = \mu_D\left(\bigcup_{i=1}^{n} D_i\right) = \sum_{i=1}^{n} \mu_D(D_i) = \sum_{i=1}^{n} C = C \cdot n = C \cdot |D|,
    \]
    что и требовалось доказать.
    
    Таким образом, функция $\mu_D(D) = C \cdot |D|$ является единственной мерой сложности выборки, удовлетворяющей условию простой генеральной совокупности и определению~\ref{chapter:complex:def-data-complexity}.
\end{proof}

Достаточный объем выборки представляет собой частный случай~\textit{условной сложности выборки}~--- минимальный объем данных из выборки~$D$, необходимый для обучения модели~$f$.

\begin{definition}[Достаточный размер выборки]
    Размер выборки $m^*$ называется \textit{достаточным} согласно критерию $T$, если $T$ выполняется для всех $k \geqslant m^*$.
\end{definition}

Исследование достаточного объема выборки является частным случаем предложенного определения меры сложности данных. Подробные методы оценки достаточного объема выборки рассматриваются в главе~\ref{chapter:samplesize}.


\section{Сходимость ландшафта оптимизационной задачи, как мера сложности модели}\label{chapter:complexity:loss}

В предыдущих разделах были введены общие определения мер сложности модели и данных, а также условной сложности выборки. Для практического применения этих определений необходимо построить конкретные вычислимые меры сложности. В настоящем разделе разрабатывается ландшафтная мера сложности модели, основанная на анализе сходимости функции потерь при увеличении объема выборки.

\begin{figure}[h!t]\center
    \centering
    \includegraphics[width=0.7\linewidth]{figures/chapter-1/losses_difference.pdf}
    \caption{Изменение функции потерь $\mathcal{L}_k(\boldsymbol{\theta})$ при добавлении нового объекта в выборку. Иллюстрация демонстрирует зависимость абсолютной разности $|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta})|$ от размера выборки $k$, что является основой для анализа сходимости ландшафта и определения ландшафтной меры сложности модели.}
    \label{fig-chapter-3-losses-difference}
\end{figure}

Рассмотрим выборку из простой генеральной совокупности~$\Gamma_C$:
\begin{equation}
    D = \left\{ (\mathbf{x}_i, \mathbf{y}_i) \right\}, \quad i = 1, \ldots, m, \quad \mathbf{x} \in \mathcal{X}, \ \mathbf{y} \in \mathcal{Y}, \quad D\subset \Gamma_C.
\end{equation}

Рассмотрим некоторое параметрическое отображение~$f_{\boldsymbol{\theta}}: \mathcal{X} \to \mathcal{Y},$ которое аппроксимирует условное распределение целевой переменной для заданного признакового описания объекта~$p(\mathbf{y}|\mathbf{x}).$ Параметры~$\boldsymbol{\theta}$ функции~$f_{\boldsymbol{\theta}}$ принадлежат пространству~$\mathbb{R}^{P},$ где~$P$ описывает число параметров отображения~$f_{\boldsymbol{\theta}}$.

Пусть для выбора оптимального вектора параметров~$\hat{\boldsymbol{\theta}}$ используется подход минимизации эмпирического риска:
\begin{equation}
    \hat{\boldsymbol{\theta}} = \arg\min_{\boldsymbol{\theta}} \mathcal{L}_m(\boldsymbol{\theta}),
\end{equation}
где функция эмпирического риска для выборки размера~$|D|=m$ задается в следующем виде: 
\begin{equation}
    \mathcal{L}_m(\boldsymbol{\theta}) = \frac{1}{m} \sum\limits_{i=1}^{m} \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), \mathbf{y}_i) \approx \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p(\mathbf{x}, \mathbf{y})} \left[ \ell(f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y}) \right],
\end{equation}
где функция~$\ell\left(\mathbf{z}, \mathbf{y}\right)$ описывает ошибку на одном объекте. Далее в качестве функции~$\ell$ будут рассматриваться либо кросс-энтропийная функция потерь, либо средняя квадратическая ошибка, в зависимости от рассматриваемой задачи и архитектуры модели.

Функция эмпирического риска~$\mathcal{L}_m(\boldsymbol{\theta})$ задает некоторую поверхность в пространстве параметров размерности~$P$. Изучение изменения этой поверхности при добавлении новых объектов данных позволяет количественно оценить влияние объема выборки на оптимизационный ландшафт.

Изменение значения функции потерь при добавлении одного объекта вычисляется следующим образом:
\begin{align}\label{chapter:complex:equation-difference}
    \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) &= \frac{1}{k+1}\sum_{i=1}^{k+1}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) - \frac{1}{k}\sum_{i=1}^k\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \\
    &= \frac{1}{k+1}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1})-\sum_{i=1}^{k}\frac{1}{k(k+1)}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \\
    &= \frac{1}{k+1} \left(\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \mathcal{L}_{k}(\boldsymbol{\theta})\right).
\end{align}

Дальнейшее исследование ландшафта нацелено на изучение данной разницы. Особый интерес представляют предельные свойства при стремлении размера выборки к бесконечности. Для дальнейших оценок данной разности вводится предположение~\ref{chapter:complex:assumption-local-optima-not-change}, которое подтверждается на практике, однако является достаточно сильным, что упрощает дальнейшие выкладки.

\begin{assumption}[Сохранение локальных минимумов]\label{chapter:complex:assumption-local-optima-not-change}
    Пусть $\boldsymbol{\theta}^*$ является локальным минимумом обеих эмпирических функций потерь $\mathcal{L}_{k}(\boldsymbol{\theta})$ и $\mathcal{L}_{k+1}(\boldsymbol{\theta})$, т.е.
    \[
        \nabla \mathcal{L}_{k}(\boldsymbol{\theta}^*) = \nabla \mathcal{L}_{k+1}(\boldsymbol{\theta}^*) = \mathbf{0}.
    \]
\end{assumption}

Теоретическое обоснование предположения~\ref{chapter:complex:assumption-local-optima-not-change} основано на нескольких фундаментальных принципах теории оптимизации и статистического обучения. 

Во-первых, при достаточно большом объеме выборки $k$ эмпирический риск $\mathcal{L}_{k}(\boldsymbol{\theta})$ сходится к истинному риску $R(\boldsymbol{\theta}) = \mathbb{E}_{(\mathbf{x},\mathbf{y}) \sim p(\mathbf{x},\mathbf{y})}[\ell(f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y})]$ в соответствии с законом больших чисел. При условии равномерной сходимости градиентов эмпирического риска к градиентам истинного риска локальные минимумы эмпирического риска сходятся к локальным минимумам истинного риска~\cite{vapnik1974StatTheory}. 

Во-вторых, при добавлении одного объекта к выборке размера $k$ изменение эмпирического риска имеет порядок $O(1/k)$, что следует из выражения~\eqref{chapter:complex:equation-difference}. Для достаточно больших $k$ указанное изменение становится пренебрежимо малым, и локальный минимум $\boldsymbol{\theta}^*$ функции $\mathcal{L}_{k}(\boldsymbol{\theta})$ остается в окрестности локального минимума функции $\mathcal{L}_{k+1}(\boldsymbol{\theta})$, при условии, что функция потерь является гладкой и локально выпуклой в окрестности $\boldsymbol{\theta}^*$. 

В-третьих, работы по анализу ландшафта функции потерь нейронных сетей~\cite{li2018visualizing,fort2019large} демонстрируют, что при достаточном объеме данных ландшафт функции потерь стабилизируется, и локальные минимумы становятся устойчивыми к малым возмущениям выборки. Указанное свойство особенно выражено для перепараметризованных моделей, где множество локальных минимумов образует связные многообразия~\cite{li2018visualizing}. 

При асимптотически большом объеме выборки указанное свойство не противоречит эмпирическим результатам, что подтверждается экспериментальной валидацией, представленной в разделе~\ref{chapter:complexity:experiments}.

Воспользуемся квадратичным приближением Тейлора для указанных выше функций потерь в окрестности точки $\boldsymbol{\theta}^*$. Предполагается, что разложение до второго порядка является достаточным для изучения локального поведения. Член первого порядка обращается в ноль, поскольку градиенты $\nabla \mathcal{L}_{k}(\boldsymbol{\theta}^*)$ и $\nabla \mathcal{L}_{k+1}(\boldsymbol{\theta}^*)$ равны нулю:
\begin{equation}\label{chapter:complex:equation-approx}
    \mathcal{L}_{k}(\boldsymbol{\theta}) \approx \mathcal{L}_{k}(\boldsymbol{\theta}^*) + \frac{1}{2} (\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top \mathbf{H}^{(k)}(\boldsymbol{\theta}^*) (\boldsymbol{\theta} - \boldsymbol{\theta}^*),
\end{equation}
где введено обозначение гессиана функции $\mathcal{L}_{k}(\boldsymbol{\theta})$ по параметрам $\boldsymbol{\theta}$ в точке $\boldsymbol{\theta}^*$ как $\mathbf{H}^{(k)}(\boldsymbol{\theta}^*) \in \mathbb{R}^{P \times P}$. Полный гессиан может быть записан как среднее значение гессианов отдельных членов эмпирической функции потерь:
\[
    \mathbf{H}^{(k)}(\boldsymbol{\theta}) = \nabla^2_{\boldsymbol{\theta}} \mathcal{L}_{k}(\boldsymbol{\theta}) = \frac{1}{k} \sum\limits_{i=1}^{k} \nabla^2_{\boldsymbol{\theta}} \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}).
\]

Используя полученное квадратичное приближение~\eqref{chapter:complex:equation-approx}, формула для разности потерь~\eqref{chapter:complex:equation-difference} принимает вид:
\begin{align}
    \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) &= \frac{1}{k+1} \left( \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right) +\\
    &\quad+ \frac{1}{k+1} (\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top \left( \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right) (\boldsymbol{\theta} - \boldsymbol{\theta}^*),
\end{align}
Используя неравенство треугольника, получаем следующую оценку:
\begin{align}\label{chapter:complex:equation-mod-difference-full}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| &\leqslant \frac{1}{k+1} \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| +\\
    &\quad+ \frac{1}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{align}

Первое слагаемое может быть легко ограничено константой, поскольку сама функция потерь принимает ограниченные значения. Выражение с гессианами требует более сложной оценки. Подробный анализ матриц Гессе для различных типов параметрических моделей глубокого обучения представлен в главе~\ref{chapter:gesian}. Анализ локальной сходимости ландшафта функции потерь основан на ее матрице Гессе.

Получаем выражение для анализа, описывающее поведение ландшафта функции потерь:
\begin{equation}\label{chapter:complex:equation-mod-difference}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{M_{\ell}}{k+1}+ \frac{1}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{equation}

Установлено, что анализ сходимости ландшафта оптимизационной задачи сводится к анализу нормы матрицы Гессе, который подробно разобран в главе~\ref{chapter:gesian}.

Оценка~\eqref{chapter:complex:equation-mod-difference} задает некоторое свойство параметрического семейства функций~$f$ на заданной выборке~$D$. Определим данное свойство как условную сложность модели~$f$ на выборке~$D$:
\begin{equation}\label{chapter:complex:equantion-subcomplex-model}
    \mu_f(f|D) : \mathfrak{F} \to \mathbb{R}_+,
\end{equation}
Более подробно рассмотрим частный случай условной меры сложности параметрического семейства функций~$f$ вида:
\begin{equation}\label{chapter:complex:equantion-subcomplex-model-surface}
    \mu_f(f|D) = \mathsf{E}_{\mathbf{x}_i\in D}\left\| \mathbf{H}_{i}(\boldsymbol{\theta}^*) -  \mathsf{E}_{\mathbf{x}_i\in D}\mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{equation}

\begin{definition}[Условная сложность параметрической модели]\label{chapter:complex:definition-subcomplex-model}
    Условной сложностью параметрической модели~$f$ относительно заданной выборки~$D$ назовем отображение~\eqref{chapter:complex:equantion-subcomplex-model}.
\end{definition}

\begin{definition}[Ландшафтная мера сложности]\label{chapter:complex:definition-subcomplex-model-surface}
    Ландшафтной мерой сложности параметрической функции~$f$ назовем условную сложность параметрической модели~$f$, заданную выражением~\eqref{chapter:complex:equantion-subcomplex-model-surface}.
\end{definition}

Определение~\ref{chapter:complex:definition-subcomplex-model} описывает прикладный способ задания сложности на параметрических семействах функций в контексте оптимизации на заданных выборках.
Условная сложность модели~$\mu_f(f|D)$ характеризует сложность архитектуры модели~$f$ при ее обучении на выборке данных $D$ и позволяет количественно оценить степень соответствия модели данным. При этом слишком простая модель может недообучаться, а слишком сложная~--- переобучаться.

Ландшафтная мера сложности~\ref{chapter:complex:definition-subcomplex-model-surface} представляет собой явный вид условной сложности, основанный на анализе оптимизационного ландшафта функции потерь. Выражение~\eqref{chapter:complex:equantion-subcomplex-model-surface} содержательно указывает на степень изменения кривизны функции потерь в окрестности оптимума при добавлении нового объекта данных.

Дальнейшее изложение в главе посвящено оценкам ландшафтной меры для различных параметрических моделей~$f$.
Все результаты основываются на анализе матриц Гессе, описанных в главе~\ref{chapter:gesian}.

Используя выражение~\eqref{chapter:complex:equation-mod-difference} и определение ландшафтной меры сложности, получаем следующую асимптотическую связь между этими оценками:
\begin{equation}\label{chapter:complex:relation-surface-subcomplex-model}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{M_{\ell}}{k+1}+ \frac{\mu_f(f|D)}{k+1}\left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2.
\end{equation}

\begin{lemma}[Связь ландшафтной меры и условной сложности выборки]\label{chapter:complex:lemma-surface-data-complexity}
    Пусть задан некоторый~$\varepsilon$, описывающий допустимое изменение ландшафта при добавлении одного объекта в некоторой окрестности оптимума~$\boldsymbol{\theta}^*$ радиуса~$R$,
    причем выборка~$D$ принадлежит простой генеральной совокупности~$\Gamma_C$.

    Тогда верно следующее соотношение между ландшафтной мерой~$\mu_f(f|D)$ и условной сложностью выборки~$\mu_D(D|f):$
    \[
        \mu_f(f|D) \geq \mu_D(D|f)\frac{\varepsilon}{CR^2} - \frac{M_\ell }{R^2}.
    \]
\end{lemma}
\begin{proof}
    Доказательство основано на связи между условной сложностью выборки и ландшафтной мерой сложности модели через анализ изменения функции потерь.
    
    Рассмотрим произвольную подвыборку $D' \subseteq D$, удовлетворяющую условию $\mu_f(f) \leq \mu_D(D')$, и пусть $k = |D'|$. По определению условной сложности выборки~\eqref{chapter:complex:eq-data-submessure-infinum} такая подвыборка существует, так как $\mu_D(D|f) = \inf \{\mu_D(D') : D' \subseteq D, \mu_f(f) \leq \mu_D(D')\}$.
    
    Из выражения~\eqref{chapter:complex:relation-surface-subcomplex-model} для подвыборки $D'$ размера $k$ в окрестности оптимума $\boldsymbol{\theta}^*$ радиуса $R$ имеем:
    \[
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{M_{\ell}}{k+1}+ \frac{\mu_f(f|D')}{k+1}\left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2.
    \]
    По условию леммы, допустимое изменение ландшафта при добавлении одного объекта в окрестности оптимума радиуса $R$ не превосходит $\varepsilon$. Следовательно, для любой точки $\boldsymbol{\theta}$ такой, что $\left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \leq R^2$, выполняется:
    \[
        \frac{M_{\ell}}{k+1} + \frac{\mu_f(f|D')}{k+1}R^2 \geq \varepsilon.
    \]
    Переписывая это неравенство относительно $\mu_f(f|D')$, получаем:
    \[
        \mu_f(f|D') \geq \frac{\varepsilon(k+1) - M_{\ell}}{R^2} = \frac{\varepsilon k}{R^2} + \frac{\varepsilon - M_{\ell}}{R^2}.
    \]
    
    Поскольку выборка $D$ принадлежит простой генеральной совокупности $\Gamma_C$, по определению простой генеральной совокупности (см. раздел~\ref{chapter:complexity:sample-size}) для любой подвыборки $D'$ выполняется $\mu_D(D') = C \cdot |D'| = C \cdot k$. Следовательно, $k = \frac{\mu_D(D')}{C}$.
    
    Подставляя $k = \frac{\mu_D(D')}{C}$ в полученное неравенство, имеем:
    \[
        \mu_f(f|D') \geq \frac{\varepsilon \mu_D(D')}{CR^2} + \frac{\varepsilon - M_{\ell}}{R^2} = \mu_D(D')\frac{\varepsilon}{CR^2} + \frac{\varepsilon - M_{\ell}}{R^2}.
    \]
    
    Поскольку данное неравенство выполняется для любой подвыборки $D' \subseteq D$, удовлетворяющей условию $\mu_f(f) \leq \mu_D(D')$, и учитывая, что $\mu_D(D|f) = \inf \{\mu_D(D') : D' \subseteq D, \mu_f(f) \leq \mu_D(D')\}$, для любой такой подвыборки $D'$ имеем:
    \[
        \mu_f(f|D') \geq \mu_D(D')\frac{\varepsilon}{CR^2} + \frac{\varepsilon - M_{\ell}}{R^2} \geq \mu_D(D|f)\frac{\varepsilon}{CR^2} + \frac{\varepsilon - M_{\ell}}{R^2},
    \]
    где последнее неравенство следует из того, что $\mu_D(D') \geq \mu_D(D|f)$ по определению инфимума.
    
    Рассмотрим теперь саму выборку $D$. Если $\mu_f(f) \leq \mu_D(D)$, то $D$ входит в множество подвыборок, для которых выполняется указанное неравенство, и мы получаем:
    \[
        \mu_f(f|D) \geq \mu_D(D)\frac{\varepsilon}{CR^2} + \frac{\varepsilon - M_{\ell}}{R^2} \geq \mu_D(D|f)\frac{\varepsilon}{CR^2} + \frac{\varepsilon - M_{\ell}}{R^2},
    \]
    где последнее неравенство следует из того, что $\mu_D(D) \geq \mu_D(D|f)$ по определению инфимума.
    
    Если же $\mu_f(f) > \mu_D(D)$, то по определению условной сложности выборки $\mu_D(D|f) = \inf \{\mu_D(D') : D' \subseteq D, \mu_f(f) \leq \mu_D(D')\}$ может быть больше $\mu_D(D)$ или бесконечным. В этом случае неравенство тривиально выполняется, так как правая часть может быть отрицательной или неограниченной, а левая часть $\mu_f(f|D)$ неотрицательна по определению.
    
    Учитывая, что $\varepsilon - M_{\ell} \geq -M_{\ell}$, в обоих случаях получаем окончательное неравенство:
    \[
        \mu_f(f|D) \geq \mu_D(D|f)\frac{\varepsilon}{CR^2} - \frac{M_{\ell}}{R^2},
    \]
    что и требовалось доказать.
\end{proof}


\subsection{Полносвязная нейросетевая модель глубокого обучения}

В настоящем подразделе получены оценки ландшафтной меры сложности для полносвязных нейронных сетей, являющихся базовой архитектурой глубокого обучения. Анализ основан на результатах главы~\ref{chapter:gesian}, а именно на теореме~\ref{theorem:hess-kiselev-lemma}, в которой показана асимптотика нормы матрицы Гессе от гиперпараметров полносвязной нейросетевой модели:
\[
    \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \propto L (hM)^{2L},
\]
из которой следует, что спектральная норма матрицы Гессе имеет полиномиальную зависимость от размера слоя и экспоненциальную зависимость от числа слоев.

\begin{theorem}[Сходимость ландшафта функции потерь для полносвязных сетей]\label{chapter:complex:theorem-kiselev-loss}
    Пусть параметры $\boldsymbol{\theta}$ выбраны так, что $\left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \leqslant R^2$ для некоторого $R > 0$. Если существует неотрицательная константа $M_{\ell}$ такая, что $\left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant M_{\ell}$ для всех объектов $i = 1, \ldots, m$ в наборе данных, то при выполнении условий Теоремы~\ref{theorem:hess-kiselev-theorem} справедливо:
    \begin{align}
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{2}{k+1}\left( M_{\ell} + \left( L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \frac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1} \right) R^2 \right),
    \end{align}
    причем выражение асимптотически стремится к~$0$, то есть
    \begin{align}
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \to 0~\text{при}~k \to \infty.
    \end{align}
    Таким образом, имеет место следующая пропорциональность:
    \begin{equation}\label{eq:rate}
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \propto \frac{L (hM)^{2L} R^2}{k}. 
    \end{equation}
\end{theorem}
\begin{proof}
    Доказательство основано на оценке разности функций потерь через выражение~\eqref{chapter:complex:equation-mod-difference-full}, которое получено из квадратичного приближения Тейлора и предположения~\ref{chapter:complex:assumption-local-optima-not-change}.
    
    Оценим первое слагаемое в выражении~\eqref{chapter:complex:equation-mod-difference-full}, используя неравенство треугольника для модуля разности:
    \begin{align}
        & \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant \\
        &\quad\leqslant \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) \right| + \left| \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right|.
    \end{align}
    Применяя неравенство треугольника к модулю суммы и используя условие ограниченности функции потерь $\left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant M_{\ell}$ для всех $i = 1, \ldots, m$, получаем:
    \begin{align}
        &\quad\leqslant \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) \right| + \frac{1}{k} \sum\limits_{i=1}^{k} \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant\\
        &\quad\leqslant M_{\ell} + \frac{1}{k} \sum\limits_{i=1}^{k} M_{\ell} = M_{\ell} + M_{\ell} = 2M_{\ell}.
    \end{align}
    Таким образом, первое слагаемое ограничено константой $2M_{\ell} = \mathcal{O}(1)$ при $k \to \infty$.
    
    Оценим норму разности матриц Гессе во втором слагаемом, используя свойства спектральной нормы матриц. Применяя неравенство треугольника для спектральной нормы, получаем:
    \begin{align}
        & \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2 \leqslant \\
        &\quad\leqslant \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) \right\|_2 + \left\| \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
    \end{align}
    Используя свойство субмультипликативности спектральной нормы и неравенство треугольника для суммы матриц, имеем:
    \begin{align}
        &\quad\leqslant \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) \right\|_2 + \frac{1}{k} \sum\limits_{i=1}^{k} \left\| \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
    \end{align}
    По теореме~\ref{theorem:hess-kiselev-theorem} о норме матрицы Гессе для полносвязных нейронных сетей, при выполнении условий теоремы существует константа $M_{\mathbf{H}}$ такая, что $\left\| \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2 \leqslant M_{\mathbf{H}}$ для всех $i = 1, \ldots, k+1$, где
    \[
        M_{\mathbf{H}} = L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \frac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1}.
    \]
    Следовательно,
    \[
        \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2 \leqslant M_{\mathbf{H}} + \frac{1}{k} \sum\limits_{i=1}^{k} M_{\mathbf{H}} = M_{\mathbf{H}} + M_{\mathbf{H}} = 2M_{\mathbf{H}}.
    \]
    Таким образом, второе слагаемое ограничено константой $2M_{\mathbf{H}} = \mathcal{O}(1)$ при $k \to \infty$.
    
    Подставляя полученные оценки в выражение~\eqref{chapter:complex:equation-mod-difference-full}, получаем:
    \[
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{2M_{\ell}}{k+1} + \frac{2M_{\mathbf{H}}}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2.
    \]
    По условию теоремы параметры $\boldsymbol{\theta}$ находятся в $R$-окрестности оптимума, то есть $\left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \leqslant R^2$. Следовательно,
    \[
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{2}{k+1}\left( M_{\ell} + M_{\mathbf{H}}R^2 \right).
    \]
    Подставляя выражение для $M_{\mathbf{H}}$, получаем требуемую оценку:
    \[
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{2}{k+1}\left( M_{\ell} + \left( L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \frac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1} \right) R^2 \right).
    \]
    
    Поскольку $M_{\ell}$ и $M_{\mathbf{H}}$ являются константами, не зависящими от $k$, имеем:
    \[
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{2(M_{\ell} + M_{\mathbf{H}}R^2)}{k+1} = \mathcal{O}\left(\frac{1}{k}\right) \to 0 \text{ при } k \to \infty,
    \]
    что доказывает асимптотическую сходимость к нулю.
    
    Из выражения для $M_{\mathbf{H}}$ и асимптотики нормы матрицы Гессе $\left\| \mathbf{H}_{i}(\boldsymbol{\theta}) \right\|_2 \propto L (hM)^{2L}$ (см. теорему~\ref{theorem:hess-kiselev-theorem}) следует, что $M_{\mathbf{H}} \propto L (hM)^{2L}$, где $h$ --- размер скрытого слоя, а $M$ --- константа, ограничивающая параметры и данные. Следовательно,
    \[
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \propto \frac{L (hM)^{2L} R^2}{k},
    \]
    что завершает доказательство.
\end{proof}

На основе теоремы~\ref{chapter:complex:theorem-kiselev-loss} и использованных в ее доказательстве оценок нормы матрицы Гессе получается выражение для ландшафтной меры полносвязной нейросетевой модели глубокого обучения, которое сформулировано в следствии~\ref{chapter:complex:corollary-theorem-kiselev-loss}.

\begin{corollary}[Асимптотика ландшафтной меры для полносвязных сетей]\label{chapter:complex:corollary-theorem-kiselev-loss}
    Ландшафтная мера сложности параметрической функции~$f$ полносвязной нейросетевой модели глубокого обучения имеет асимптотику:
    \[
        \mu_f(f|D) \propto L(hM)^{2L},
    \]
    где~$M$ --- некоторая константа, ограничивающая параметры и данные.
\end{corollary}

Теорема~\ref{chapter:complex:theorem-kiselev-loss} и следствие~\ref{chapter:complex:corollary-theorem-kiselev-loss} устанавливают, что сходимость ландшафта функции потерь для полносвязных сетей происходит со скоростью $O(L(hM)^{2L}/k)$ при увеличении размера выборки $k$. Это означает, что для более глубоких сетей требуется больше данных для стабилизации ландшафта, что согласуется с интуитивным представлением о том, что сложные модели требуют большего объема данных для обучения.

В работе~\cite{eldan2016exponentialcomplexity} показано, что существуют функции, которые могут быть эффективно представлены трехслойными сетями, но требуют экспоненциального числа нейронов в двухслойных сетях. Следствие~\ref{chapter:complex:corollary-theorem-kiselev-loss} указывает на схожий результат: сложность модели увеличивается экспоненциально при увеличении числа слоев ($\mu_f(f|D) \propto L(hM)^{2L}$), и, следовательно, при уменьшении числа слоев для сохранения заданной сложности модели потребуется экспоненциальный рост параметров. Это демонстрирует фундаментальный компромисс между глубиной и шириной нейронных сетей с точки зрения их выразительной способности.

\subsection{Сверточные модели глубокого обучения}

В настоящем подразделе получены оценки ландшафтной меры сложности для сверточных нейронных сетей, широко применяющихся в задачах обработки последовательностей и изображений. Анализ ландшафта основан на результатах о матрицах Гессе, представленных в главе~\ref{chapter:gesian}.

Начнем с анализа 1D-сверточных сетей, применяющихся в обработке последовательностей, временных рядов и сигналов. Основные результаты для определения ландшафтной меры сложности сформулированы в теореме~\ref{chapter:complex:theorem-1Dconv-loss}.

\begin{theorem}[Сходимость ландшафта для 1D-сверточных сетей]\label{chapter:complex:theorem-1Dconv-loss}
Пусть параметры~$\boldsymbol{\theta}$ находятся в $R$-окрестности оптимума:
\[
    \|\boldsymbol{\theta} - \boldsymbol{\theta}^*\| \leqslant R,
\]
а функция потерь ограничена некоторой константой:
\[
    \exists \; M_{\ell} > 0: \; \forall i\; |\ell_i| \leqslant M_{\ell}.
\]
Пусть все объекты в наборе данных ограничены:
\[
    \exists M_{\mathbf{x}} > 0\; \forall i \; \|\mathbf{x}_i\| \leqslant M_{\mathbf{x}}.
\]
Тогда в условиях теоремы~\ref{thm:1Dconv}:
\begin{align}
    & \big|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_{k}(\boldsymbol{\theta})\big| \leqslant \frac{2}{k+1}M_{\ell} +\\
    & + \frac{2}{k+1}R^2\sqrt{2}d^2M_{\mathbf{x}}^2(L+1)(C^2w^2kd)^L,
\end{align}
где $M_{\mathbf{x}}$ --- константа, ограничивающая нормы объектов данных.
\end{theorem}
\begin{proof}
    Доказательство проводится аналогично доказательству теоремы~\ref{chapter:complex:theorem-kiselev-loss} с подстановкой оценок нормы матрицы Гессе из теоремы~\ref{thm:1Dconv}.
\end{proof}

На основе теоремы~\ref{chapter:complex:theorem-1Dconv-loss} и использованных в ее доказательстве оценок нормы матрицы Гессе получается выражение для ландшафтной меры 1D-сверточной нейросетевой модели, которое сформулировано в следствии~\ref{chapter:complex:corollary-theorem-1Dconv-loss}.

\begin{corollary}[Асимптотика ландшафтной меры для 1D-сверточных сетей]\label{chapter:complex:corollary-theorem-1Dconv-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 1D-сверточной модели глубокого обучения имеет асимптотику:
    \[
        \mu_f(f|D) \propto L(C^2M^2kd)^{L},
    \]
    где~$C$~--- максимальное число каналов,~$d$~--- длина входной последовательности,~$k$~--- размер свертки,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{corollary}

Теорема~\ref{chapter:complex:theorem-1Dconv-loss} и следствие~\ref{chapter:complex:corollary-theorem-1Dconv-loss} показывают, что для 1D-сверточных сетей сходимость ландшафта происходит со скоростью $O(L(C^2M^2kd)^{L}/k)$. Полученные оценки демонстрируют, что сложность 1D-сверточных нейросетевых моделей экспоненциально зависит от глубины $L$ и полиномиально~--- от остальных гиперпараметров архитектуры.
Особенностью 1D-архитектур является линейная зависимость от длины последовательности $d$, что отражает специфику обработки последовательностей и отличает их от полносвязных сетей, где такая зависимость отсутствует.

Перейдем к анализу 2D-сверточных сетей, применяющихся в задачах обработки изображений. 

Основные результаты для определения ландшафтной меры сложности 2D-сверточных сетей сформулированы в теореме~\ref{chapter:complex:theorem-2Dconv-loss}.

\begin{theorem}[Сходимость ландшафта для 2D-сверточных сетей]\label{chapter:complex:theorem-2Dconv-loss}
Пусть параметры $\boldsymbol{\theta}$ находятся в $R$-окрестности оптимума:
\[
    \|\boldsymbol{\theta} - \boldsymbol{\theta}^*\| \leqslant R.
\]
Также функция потерь ограничена некоторой константой:
\[
    \exists \; M_{\ell} > 0: \; \forall i\; |\ell_i| \leqslant M_{\ell}.
\]
Пусть все объекты в наборе данных также ограничены:
\[
    \exists M_{\mathbf{x}} > 0\; \forall i \; \|\mathbf{x}_i\| \leqslant M_{\mathbf{x}}.
\]

Тогда при выполнении условий теоремы~\ref{thm:2Dconv} справедливо:
\begin{align}
    & \big|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_{k}(\boldsymbol{\theta})\big| \leqslant \frac{2}{k+1}M_{\ell} +\\
    & + \frac{2}{k+1}R^2\sqrt{2}q^2M_{\mathbf{x}}^2(L+1)(C^2k^2w^2mn)^L,
\end{align}
где $q^2 = C^2k^2mn$, а $M_{\mathbf{x}}$ --- константа, ограничивающая нормы объектов данных.
\end{theorem}
\begin{proof}
    Доказательство проводится аналогично доказательству теоремы~\ref{chapter:complex:theorem-kiselev-loss} с подстановкой оценок нормы матрицы Гессе из теоремы~\ref{thm:2Dconv}.
\end{proof}

На основе теоремы~\ref{chapter:complex:theorem-2Dconv-loss} и использованных в ее доказательстве оценок нормы матрицы Гессе получается выражение для ландшафтной меры 2D-сверточной нейросетевой модели, которое сформулировано в следствии~\ref{chapter:complex:corollary-theorem-2Dconv-loss}.

\begin{corollary}[Асимптотика ландшафтной меры для 2D-сверточных сетей]\label{chapter:complex:corollary-theorem-2Dconv-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 2D-сверточной модели глубокого обучения имеет асимптотику:
    \[
        \mu_f(f|D) \propto C^2k^2L(C^2k^2M^2mn)^{L},
    \]
    где~$C$~--- максимальное число каналов,~$m,n$~--- размеры входного изображения,~$k$~--- размер свертки,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{corollary}

Теорема~\ref{chapter:complex:theorem-2Dconv-loss} и следствие~\ref{chapter:complex:corollary-theorem-2Dconv-loss} устанавливают, что для 2D-сверточных сетей сходимость ландшафта происходит со скоростью $O(C^2k^2L(C^2k^2M^2mn)^{L}/k)$. Для 2D-сверточных сетей наблюдается более быстрый рост сложности по сравнению с 1D-архитектурами, что обусловлено двумерной природой данных: зависимость от размеров изображения $m \times n$ является квадратичной, в отличие от линейной зависимости от длины последовательности в 1D-сверточных сетях.

\subsection{Трансформер модели глубокого обучения}

Архитектура трансформеров представляет собой один из наиболее значительных прорывов в области глубокого обучения последних лет. Указанные модели демонстрируют state-of-the-art результаты в задачах обработки естественного языка, компьютерного зрения и других областях. Особенностью трансформеров является механизм самовнимания, который позволяет модели учитывать глобальные зависимости в данных независимо от их положения. В настоящем подразделе получены оценки ландшафтной меры сложности для трансформерных моделей, основанные на анализе матриц Гессе компонентов трансформера.

\begin{theorem}[Сходимость ландшафта для трансформеров]\label{chapter:complex:theorem-transformer-loss}
Для одного блока самовнимания и одного блока трансформера \ref{eq:transformer} при условии ограниченности функции потерь \[
    0 \leqslant \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_i), \mathbf{y}_i) \leqslant L,
\]
и ограниченности норм матриц Гессе справедливо:
\[
    \left| \mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w}) \right| \leqslant \frac{2L}{k+1} + \frac{M \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2}{(k+1)},
\]
где для блока самовнимания константа $M$ может быть непосредственно вычислена из Теоремы \ref{thm:self_attention_hessian_estimation},
а для блока трансформера \(M = M_{\text{tr}}\) вычисляется в соответствии с Теоремой~\ref{thm:transformer_hessian_estimate}.
\end{theorem}
\begin{proof}
Доказательство проводится в несколько этапов. На первом этапе оценивается разность значений функции потерь в оптимальной точке, на втором этапе оценивается разность гессианов. На третьем этапе, используя результаты обоих этапов, производится объединение оценок.

Рассмотрим разность эмпирических функций потерь при добавлении нового объекта. Используя разложение разности и свойства норм, получаем:
\begin{align}
    \left| \mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w}) \right| &\leqslant \frac{1}{k+1} \left| \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum_{i=1}^{k} \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| +\\
    &\quad+ \frac{1}{2 (k+1)} \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2 \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2.
\end{align}

Первое слагаемое характеризует изменение значения функции потерь в оптимальной точке параметров $\mathbf{w}^*$ при добавлении нового объекта.
Это разность между значением потерь на новом объекте и средним значением потерь на предыдущей выборке, до добавления нового объекта.
Предположим, что функция потерь $\ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_i), \mathbf{y}_i)$ ограничена сверху константой $L$ для всех объектов выборки:
\[
    0 \leqslant \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_i), \mathbf{y}_i) \leqslant L.
\]
Это предположение является естественным для большинства функций потерь, используемых в машинном обучении, таких как кросс-энтропия или среднеквадратичная ошибка.
Тогда для нового объекта выполняется:
\[
    \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) \leqslant L,
\]
а для среднего значения по предыдущей выборке:
\[
    \frac{1}{k} \sum_{i=1}^{k} \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \leqslant \frac{1}{k} \sum_{i=1}^{k} L = L.
\]
Используя неравенство треугольника для модуля разности, получаем:
\[
    \left| \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum_{i=1}^{k} \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant L + L = 2L.
\]
Таким образом, вклад первого слагаемого в общую оценку не превосходит:
\[
    \frac{1}{k+1} \left| \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum_{i=1}^{k} \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant \frac{2L}{k+1}.
\]

Второе слагаемое в оценке связано с изменением гессиана функции потерь. Рассмотрим выражение:
\[
    \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2,
\]
где $\mathbf{H}_{k+1}(\mathbf{w}^*) = \nabla^2_{\mathbf{w}} \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1})$~--- матрица Гессе функции потерь для нового объекта, а $\frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) = \mathbf{H}_k(\mathbf{w}^*)$~--- средняя матрица Гессе по всей предыдущей выборке.
Перепишем это выражение в более удобной форме:
\begin{align}
    \mathbf{H}_k(\mathbf{w}^*) &= \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*),\\
    \mathbf{H}_{k+1}(\mathbf{w}^*) - \mathbf{H}_k(\mathbf{w}^*) &= \mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*).
\end{align}
Для оценки нормы этой разности используем неравенство треугольника:
\[
    \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) \right\|_2 + \left\| \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2.
\]
Предположим, что выполняется ограниченность следующих матриц Гессе:
\[
    \left\| \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant M
\]
для некоторой константы $M$.
Тогда для гессиана нового объекта:
\[
    \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) \right\|_2 \leqslant M,
\]
а для суммы гессианов:
\[
    \left\| \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant \sum_{i=1}^{k} \left\| \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant k M.
\]
Следовательно:
\[
    \left\| \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant \frac{1}{k} \cdot k M = M.
\]
Объединяя полученные оценки, получаем:
\[
    \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant M + M = 2M.
\]
Теперь оценим вклад второго слагаемого в общую разность функций потерь:
\begin{align}
    \frac{1}{2 (k+1)} \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2 \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) - \mathbf{H}_k(\mathbf{w}^*) \right\|_2 &\leqslant \frac{2M}{2 (k+1)} \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2 = \\
    &=\frac{M \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2}{k+1}.
\end{align}


Комбинируя оценки для обоих слагаемых, получаем итоговую оценку:
\[
    \left| \mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w}) \right| \leqslant \frac{2L}{k+1} + \frac{M \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2}{k+1}.
\]
\end{proof}

\section{Результаты вычислительных экспериментов}\label{chapter:complexity:experiments}

В настоящем разделе представлены результаты вычислительных экспериментов, направленных на эмпирическую валидацию теоретических результатов, полученных в предыдущих разделах главы.

\begin{figure}[h!t]\center
    \includegraphics[width=0.5\linewidth]{thesis/figures/chapter-1/transformer/assumption}
    \caption{Проверка предположения~\ref{chapter:complex:assumption-local-optima-not-change} о сходимости локальных минимумов при увеличении объема выборки. График демонстрирует выполнимость предположения о том, что локальный минимум $\boldsymbol{\theta}^*$ функции потерь $\mathcal{L}_k(\boldsymbol{\theta})$ остается локальным минимумом функции $\mathcal{L}_{k+1}(\boldsymbol{\theta})$ при добавлении нового объекта, причем выполнимость улучшается с увеличением длины последовательностей.}
    \label{chapter:complex:experiment:fig:ass_1_validation}
\end{figure}

На рис.~\ref{chapter:complex:experiment:fig:ass_1_validation} представлены соответствующие результаты, показывающие, что хотя предположение~\ref{chapter:complex:assumption-local-optima-not-change} может быть ослаблено, его выполнимость улучшается с увеличением длины последовательностей.


\subsection{Полносвязная нейросетевая модель глубокого обучения}

\begin{figure}[h!t]\center
    \includegraphics[width=0.5\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/mnist_hidden_size}\hfill
    \includegraphics[width=0.5\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/mnist_num_layers.pdf}
    \caption{Зависимость абсолютного значения разности функций потерь $|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta})|$ от размера выборки $k$ для полносвязной нейронной сети на наборе данных MNIST. Левый график: при $L=5$ уменьшение разности с увеличением размера скрытого слоя $h$ от $4$ до $64$; правый график: при $h=16$ увеличение разности с увеличением количества слоев $L$ от $1$ до $10$. Результаты подтверждают теорему~\ref{chapter:complex:theorem-kiselev-loss}.}
    \label{chapter:complex:experiment:fig:mnist}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=0.5\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/mnist_hidden_size.pdf}\hfill
    \includegraphics[width=0.5\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/mnist_num_layers.pdf}
    \caption{Зависимость абсолютного значения разности функций потерь $|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta})|$ от размера выборки $k$ для полносвязной нейронной сети с предобученным экстрактором признаков (Vision Transformer, ViT) на наборе данных MNIST. Левый график: при $L=5$ уменьшение разности с увеличением $h$ от $4$ до $64$; правый график: при $h=16$ увеличение разности с увеличением $L$ от $1$ до $10$. Результаты подтверждают независимость сходимости от природы пространства исходных объектов и согласуются с теоремой~\ref{chapter:complex:theorem-kiselev-loss}.}
    \label{chapter:complex:experiment:fig:mnist-extraction}
\end{figure}

\begin{table}[h!t]\center
  \caption{Описание наборов данных для классификации изображений, использованных в экспериментах по валидации теоретических оценок сходимости ландшафта функции потерь. Все наборы данных из библиотеки \texttt{torchvision} с нормализацией значений пикселей к диапазону $[-1, 1]$.}
  \label{chapter:complex:experiment:table:datasets}
  \begin{tabular}{llll}
    \toprule
    Название     & Описание     & Формат & Разрешение \\
    \midrule
    MNIST \cite{deng2012mnist} & Рукописные цифры & Оттенки серого & $28 \times 28$ \\
    FashionMNIST \cite{xiao2017fashionmnistnovelimagedataset} & Элементы одежды & Оттенки серого & $28 \times 28$ \\
    CIFAR10 \cite{krizhevsky2009learning} & Различные объекты & RGB & $32\times 32$ \\
    CIFAR100 \cite{krizhevsky2009learning} & Различные объекты & RGB & $32 \times 32$ \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[!ht]\center
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/fashion_mnist_hidden_size} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/fashion_mnist_num_layers} 
  \end{subfigure} 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/cifar10_hidden_size} 
  \end{subfigure}%%
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/cifar10_num_layers} 
  \end{subfigure} 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/cifar100_hidden_size} 
  \end{subfigure}%%
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/cifar100_num_layers} 
  \end{subfigure} 
  \caption{Зависимость абсолютного значения разности функций потерь $|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta})|$ от размера выборки $k$ для полносвязной нейронной сети при прямой классификации на наборах данных FashionMNIST, CIFAR10 и CIFAR100. Верхний ряд: при $L=5$ уменьшение разности с увеличением $h$ от $4$ до $64$; нижний ряд: при $h=16$ увеличение разности с увеличением $L$ от $1$ до $10$. Результаты подтверждают теорему~\ref{chapter:complex:theorem-kiselev-loss} для различных наборов данных.}
  \label{chapter:complex:experiment:fig:additional-exp} 
\end{figure}

\begin{figure}[h!t]\center
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/fashion_mnist_hidden_size} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/fashion_mnist_num_layers} 
  \end{subfigure} 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/cifar10_hidden_size} 
  \end{subfigure}%%
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/cifar10_num_layers} 
  \end{subfigure} 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/cifar100_hidden_size} 
  \end{subfigure}%%
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/cifar100_num_layers} 
  \end{subfigure} 
  \caption{Зависимость абсолютного значения разности функций потерь $|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta})|$ от размера выборки $k$ для полносвязной нейронной сети с предобученным экстрактором признаков (Vision Transformer, ViT) на наборах данных FashionMNIST, CIFAR10 и CIFAR100. Верхний ряд: при $L=5$ уменьшение разности с увеличением $h$ от $4$ до $64$; нижний ряд: при $h=16$ увеличение разности с увеличением $L$ от $1$ до $10$. Результаты подтверждают независимость сходимости от природы пространства исходных объектов и согласуются с теоремой~\ref{chapter:complex:theorem-kiselev-loss}.}
  \label{chapter:complex:experiment:fig:additional-exp-extraction} 
\end{figure}

Для проверки полученных теоретических оценок проведен вычислительный эксперимент. В настоящем разделе представлены результаты обучения полносвязной нейронной сети для задачи классификации изображений. Основной целью экспериментов является эмпирическое подтверждение сходимости ландшафта функции потерь с увеличением размера выборки.
Для достижения этой цели обучена полносвязная нейронная сеть на полном наборе данных и получены соответствующие параметры $\hat{\boldsymbol{\theta}}$ как точка вблизи минимума.

Для вычислительного эксперимента использовалась библиотека \texttt{pytorch}~\cite{pytorch2019} в качестве Python-фреймворка для обучения нейронных сетей.
Архитектура сети была единообразной и состояла из нескольких линейных слоев с функцией активации ReLU после каждого слоя, за исключением последнего.
Размер~$h$ был зафиксирован для всех скрытых слоев $L$.
Обучение сети проводилось в течение нескольких эпох с использованием оптимизатора \texttt{Adam}~\cite{adam2015} с постоянной скоростью обучения $10^{-3}$.
Для обучения использовались различные наборы данных для классификации изображений, доступные в библиотеке~\texttt{torchvision}.
Для процесса обучения был выбран размер батча (англ. batch size) 64.
Эксперименты проводились на GPU Tesla A100 80GB с 16 CPU ядрами и 243 GB оперативной памяти.

В первом эксперименте использовались значения пикселей изображений в качестве входных данных. На рис.~\ref{chapter:complex:experiment:fig:mnist} представлены результаты, полученные при анализе $10\,000$ объектов из набора данных MNIST~\cite{deng2012mnist}, при этом сеть обучалась в течение $10$ эпох. Соответствующий размер входа составляет 784, а размер выхода~---~$10$. Графики слева были получены при фиксированном количестве слоев~$L=5$ в сети. Размер скрытого слоя во всех слоях варьировался от~$4$ до~$64$. График справа иллюстрирует поведение разности потерь при изменении количества скрытых слоев от~$1$ до~$10$, при фиксированном размере скрытого слоя~$h=16$. Последовательность была повторена~$100$ раз для усреднения. К полученным результатам было применено экспоненциальное скользящее среднее с коэффициентом сглаживания~$0{,}99$.
Из наблюдаемых зависимостей следует, что, хотя изменение и не является значительным, добавление большего количества слоев приводит к большей разнице в функциях потерь. И наоборот, увеличение размера скрытого слоя приводит к меньшей разнице между функциями потерь. На практике константа $M$, ограничивающая величину весов, оказывается относительно небольшой.

Поскольку задача классификации набора данных MNIST считается относительно простой, неглубокая, но широкая нейронная сеть может давать хорошие результаты классификации. Наблюдалось, что значения функции потерь ниже для больших значений $h$, и поэтому их разница также меньше.

В отличие от предыдущего эксперимента, использовался предварительно обученный экстрактор признаков изображений. Полносвязная сеть использовалась в качестве многоклассового классификатора. В качестве модели была выбрана модель Vision Transformer (ViT)~\cite{wu2020visual} от Google. Аналогичным образом были выбраны случайным образом~$10\,000$ объектов из набора данных MNIST и варьировались размер скрытого слоя и количество слоев. Результаты согласуются с наблюдениями при прямой классификации изображений. Согласованность подтверждает, что представленная сходимость не зависит от природы пространства исходных объектов $\mathcal{X}$. Для наблюдения сходимости ландшафта функции потерь достаточно ограниченности этого пространства.

Эксперимент подтверждает сходимость, доказанную в теореме~\ref{chapter:complex:theorem-kiselev-loss}. Верхняя оценка скорости этой сходимости остается верной. Изменение параметров нейронной сети, таких как количество слоев и размер слоя, приводит к изменению разности функций потерь, что согласуется с теоретическими предсказаниями.

Приведем расширенную версию проведенных экспериментов. В таблице~\ref{chapter:complex:experiment:table:datasets} представлено описание используемых наборов данных. Были использованы четыре набора данных из библиотеки \texttt{torchvision}: MNIST~\cite{deng2012mnist}, FashionMNIST~\cite{xiao2017fashionmnistnovelimagedataset}, CIFAR10 и CIFAR100~\cite{krizhevsky2009learning}.
Единственной предобработкой данных являлась нормализация для приведения значений к диапазону~$[-1; 1]$.

На рис.~\ref{chapter:complex:experiment:fig:additional-exp} графики слева получены при фиксированном количестве слоев $L=5$ в сети, при этом размер скрытого слоя варьировался на всех уровнях от $4$ до $64$. График справа демонстрирует поведение разности потерь при изменении количества скрытых слоев от $1$ до $10$, при сохранении размера $h = 16$ неизменным. Последовательность была повторена $100$ раз для усреднения. Для полученных результатов применялось экспоненциальное скользящее среднее с коэффициентом сглаживания $0.99$.

Аналогично рис.~\ref{chapter:complex:experiment:fig:mnist-extraction}, на рис.~\ref{chapter:complex:experiment:fig:additional-exp-extraction} результаты подтверждают сходимость, доказанную в теореме~\ref{chapter:complex:theorem-kiselev-loss}. Верхняя оценка скорости этой сходимости также верна. Изменение параметров нейронной сети приводит к изменению разности функций потерь, что согласуется с теоретическими предсказаниями.

\subsection{Сверточные модели глубокого обучения}
\begin{figure}[h!t]
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/mnist_change_layers} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_layers.pdf} 
  \end{subfigure} 
  \caption{Зависимость абсолютного значения разности функций потерь $|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta})|$ от размера выборки $k$ для сверточной нейронной сети при изменении количества сверточных слоев $L$ при фиксированных размере ядра $k = 3$ и количестве каналов $C = 6$. Графики демонстрируют немонотонный характер зависимости разности потерь от количества слоев.}
   \label{chapter:complex:experiment:matricez:fig:layers}
\end{figure}

\begin{figure}[h!t]
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/mnist_change_kers} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_kers} 
  \end{subfigure} 
  \caption{Зависимость абсолютного значения разности функций потерь $|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta})|$ от размера выборки $k$ для сверточной нейронной сети при изменении размера ядра свертки $k$ при фиксированных количестве слоев $L$ и количестве каналов $C = 6$. Графики демонстрируют немонотонный характер зависимости разности потерь от размера ядра.}
   \label{chapter:complex:experiment:matricez:fig:kernels}
\end{figure}

\begin{figure}[h!t]
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/mnist_change_channels.pdf} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_channels.pdf} 
  \end{subfigure} 
  \caption{Зависимость абсолютного значения разности функций потерь $|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta})|$ от размера выборки $k$ для сверточной нейронной сети при изменении количества каналов $C$ при фиксированных количестве слоев $L$ и размере ядра $k=3$. Графики демонстрируют монотонный характер зависимости: увеличение числа каналов приводит к увеличению разности потерь, что согласуется с теоретическими оценками.}
   \label{chapter:complex:experiment:matricez:fig:channels}
\end{figure}

\begin{figure}[h!t]
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_maxpool_pos} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_avgpool_pos} 
  \end{subfigure} 
  \caption{Зависимость абсолютного значения разности функций потерь $|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta})|$ от размера выборки $k$ для сверточной нейронной сети при изменении позиции операции пулинга на наборе данных CIFAR10 при фиксированных количестве слоев $L$, размере ядра $k=3$ и количестве каналов $C$. Графики демонстрируют монотонную зависимость: более раннее применение пулинга приводит к меньшей разности потерь.}
   \label{chapter:complex:experiment:matricez:fig:pool_pos}
\end{figure}

В настоящем разделе представлены результаты обучения сверточных сетей с различными параметрами. Основной целью экспериментов является демонстрация зависимости ландшафта функции потерь от таких параметров, как количество слоев, размер ядра, количество каналов и позиции пулинга, а также наблюдение того, как скорость сходимости зависит от этих параметров.

Для достижения указанной цели обучались сверточные сети и получались параметры $\hat{\boldsymbol{\theta}}$ вблизи оптимума. В качестве модели использовалась сверточная архитектура с функцией активации ReLU после каждого слоя. Для прослеживания влияния заданного параметра на сходимость фиксировались все другие параметры нейронной сети, а заданный параметр варьировался.

Исследовалась зависимость между средним абсолютным различием значений функции потерь и доступным размером выборки. Для каждой модели производилось усреднение разности потерь по перемешанным выборкам. Для улучшения визуализации использовалось экспоненциальное сглаживание с коэффициентом $0.995$. Использовалось числовое представление пикселей изображений в качестве входных данных. Результаты получены на основе анализа выборок из баз данных MNIST~\cite{deng2012mnist}, FashionMNIST~\cite{xiao2017fashionmnistnovelimagedataset} и CIFAR10~\cite{krizhevsky2009learning}. Во всех экспериментах использовались следующие гиперпараметры: постоянная скорость обучения $10^{-3}$, оптимизатор Adam, мини-пакеты размером 64, обучение проводилось в течение 10 эпох на наборах данных MNIST и Fashion-MNIST и 15 эпох на наборе данных CIFAR-10. Если параметр не варьировался, он сохранялся одинаковым во всех слоях.

\subsection{Трансформер модели глубокого обучения}

\begin{figure}[h!t]\center
    \includegraphics[width=0.5\textwidth]{thesis/figures/chapter-1/transformer/loss_landscape_convergence}
    \caption{Зависимость абсолютного значения разности функций потерь $|\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w})|$ от количества обучающих примеров $k$ для трансформерной модели на наборе данных CIFAR-100, отображенная в двойном логарифмическом масштабе. Результаты подтверждают теорему~\ref{chapter:complex:theorem-transformer-loss} и демонстрируют стабилизацию ландшафта функции потерь с ростом объема данных.}
    \label{chapter:complex:experiment:fig:loss_landscape_convergence}
\end{figure}

Для более глубокого изучения зависимости между функцией потерь и ее гессианом проведен эксперимент, соответствующий теореме~\ref{chapter:complex:theorem-transformer-loss}. Использовалась конфигурация модели на наборе данных CIFAR-100~\cite{krizhevsky2009learning}. По сравнению с моделью для набора данных MNIST, указанная модель имеет в $8$ раз больше блоков трансформера, а также скрытые слои в $8$ раз шире. Во время обучения модель обучалась в течение ряда эпох для достижения точности более $50\%$ на валидационном наборе данных. Результаты представлены на рис.~\ref{chapter:complex:experiment:fig:loss_landscape_convergence}.

Настройка эксперимента организована следующим образом:
\begin{enumerate}
    \item Модель обучается до сходимости и сохраняется вектор параметров~$\mathbf{w}^*$.
    \item Начиная с пустого набора данных, добавляются данные батч за батчем (англ. batch), и вычисляется среднее значение потерь по просмотренным батчам.
    \item Вычисляется абсолютная разность в соответствии с выражением:
        \[
            \left| \mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w}) \right|.
        \]
\end{enumerate}

\section{Заключение по главе}
В настоящей главе разработан единый теоретический аппарат для формализации соотношения между сложностью модели и сложностью данных в контексте обучения глубоких нейронных сетей.

Основным результатом главы является введение формальных определений меры сложности выборки $\mu_D(D)$ и меры сложности модели $\mu_f(f)$ в рамках теории мер, а также установление критерия обучаемости модели на выборке: $\mu_f(f) \leq \mu_D(D)$. Указанный критерий обеспечивает необходимое условие для предотвращения переобучения и формализует соответствие между выразительной способностью модели и информационной емкостью данных.

В рамках предложенного формализма введены понятия условной сложности выборки $\mu_D(D|f)$ и условной сложности модели $\mu_f(f|D)$, устанавливающие взаимосвязь между свойствами данных и архитектурными характеристиками модели. Доказана теорема~\ref{chapter:complex:theorem-finetunning}, устанавливающая необходимое и достаточное условие дообучения модели на расширенной выборке данных.

Ключевым частным случаем условной сложности модели является ландшафтная мера сложности, определяемая через спектральные свойства матриц Гессе функции потерь. Установлено, что анализ сходимости ландшафта оптимизационной задачи при увеличении объема выборки сводится к анализу спектральной нормы матрицы Гессе, что позволяет количественно оценить влияние добавления новых объектов данных на локальную геометрию функции потерь в окрестности оптимума.

Для полносвязных нейронных сетей получены строгие теоретические оценки сходимости функции потерь при увеличении размера выборки. Теорема~\ref{chapter:complex:theorem-kiselev-loss} устанавливает, что абсолютная разность между значениями функции потерь при добавлении нового объекта стремится к нулю со скоростью $O(L(hM)^{2L}/k)$ при $k \to \infty$, где $L$ --- число слоев, $h$ --- размер скрытого слоя, $M$ --- константа, ограничивающая параметры и данные. Следствие~\ref{chapter:complex:corollary-theorem-kiselev-loss} определяет асимптотику ландшафтной меры сложности: $\mu_f(f|D) \propto L(hM)^{2L}$, что демонстрирует экспоненциальную зависимость сложности от глубины сети.

Для сверточных нейронных сетей установлены теоретические оценки ландшафтной меры сложности, демонстрирующие экспоненциальную зависимость от глубины и полиномиальную --- от остальных гиперпараметров архитектуры. Теоремы~\ref{chapter:complex:theorem-1Dconv-loss} и~\ref{chapter:complex:theorem-2Dconv-loss} определяют асимптотики для 1D- и 2D-сверточных сетей соответственно: $\mu_f(f|D) \propto L(C^2M^2kd)^{L}$ и $\mu_f(f|D) \propto C^2k^2L(C^2k^2M^2mn)^{L}$.

Для трансформерных моделей получены теоретические оценки сходимости ландшафта функции потерь, восполняющие пробел в анализе путем явного вывода якобианов и гессианов для компонентов LayerNorm и FFN. Теорема~\ref{chapter:complex:theorem-transformer-loss} устанавливает неравенство сходимости $|\mathcal{L}_{k+1}(\mathbf{w})-\mathcal{L}_{k}(\mathbf{w})| \le 2L/(k+1) + M\|\mathbf{w}-\mathbf{w}^*\|_2^2/(k+1)$ и демонстрирует гетерогенность гессиана по блокам трансформера.

Проведенные вычислительные эксперименты подтверждают теоретические оценки для всех рассмотренных архитектур. Эмпирические результаты демонстрируют согласованность с теоретическими предсказаниями как для полносвязных сетей, так и для сверточных и трансформерных моделей.

Полученные результаты создают теоретическую основу для формального анализа соответствия между сложностью модели и характеристиками данных, что имеет практическое значение для проектирования архитектур нейронных сетей, планирования экспериментов и оптимизации процессов обучения. Предложенный формализм открывает перспективы для разработки методов определения достаточного размера выборки и алгоритмов адаптивного обучения.

Основные ограничения исследования связаны с детерминистическим характером анализа, предположением о существовании единой точки минимума для последовательных размеров выборки, а также возможностью улучшения верхних оценок за счет учета специфики разреженных матриц. Полученные результаты вносят вклад в теорию анализа локальной геометрии ландшафтов функции потерь и создают основу для дальнейших исследований в области формализации сложности моделей глубокого обучения.