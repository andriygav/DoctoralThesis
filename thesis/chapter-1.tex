Современные модели глубокого обучения демонстрируют исключительную эффективность в решении сложных задач, однако их успешное применение требует понимания фундаментального взаимодействия между сложностью модели и характеристиками данных.
В главе~\ref{chapter:gesian} проводится анализ матриц Гессе для различных архитектур нейронных сетей, что позволяет получить количественные оценки кривизны функции потерь и сложности оптимизационного ландшафта.
Эти результаты создают теоретическую основу для формального определения и измерения сложности как моделей, так и данных.

Ключевой идеей настоящей главы является установление формального соотношения между мерой сложности модели $\mu_f(f)$ и мерой сложности данных $\mu_D(D)$, определяемого через условие обучаемости:
\begin{equation}
    \mu_f(f) \leq \mu_D(D),
\end{equation}
а также получения частных случаев, которые имеют более подробный практический и теоретический анализ.

В рамках данного подхода основным является анализ изменения функции потерь при непрерывном изменении выборки. В разделе~\ref{chapter:complexity:loss} описывается то как абсолютное изменение функции потерь оценивается через спектральную норму матрицы Гессе:
\begin{equation}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant M_l + \frac{1}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2
\end{equation}

Этот результат, основанный на теоретических выкладках из главы~\ref{chapter:gesian}, позволяет формализовать понятие~\textit{условной сложности выборки}~$\mu_D(D_i|f)$ и установить критерии достаточности объема данных для обучения конкретной модели.

Предлагаемый формализм не только углубляет теоретическое понимание процессов обучения глубоких нейронных сетей, но и имеет практическую значимость для разработки эффективных стратегий обучения, выбора архитектур моделей и планирования экспериментов.
Результаты данной главы создают мост между теоретическим анализом оптимизационных свойств моделей и практическими аспектами их применения к реальным данным, открывая новые возможности для систематического подхода к проектированию и обучению сложных нейросетевых архитектур.

\section{Оценка сложности моделей и данных}

В современной теории глубокого обучения фундаментальной проблемой является установление соответствия между сложностью модели и характеристиками данных.
В рамках данного раздела произведем формализацию данного определения.

\begin{definition}\label{chapter:complex:def-gamma}
    Генеральной совокупностью данных~$\Gamma$ назовем произвольное множество объектов, которые исследуются в рамках той либо иной задаче. В общем случае нет никаких ограничение на счетность множества генеральной совокупности.
\end{definition}

Определение~\ref{chapter:complex:def-gamma} позволяет работать как с однородными, так и с многородной генеральными совокупностями.

\begin{definition}\label{chapter:complex:def-gamma-modality}
    Генеральную совокупность~$\Gamma$ назовем однородной, если все объекты генеральной совокупности порождаются из одного распределения. В противном случае выборку назовем~$k$-родной, где~$k$ является числом распределений на основе которой была сгенерирована генеральная совокупность;
\end{definition}

В определении~\ref{chapter:complex:def-gamma-modality} примером двуродной генеральной совокупности выступает выборка состоящая из текстов и из изображений в качестве объекта исследования. Например, современные большие языковые модели одновременно работают как с текстами, так и с изображениями и называются многомодальными моделями.

Пусть задана генеральная совокупность данных~$\Gamma,$ где задано множество всех подмножеств объектов образующих кольцо выборок:
\[
    \mathfrak{D} = \{D_\Gamma^i\}, \quad D_\Gamma^i \subset \Gamma.
\]

\begin{definition}[Мера сложности выборки]\label{chapter:complex:def-data-complexity}
    Мерой сложности выборки назовем отображение~$\mu_D,$ такое, что: 
    \[
        \mu_D(D_i) : \mathfrak{D}_\Gamma \to \mathbb{R}_+,
    \]
    удовлетворяющее свойству:
    \begin{align}
        \mu_D(D_i\cup D_j) \leq \mu_D(D_i)+\mu_D(D_j),
    \end{align}
    где равенство достигается при условии~$D_i\cap D_j=\emptyset.$
\end{definition}

Определение~\ref{chapter:complex:def-data-complexity} является классическим определением из теории меры, удовлетворяющее свойству конечной-адитивности. Конечной-адитивности нам достаточно, так как в рамках исследований предполагается конечное число объектов при обучении моделей глубокого обучения.
Стоит заметить, что предполагается сравнение выборок только из одной генеральной совокупности, но заметим, что этим никак не ограничивается сама генеральная совокупность и в рамках определения возможны мультимодальные генеральные совокупности.


Пусть задано множество параметрических аппроксимирующих моделей
\[
    \mathfrak{F} = \left\{f_i\right\},
\]
где каждое~$f_i$ является некоторым множеством параметрическим функций.
В определении~\ref{chapter:complex:def-model} вводиться общее определение характеристики параметрического семейства функций~$f,$ которые в дальнейшем рассматриваются в качестве моделей моделей глубокого обучения.
\begin{definition}\label{chapter:complex:def-model}
    Мерой сложности модели~$f$ назовем отображение~$\mu_f(f_i)$:
    \[
        \mu_f(f_i) : \mathfrak{F} \to \mathbb{R}_+.
    \]
\end{definition}

Заметим, что определение меры сложности модели~$f$ не является определением меры в общем случае, так как множество~$\mathfrak{F}$ не является кольцом, поэтому данная мера является некоторым отображением, которая является некоторой характеристикой сложности.
К примеру число параметров модели удовлетворяет определению~\ref{chapter:complex:def-model}.

Введя определения меры сложности как для выборки, так и для модели, перейдем к определению обучаемости модели на выборке6 которое сформулировано в определении~\ref{chapter:complex:def-model-bound-data}.

\begin{definition}\label{chapter:complex:def-model-bound-data}
    Назовем модель $f\in\mathfrak{F}$ \textit{обучаемой} на выборке $D\in\mathfrak{D},$ если 
    \[
        \mu_f(f)\leq \mu_D(D).
    \]
\end{definition}

В рамках определения~\ref{chapter:complex:def-model-bound-data} не вводится никакого ограничения на качество аппроксимации модели после обучения, более подробно это будет определено для частных случаев мер в следующих разделах. Также, определение~\ref{chapter:complex:def-model-bound-data} имеет эмпирическую интерпретацию: сложность модели не должна превышать сложности данных, на которых она обучается, так как в противном случае возникает проблема переобучения, когда модель запоминает шум в данных вместо выявления значимых закономерностей.

В рамках диссертационной работы предполагается исследовать частный случай меры сложности модели на основе оценок ландшафта оптимизационных задач используя матрицы Гессе для различных нейросетевых архитектур, которые описаны в главе~\ref{chapter:gesian}. Подробнее о частном случае меры сложности описано в разделе~\ref{chapter:complexity:loss}.

\begin{theorem}\label{chapter:complex:theorem-finetunning}
    Если для исходной выборки~$D\in\mathfrak{D}$ выполняется условие~$\mu_f(f) \leq \mu_D(D)$, тогда для новой выборки~$D'\in\mathfrak{D}$ модель дообучаема при условии:
    \[
        \mu_f(f) - \mu_D(D) \leq \mu_D(D').
    \]
\end{theorem}
\begin{proof}
Доказательство основано на свойствах мер сложности и условии обучаемости модели. 

По определению обучаемости модели на выборке $D$ имеем:
\[
    \mu_f(f) \leq \mu_D(D).
\]
При добавлении новых данных $D'$ к исходной выборке $D$ сложность объединенной выборки не убывает:
\[
    \mu_D(D) \leq \mu_D(D\cup D')
\]
Из свойства адитивности меры сложности выборки, получаем:
\[
    \mu_D(D\cup D') \leq \mu_D(D)+\mu_D(D'),
\]
тогда, объединяя эти три неравенства, получаем цепочку:
\[
    \mu_f(f) \leq \mu_D(D) \leq \mu_D(D\cup D') \leq \mu_D(D)+\mu_D(D'),
\]
тогда перенося $\mu_D(D)$ в левую часть, получаем окончательное неравенство:
\[
    \mu_f(f) - \mu_D(D) \leq \mu_D(D').
\]
\end{proof}

Это неравенство показывает, что ``оставшаяся емкость'' модели, а именно, что разность между сложностью модели и сложностью исходных данных не превосходит сложности новых данных~$D'$, что является необходимым условием для успешного дообучения модели на новых данных.

Теорема~\ref{chapter:complex:theorem-finetunning} предполагает, что мера сложности данных обладает свойством монотонности и субаддитивности. В практических приложениях эти свойства должны быть проверены для конкретных выбранных мер сложности.

Таким образом, введение формальных мер сложности моделей и данных создает теоретическую основу для решения практических задач проектирования архитектур нейронных сетей, планирования экспериментов и оптимизации процессов обучения.


\section{Достаточный объем выборки, как мера сложности данных}

В рамках введенного определения об обучаемости модели на выборке, ключевым понятием становится \textit{условная сложность выборки}:
\begin{equation}\label{chapter:complex:eq-data-submessure}
    \mu_D(D|f) : \mathfrak{D} \to \mathbb{R}_+,
\end{equation}
которая характеризует сложность данных $D\in\mathfrak{D}$ относительно заданной параметрической модели~$f$.
Эта мера отражает, насколько ``трудной'' является выборка~$D$ для обучения модели $f$.

Мотивация введения условной сложности выборки проистекает из практического опыта обучения нейронных сетей.
Одна и та же выборка данных может представлять различную сложность для разных архитектур моделей.

Таким образом мера сложности модели~$\mu_f(f)$ индуцирует меру сложности выборки следующим образом:
\begin{equation}\label{chapter:complex:eq-data-submessure-infinum}
    \mu_D(D|f) = \inf \{ \mu_D(D') : D' \subseteq D, \quad \mu_f(f) \leq \mu_D(D') \},
\end{equation}
то есть условная сложность выборки может быть задана как минимальная сложность данных, при которой модель $f$ остается обучаемой.

\begin{definition}
    Условной сложностью выборки~$D$ относительно заданной параметрической модели~$f$ назовем отображение~\eqref{chapter:complex:eq-data-submessure} определяющиеся выражением~\eqref{chapter:complex:eq-data-submessure-infinum}.
\end{definition}

Рассмотрим частный случай меры сложности данных~$\mu_D$ заданной из определения достаточного объема выборки.
Предположим, что генеральная совокупность~$\Gamma_C$ состоит из объектов одинаковой сложности~$C$, то есть для каждого объекта $\gamma \in \Gamma_C$ выполняется:
\[
    \mu_D(\gamma) = C,
\]
где~$C\in\mathbb{R}_+$ некоторая агрегирована сложность одного объекта выборки. Заметим, что данное предположение является сильным ограничением и может не выполняться на практике, поскольку в реальных задачах различные объекты могут обладать разной сложностью для модели.

\begin{remark}
Константа $C$ представляет собой ``стоимость'' одного объекта выборки в единицах сложности.
На практике $C$ может зависеть от характеристик генеральной совокупности~$\Gamma$ и должна калиброваться экспериментально.
\end{remark}

\begin{definition}
    Однородную генеральную совокупность~$\Gamma_C$ назовем простой, если она состоит из объектов одинаковой сложности~$C.$
\end{definition}

\begin{theorem}
    Для простой генеральной совокупности, мера сложности любой выборки $D \subset \Gamma$ равна ее объему:
    \[
        \mu_D(D) = C\cdot|D|.
    \]
\end{theorem}
\begin{proof}
Докажем, что функция $\mu_D(D) = C \cdot |D|$ удовлетворяет определению меры сложности выборки~\ref{chapter:complex:def-data-complexity}.

Для начала докажем, что функция~$\mu_D$ является неотрицательной.
Поскольку $|D| \geq 0$ для любой выборки $D \subset \Gamma$, и константа $C\in\mathbb{R}_+$, то $\mu_D(D) = C|D| \geq 0$.

Докажем монотоность функции~$\mu_D$.
Пусть $D_1 \subseteq D_2 \subset \Gamma$.
Тогда $|D_1| \leq |D_2|$, следовательно:
\[
    \mu_D(D_1) = C|D_1| \leq C|D_2| = \mu_D(D_2)
\]

Докажем субадитивность функции~$\mu_D$.
Для любых непересекающихся выборок $D_1, D_2 \subset \Gamma$ выполняется:
\[
    \mu_D(D_1 \cup D_2) = C|D_1 \cup D_2| = C(|D_1| + |D_2|) = \mu_D(D_1) + \mu_D(D_2)
\]

Таким образом, функция $\mu_D(D) = C\cdot|D|$ удовлетворяет всем требованиям меры сложности данных в рамках сделанных предположений.
\end{proof}

Частным случаем~\textit{условной сложности выборки} является достаточный объем выборки~--- минимальный объем данных из выборки~$D$ необходимый для обучения модели~$f$.

\begin{definition}
    Размер выборки $m^*$ называется \textit{достаточным} согласно критерию $T$, если $T$ выполняется для всех $k \geqslant m^*$.
\end{definition}

Таким образом исследования достаточного объема выборки является частным случаем предложенного определения меры сложности данных.
Подробный методов оценки достаточного объема выборки рассматривается в главе~\ref{chapter:samplesize}.


\section{Сходимость ландшафта оптимизационной задачи, как мера сложности модели}\label{chapter:complexity:loss}

\begin{figure}[h!t]\center
    \centering
    \includegraphics[width=0.7\linewidth]{figures/chapter-3/losses_difference.pdf}
    \caption{Пример изменения функции потерь при добавлении нового объекта}
    \label{fig-chapter-3-losses-difference}
\end{figure}


Рассмотрим выборку из простой генеральной совокупности~$\Gamma_C$:
\begin{equation}
    D = \left\{ (\mathbf{x}_i, \mathbf{y}_i) \right\}, \quad i = 1, \ldots, m, \quad \mathbf{x} \in \mathcal{X}, \ \mathbf{y} \in \mathcal{Y}, \quad D\subset \Gamma_C.
\end{equation}

Рассмотрим некоторое параметрическое отображение~$f_{\boldsymbol{\theta}}: \mathcal{X} \to \mathcal{Y},$ которое аппроксимирует условное распределение целевой переменной для заданного признакового описания объекта~$p(\mathbf{y}|\mathbf{x}).$ Параметры~$\boldsymbol{\theta}$ функции~$f_{\boldsymbol{\theta}}$ принадлежат пространству~$\mathbb{R}^{P},$ где~$P$ описывает число параметров отображения~$f_{\boldsymbol{\theta}}$.

Пусть, для выбора оптимального вектора параметров~$\hat{\boldsymbol{\theta}}$ используется подход минимизации эмпирического риска:
\begin{equation}
    \hat{\boldsymbol{\theta}} = \arg\min_{\boldsymbol{\theta}} \mathcal{L}_m(\boldsymbol{\theta}),
\end{equation}
где функция эмпирического риска для выборки размера~$|D|=m$ задается в следующем виде: 
\begin{equation}
    \mathcal{L}_m(\boldsymbol{\theta}) = \frac{1}{m} \sum\limits_{i=1}^{m} \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), \mathbf{y}_i) \approx \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p(\mathbf{x}, \mathbf{y})} \left[ \ell(f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y}) \right],
\end{equation}
где функция~$\ell\left(\mathbf{z}, \mathbf{y}\right)$ описывает ошибку на одном объекте. Далее в качестве функции~$\ell$ будут рассматриваться либо кросс-энтропийная функция ошибка либо средняя квадратическая ошибка, в зависимости от рассматриваемой задачи и архитектуры модели.

Заметим, что функция эмпирического риска~$\mathcal{L}_m(\boldsymbol{\theta})$ задает некоторую поверхностью в пространстве размерности~$P.$
Изменение значения при добавлении одного объекта
\begin{align}\label{chapter:complex:equation-difference}
    \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) &= \frac{1}{k+1}\sum_{i=1}^k\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) - \frac{1}{k+1}\sum_{i=1}^k\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \\
    &= \frac{1}{k+1}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1})-\sum_{i=1}^{k}\frac{1}{k(k+1)}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \\
    &= \frac{1}{k+1} \left(\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \mathcal{L}_{k}(\boldsymbol{\theta})\right).
\end{align}

Дальнейшее исследования ландшафта нацелено на исследования данной разницы, причем особенно особенно интересуют предельные свойства при стремлении размера выборки к бесконечности. Для дальнейших оценок данной разности вводиться предположение~\ref{chapter:complex:assumption-local-optima-not-change}, которое в целом подтверждается на практике, но в свою очередь является достаточно сильным, что упрощает дальнейшие выкладки.

\begin{assumption}\label{chapter:complex:assumption-local-optima-not-change}
    Пусть $\boldsymbol{\theta}^*$ является локальным минимумом обеих эмпирических функций потерь $\mathcal{L}_{k}(\boldsymbol{\theta})$ и $\mathcal{L}_{k+1}(\boldsymbol{\theta})$, т.е.
    \[
        \nabla \mathcal{L}_{k}(\boldsymbol{\theta}^*) = \nabla \mathcal{L}_{k+1}(\boldsymbol{\theta}^*) = \mathbf{0}.
    \]
\end{assumption}
Содержательно, предположение \ref{chapter:complex:assumption-local-optima-not-change} имеет простую эвристическую интерпретацию, что новый объект данных является \textit{репрезентативным} для уже обученной модели~--- он не приносит ``новой информации'', а лишь уточняет ее.
В целом при асимптотически большом объеме выборки, данное свойство не противоречит эмпирическим результатам.

Воспользуемся квадратичным приближением Тейлора для упомянутых выше функций потерь в окрестности точки $\boldsymbol{\theta}^*$. Предполагаем, что разложение до второго порядка будет достаточным для изучения локального поведения. Член первого порядка обращается в ноль, поскольку градиенты $\nabla \mathcal{L}_{k}(\boldsymbol{\theta}^*)$ и $\nabla \mathcal{L}_{k+1}(\boldsymbol{\theta}^*)$ равны нулю:
\begin{equation}\label{chapter:complex:equation-approx}
    \mathcal{L}_{k}(\boldsymbol{\theta}) \approx \mathcal{L}_{k}(\boldsymbol{\theta}^*) + \frac{1}{2} (\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top \mathbf{H}^{(k)}(\boldsymbol{\theta}^*) (\boldsymbol{\theta} - \boldsymbol{\theta}^*),
\end{equation}
где введено обозначение гессиана функции $\mathcal{L}_{k}(\boldsymbol{\theta})$ по параметрам $\boldsymbol{\theta}$ в точке $\boldsymbol{\theta}^*$ как $\mathbf{H}^{(k)}(\boldsymbol{\theta}^*) \in \mathbb{R}^{P \times P}$. Более того, полный гессиан может быть записан как среднее значение гессианов отдельных членов эмпирической функции потерь:
\[
    \mathbf{H}^{(k)}(\boldsymbol{\theta}) = \nabla^2_{\boldsymbol{\theta}} \mathcal{L}_{k}(\boldsymbol{\theta}) = \frac{1}{k} \sum\limits_{i=1}^{k} \nabla^2_{\boldsymbol{\theta}} \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}).
\]

Следовательно, используя полученное квадратичное приближение~\eqref{chapter:complex:equation-approx}, формула для разности потерь~\eqref{chapter:complex:equation-difference} принимает вид:
\begin{align}
    \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) &= \frac{1}{k+1} \left( \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right) +\\
    &\quad+ \frac{1}{k+1} (\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top \left( \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right) (\boldsymbol{\theta} - \boldsymbol{\theta}^*),
\end{align}
причем, используя неравенство треугольника, получаем следующую оценку:
\begin{align}\label{chapter:complex:equation-mod-difference-full}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| &\leqslant \frac{1}{k+1} \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| +\\
    &\quad+ \frac{1}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{align}

Заметим, что первое слагаемое может быть легко ограничено константой, поскольку сама функция потерь принимает ограниченные значения.
Однако выражение с гессианами не так просто оценить.
Подробный анализ матриц Гессе для различных типов параметрических моделей глубокого обучения представлен в главе~\ref{chapter:gesian}.
Таким образом, анализ локальной сходимости ландшафта функции потерь, основан на ее матрице Гессе.

Получаем выражение для анализа, описывающее поведение ландшафта функции потерь:
\begin{equation}\label{chapter:complex:equation-mod-difference}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{M_l}{k+1}+ \frac{1}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{equation}

Таким образом получили, что анализ сходимости ландшафта оптимизационной задачи сводится к анализу нормы матрицы Гессе, которые подробно разобраны в главе~\ref{chapter:gesian}.

Оценка~\ref{chapter:complex:equation-mod-difference} задает некоторое свойство параметрического семейства функций~$f$ на заданной выборке~$D.$
Определим данное свойство как условную сложность модели~$f$ на выборке~$D:$
\begin{equation}\label{chapter:complex:equantion-subcomplex-model}
    \mu_f(f|D) : \mathfrak{F} \to \mathbb{R}_+,
\end{equation}
причем, более подробно рассмотрим частный случай условной меры сложности параметрического семейства функций~$f$ вида:
\begin{equation}\label{chapter:complex:equantion-subcomplex-model-surface}
    \mu_f(f|D) = \mathsf{E}_{\mathbf{x}_i\in D}\left\| \mathbf{H}_{i}(\boldsymbol{\theta}^*) -  \mathsf{E}_{\mathbf{x}_i\in D}\mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{equation}

\begin{definition}\label{chapter:complex:definition-subcomplex-model}
    Условной сложностью параметрической модели~$f$ относительно заданной выборки~$D$ назовем отображение~\eqref{chapter:complex:equantion-subcomplex-model}.
\end{definition}

\begin{definition}\label{chapter:complex:definition-subcomplex-model-surface}
    Ландшафтноя мерой сложности параметрической функции~$f$ назовем условную сложность параметрической модели~$f$ заданной выражением~\eqref{chapter:complex:equantion-subcomplex-model-surface}.
\end{definition}

Определение~\ref{chapter:complex:definition-subcomplex-model} описывает прикладный способ задания сложности на параметрических семействах функций в контексте оптимизации на заданных выборках.
Причем, условная сложность модели~$\mu_f(f|D)$ характеризует сложность архитектуры модели~$f$ при ее обучении на выборке данных $D$.
Это позволяет количественно оценить, насколько модель ``соответствует'' данным.
Так слишком простая модель может недообучаться, а слишком сложная~--- переобучаться.

Ландшафтная же мера сложности~\ref{chapter:complex:definition-subcomplex-model-surface} представляет собой явный вид условной сложности, основанную на анализе оптимизационного ландшафта функции потерь.
Заметим, что выражение \eqref{chapter:complex:equantion-subcomplex-model-surface} содержательно указывает на то, насколько сильно добавление нового объекта данных изменяет кривизну функции потерь в окрестности оптимума.

Дальнейшее повествование в главе посвящено к оценкам ландшафтной меры для различных параметрических моделей~$f.$
Все результаты основываются на анализе матриц Гессе описаных в главе~\ref{chapter:gesian}.

Используя выражение~\ref{chapter:complex:equation-mod-difference} и определение ландшафтной меры сложности получаем следующую асимптотическую связь между этими оценками:
\begin{equation}\label{chapter:complex:relation-surface-subcomplex-model}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{M_l}{k+1}+ \frac{\mu_f(f|D)}{k+1}\left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2.
\end{equation}

\begin{lemma}
    Пусть задан некоторый~$\varepsilon,$ описывающей допустимое изменение ландшафта при добавления одного объекта в некоторой окрестности оптимума~$\boldsymbol{\theta}^*$ радиуса~$R$,
    причем выборка~$D$ из простой генеральной совокупности~$\Gamma_C$.

    Тогда верно следующее соотношение между ландшафтной мерой~$\mu_f(f|D)$ и условной сложностью выборки~$\mu_D(D|f):$
    \[
        \mu_f(f|D) \geq \mu_D(D|f)\frac{\varepsilon}{CR^2} - \frac{M_\ell }{R^2}.
    \]
\end{lemma}
\begin{proof}
    Очевидно из определения условных мер сложности моделей и данных и подстановке всей выборки~$D$ в выражение~\eqref{chapter:complex:relation-surface-subcomplex-model}.
\end{proof}


\subsection{Полносвязная нейросетевая модель глубокого обучения}

Используя результаты, полученные в рамках главы~\ref{chapter:gesian}, а именно результаты теоремы~\ref{theorem:hess-kiselev-lemma} в которой показана асимптотика нормы матрицы Гессе от гиперпараметров полносвязной нейросетевой модели:
\[
    \left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \propto L (hM)^{2L},
\]
из которой видно, что спектральная норма матрицы Гессе имеет полиномиальную зависимость от размера слоя и экспоненциальную зависимость от числа слоев.

\begin{theorem}\label{chapter:complex:theorem-kiselev-loss}
    Пусть параметры $\boldsymbol{\theta}$ выбраны так, что $\left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \leqslant R^2$ для некоторого $R > 0$. Если существует неотрицательная константа $M_{\ell}$ такая, что $\left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant M_{\ell}$ для всех объектов $i = 1, \ldots, m$ в наборе данных, то при выполнении условий Теоремы~\ref{theorem:hess-kiselev-theorem} справедливо:
    \begin{align}
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{2}{k+1}\left( M_{\ell} + \left( L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \frac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1} \right) R^2 \right),
    \end{align}
    причем, что выражение асимптотически стремиться к~$0,$ то есть:
    \begin{align}
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \to 0~\text{при}~k \to \infty.
    \end{align}
    Таким образом, имеет место следующая пропорциональность:
    \begin{equation}\label{eq:rate}
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \propto \frac{L (hM)^{2L} R^2}{k}. 
    \end{equation}
\end{theorem}
\begin{proof}
    Используя неравенство треугольника для выражения~\ref{chapter:complex:equation-mod-difference-full}, получаем
    \begin{align}
        & \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant \\
        &\quad\leqslant \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) \right| + \left| \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant \\
        &\quad\leqslant \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) \right| + \frac{1}{k} \sum\limits_{i=1}^{k} \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant\\
        &\quad\leqslant M_{\ell} + \frac{1}{k} \sum\limits_{i=1}^{k} M_{\ell} = 2M_{\ell} = \mathcal{O}(1) \text{ при } k \to \infty.
    \end{align}
    Аналогично для норм матриц Гессе получаем:
    \begin{align}
        & \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2 \leqslant \\
        &\quad\leqslant \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) \right\| + \left\| \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2 \leqslant \\
        &\quad\leqslant \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) \right\| + \frac{1}{k} \sum\limits_{i=1}^{k} \left\| \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2 \leqslant \\
        &\quad\leqslant M_{\mathbf{H}} + \frac{1}{k} \sum\limits_{i=1}^{k} M_{\mathbf{H}} = 2 M_{\mathbf{H}} = \mathcal{O}(1)~\text{при}~k \to \infty,
    \end{align}
    где из Теоремы~\ref{theorem:hess-kiselev-theorem} получаем
    \[
        M_{\mathbf{H}} = L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \frac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1}.
    \]
    
    Таким образом, подставляя полученные оценки в выражение для разности, получаем
    \[
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{2M_{\ell}}{k+1} + \frac{2M_{\mathbf{H}}}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2,
    \]
    где выбирая окрестность локального минимума $\boldsymbol{\theta}^*$, т.е. $\left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \leqslant R^2$, получаем
    \[
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{2}{k+1}\left( M_{\ell} + M_{\mathbf{H}}R^2 \right) \to 0 \text{ при } k \to \infty.
    \]
\end{proof}

Используя доказательство теоремы~\ref{chapter:complex:theorem-kiselev-loss} легко получается выражение для ландшафтной меры полносвязной нейросетевой модели глубокого обучения, которое описано в следствии~\ref{chapter:complex:corollary-theorem-kiselev-loss}.

\begin{corollary}\label{chapter:complex:corollary-theorem-kiselev-loss}
    Ландшафтная мера сложности параметрической функции~$f$ полносвязной нейросетевой модели глубокого обучения имеет асимптотику:
    \[
        \mu_f(f|D) \propto L(hM)^{2L},
    \]
    где~$M$ некоторая константа зависящая ограничивающая параметры и данные.
\end{corollary}

В работе~\cite{eldan2016exponentialcomplexity} показано, что существуют функции, которые могут быть эффективно представлены трехслойными сетями, но требуют экспоненциального числа нейронов в двухслойных сетях.
Следствие указывает~\ref{chapter:complex:corollary-theorem-kiselev-loss}, указывает на схожий результат, описывающий то, что экспоненциальный рост сложность модели увеличивается экспоненциально при увеличении числа слоев, и следовательно, уменьшив число слоем, потребуется экспоненциальный рост параметров модели, чтобы сохранить заданную сложность модели.

\subsection{Сверточные модели глубокого обучения}

В данном подразделе рассматривается оценка ландшафтной меры сложности для сверточных нейронных сетей, которые являются одними из популярных на текущий момент моделей глубокого обучения.
В анализе ландшафта используются результаты о матрицах Гессе из главы~\ref{chapter:gesian}.

Начнем с анализа 1D-сверточных сетей, которые применяются в обработке последовательностей, временных рядов и сигналов.
Основные результаты, важные для определения ландшафтной меры сложности, представлены в теореме~\ref{chapter:complex:theorem-1Dconv-loss}.

\begin{theorem}\label{chapter:complex:theorem-1Dconv-loss}
Пусть параметры~$\boldsymbol{\theta}$ находятся в $R$-окрестности оптимума:
\[
    \|\boldsymbol{\theta} - \boldsymbol{\theta}^*\| \leqslant R,
\]
а функция потерь ограничена некоторой константой:
\[
    \exists \; W_l > 0: \; \forall i\; |\ell_i| \leqslant M_{\ell}.
\]
Пусть все объекты в наборе данных ограничены:
\[
    \exists W_x\; \forall i \; \|{x_i}\| \leqslant W_x.
\]
Тогда в условиях теоремы~\ref{thm:1Dconv}:
\begin{align}
    & \big|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_{k}(\boldsymbol{\theta})\big| \leqslant \frac{2}{k+1}M_{\ell} +\\
    & + \frac{2}{k+1}R^2\sqrt{2}d^2W_x^2(L+1)(C^2w^2kd)^L.
\end{align}
\end{theorem}
\begin{proof}
    Доказательство эквивалентно доказательству теореме~\ref{chapter:complex:theorem-kiselev-loss} подставляя оценки нормы матрицы Гессе из теоремы~\ref{thm:1Dconv}.
\end{proof}

Используя теорему~\ref{chapter:complex:theorem-1Dconv-loss} легко получается выражение для ландшафтной меры 1D-сверточной нейросетевой модели глубокого обучения, которое описано в следствии~\ref{chapter:complex:corollary-theorem-1Dconv-loss}.

\begin{corollary}\label{chapter:complex:corollary-theorem-1Dconv-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 1D-сверточной модели глубокого обучения имеет асимптотику:
    \[
        \mu_f(f|D) \propto L(C^2M^2kd)^{L},
    \]
    где~$C$~--- максимальное число каналов,~$d$~--- длина входной последовательности,~$k$~--- размер свертки,~$M$~--- некоторая константа зависящая ограничивающая параметры и данные.
\end{corollary}

Полученные оценки демонстрируют, что сложность 1D-сверточных нейросетевых моделей экспоненциально зависит от глубины $L$ и полиномиально~--- от остальных гиперпараметров архитектуры.
Особенностью 1D-архитектур является линейная зависимость от длины последовательности $d$, что отражает специфику обработки последовательностей.

Перейдем к анализу 2D-сверточных сетей, которые применяются в задачах обработки изображений.
Основные результаты, важные для определения ландшафтной меры сложности, представлены в теореме~\ref{chapter:complex:theorem-2Dconv-loss}.

\begin{theorem}\label{chapter:complex:theorem-2Dconv-loss}
Пусть параметры $\boldsymbol{\theta}$ находятся в $R$-окрестности оптимума:
\[
    \|\boldsymbol{\theta} - \boldsymbol{\theta}^*\| \leqslant R.
\]
Также функция потерь ограничена некоторой константой:
\[
    \exists \; W_l > 0: \; \forall i\; |\ell_i| \leqslant W_l.
\]
Пусть все объекты в наборе данных также ограничены:
\[
    \exists W_x\; \forall i \; \|{x_i}\| \leqslant W_x.
\]

Тогда при выполнении условий теоремы~\ref{thm:2Dconv} справедливо:
\begin{align}
    & \big|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_{k}(\boldsymbol{\theta})\big| \leqslant \frac{2}{k+1}W_{\ell} +\\
    & + \frac{2}{k+1}R^2\sqrt{2}q^2W_{x}^2(L+1)(C^2k^2w^2mn)^L,
\end{align}
где $q^2 = C^2k^2mn$.
\end{theorem}
\begin{proof}
    Доказательство эквивалентно доказательству теореме~\ref{chapter:complex:theorem-kiselev-loss} подставляя оценки нормы матрицы Гессе из теоремы~\ref{thm:2Dconv}.
\end{proof}

Используя теорему~\ref{chapter:complex:theorem-2Dconv-loss} легко получается выражение для ландшафтной меры 2D-сверточной нейросетевой модели глубокого обучения, которое описано в следствии~\ref{chapter:complex:corollary-theorem-2Dconv-loss}.

\begin{corollary}\label{chapter:complex:corollary-theorem-2Dconv-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 2D-сверточной модели глубокого обучения имеет асимптотику:
    \[
        \mu_f(f|D) \propto C^2k^2L(C^2k^2M^2mn)^{L},
    \]
    где~$C$~--- максимальное число каналов,~$m,n$~--- размеры входного изображения,~$k$~--- размер свертки,~$M$~--- некоторая константа зависящая ограничивающая параметры и данные.
\end{corollary}

Для 2D-сверточных сетей наблюдается более быстрый рост сложности по сравнению с 1D-архитектурами, что обусловлено двумерной природой данных.

\subsection{Трансформер модели глубокого обучения}

Архитектура трансформеров представляет собой один из наиболее значительных прорывов в области глубокого обучения последних лет.
Эти модели демонстрируют state-of-the-art результаты в задачах обработки естественного языка, компьютерного зрения и других областях.
Особенностью трансформеров является механизм самовнимания, который позволяет модели учитывать глобальные зависимости в данных независимо от их положения.

\begin{theorem}\label{chapter:complex:theorem-transformer-loss}
Для одного блока самовнимания и одного блока трансформера \ref{eq:transformer} при условии ограниченности функции потерь \[
    0 \leqslant \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_i), \mathbf{y}_i) \leqslant L,
\]
и ограниченности норм матриц Гессе справедливо:
\[
    \left| \mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w}) \right| \leqslant \frac{2L}{k+1} + \frac{M \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2}{(k+1)},
\]
где для блока самовнимания константа $M$ может быть непосредственно вычислена из Теоремы \ref{thm:self_attention_hessian_estimation},
а для блока трансформера \(M = M_{\text{tr}}\) вычисляется в соответствии с Теоремой~\ref{thm:transformer_hessian_estimate}.
\end{theorem}
\begin{proof}
Доказательство проводится в несколько этапов.
На первом этапе оценивается разность значений функции потерь в оптимальной точке, затем на втором этапе оценивается разность гессианов.
После чего используя результаты обоих этапов, проводиться объединение обоих оценок.

Рассмотрим разность эмпирических функций потерь при добавлении нового объекта. Используя разложение разности и свойства норм, получаем:
\begin{align}
    \left| \mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w}) \right| &\leqslant \frac{1}{k+1} \left| \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum_{i=1}^{k} \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| +\\
    &\quad+ \frac{1}{2 (k+1)} \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2 \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2.
\end{align}

Первое слагаемое характеризует изменение значения функции потерь в оптимальной точке параметров $\mathbf{w}^*$ при добавлении нового объекта.
Это разность между значением потерь на новом объекте и средним значением потерь на предыдущей выборке, до добавления нового объекта.
Предположим, что функция потерь $\ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_i), \mathbf{y}_i)$ ограничена сверху константой $L$ для всех объектов выборки:
\[
    0 \leqslant \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_i), \mathbf{y}_i) \leqslant L.
\]
Это предположение является естественным для большинства функций потерь, используемых в машинном обучении, таких как кросс-энтропия или среднеквадратичная ошибка.
Тогда для нового объекта выполняется:
\[
    \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) \leqslant L,
\]
а для среднего значения по предыдущей выборке:
\[
    \frac{1}{k} \sum_{i=1}^{k} \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \leqslant \frac{1}{k} \sum_{i=1}^{k} L = L.
\]
Используя неравенство треугольника для модуля разности, получаем:
\[
    \left| \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum_{i=1}^{k} \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant L + L = 2L.
\]
Таким образом, вклад первого слагаемого в общую оценку не превосходит:
\[
    \frac{1}{k+1} \left| \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum_{i=1}^{k} \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant \frac{2L}{k+1}.
\]

Второе слагаемое в оценке связано с изменением гессиана функции потерь. Рассмотрим выражение:
\[
    \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2,
\]
где $\mathbf{H}_{k+1}(\mathbf{w}^*) = \nabla^2_{\mathbf{w}} \ell(\mathbf{f}_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1})$~--- матрица Гессе функции потерь для нового объекта, а $\frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) = \mathbf{H}_k(\mathbf{w}^*)$~--- средняя матрица Гессе по всей предыдущей выборке.
Перепишем это выражение в более удобной форме:
\begin{align}
    \mathbf{H}_k(\mathbf{w}^*) &= \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*),\\
    \mathbf{H}_{k+1}(\mathbf{w}^*) - \mathbf{H}_k(\mathbf{w}^*) &= \mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*).
\end{align}
Для оценки нормы этой разности используем неравенство треугольника:
\[
    \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) \right\|_2 + \left\| \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2.
\]
Предположим, что выполняется ограниченость следующих метриц Гессе:
\[
    \left\| \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant M
\]
для некоторой константы $M$.
Тогда для гессиана нового объекта:
\[
    \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) \right\|_2 \leqslant M,
\]
а для суммы гессианов:
\[
    \left\| \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant \sum_{i=1}^{k} \left\| \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant k M.
\]
Следовательно:
\[
    \left\| \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant \frac{1}{k} \cdot k M = M.
\]
Объединяя полученные оценки, получаем:
\[
    \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k} \sum_{i=1}^{k} \mathbf{H}_i(\mathbf{w}^*) \right\|_2 \leqslant M + M = 2M.
\]
Теперь оценим вклад второго слагаемого в общую разность функций потерь:
\begin{align}
    \frac{1}{2 (k+1)} \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2 \left\| \mathbf{H}_{k+1}(\mathbf{w}^*) - \mathbf{H}_k(\mathbf{w}^*) \right\|_2 &\leqslant \frac{2M}{2 (k+1)} \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2 = \\
    &=\frac{M \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2}{k+1}.
\end{align}


Комбинируя оценки для обоих слагаемых, получаем итоговую оценку:
\[
    \left| \mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w}) \right| \leqslant \frac{2L}{k+1} + \frac{M \left\| \mathbf{w} - \mathbf{w}^* \right\|_2^2}{k+1}.
\]
\end{proof}

\section{Результаты вычислительных экспериментов}

В данном разделе описываются результаты вычислительных экспериментов для методов, описанных в данной главе.

\begin{figure}[h!t]\center
    \includegraphics[width=0.5\linewidth]{thesis/figures/chapter-1/transformer/assumption}
    \caption{Проверка Предположения~\ref{chapter:complex:assumption-local-optima-not-change} о сходимости локальных минимумов при увеличении объема выборки.}
    \label{chapter:complex:experiment:fig:ass_1_validation}
\end{figure}

На Рис.~\ref{chapter:complex:experiment:fig:ass_1_validation} представлены соответствующие результаты, показывающие, что хотя Предположение~\ref{chapter:complex:assumption-local-optima-not-change} может быть ослаблено, его выполнимость улучшается с увеличением длины последовательностей.


\subsection{Полносвязная нейросетевая модель глубокого обучения}

\begin{figure}[h!t]\center
    \includegraphics[width=0.5\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/mnist_hidden_size}\hfill
    \includegraphics[width=0.5\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/mnist_num_layers.pdf}
    \caption{Зависимость абсолютного значения разности функций потерь от доступного размера выборки, прямая классификация изображений. Графики слева показывают уменьшение значений с увеличением размерности скрытого слоя. Графики справа показывают увеличение значений с увеличением количества слоев.}
    \label{chapter:complex:experiment:fig:mnist}
\end{figure}

\begin{figure}[h!t]\center
    \includegraphics[width=0.5\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/mnist_hidden_size.pdf}\hfill
    \includegraphics[width=0.5\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/mnist_num_layers.pdf}
    \caption{Зависимость абсолютного значения разности функций потерь от доступного размера выборки, \textbf{извлечение признаков изображений}. Графики слева показывают уменьшение значений с увеличением размерности скрытого слоя. Графики справа показывают увеличение значений с увеличением количества слоев.}
    \label{chapter:complex:experiment:fig:mnist-extraction}
\end{figure}

\begin{table}[h!t]\center
  \caption{Описание наборов данных для классификации изображений}
  \label{chapter:complex:experiment:table:datasets}
  \begin{tabular}{llll}
    \toprule
    Название     & Описание     & Формат & Разрешение \\
    \midrule
    MNIST \cite{deng2012mnist} & Рукописные цифры & Оттенки серого & $28 \times 28$ \\
    FashionMNIST \cite{xiao2017fashionmnistnovelimagedataset} & Элементы одежды & Оттенки серого & $28 \times 28$ \\
    CIFAR10 \cite{krizhevsky2009learning} & Различные объекты & RGB & $32\times 32$ \\
    CIFAR100 \cite{krizhevsky2009learning} & Различные объекты & RGB & $32 \times 32$ \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[!ht]\center
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/fashion_mnist_hidden_size} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/fashion_mnist_num_layers} 
  \end{subfigure} 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/cifar10_hidden_size} 
  \end{subfigure}%%
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/cifar10_num_layers} 
  \end{subfigure} 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/cifar100_hidden_size} 
  \end{subfigure}%%
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs/cifar100_num_layers} 
  \end{subfigure} 
  \caption{Зависимость абсолютного значения разности функций потерь от доступного размера выборки, прямая классификация изображений. Графики слева показывают уменьшение значений с увеличением размерности скрытого слоя. Графики справа показывают увеличение значений с увеличением количества слоев. Результаты для различных наборов данных: FashionMNIST, CIFAR10 и CIFAR100.}
  \label{chapter:complex:experiment:fig:additional-exp} 
\end{figure}

\begin{figure}[h!t]\center
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/fashion_mnist_hidden_size} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/fashion_mnist_num_layers} 
  \end{subfigure} 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/cifar10_hidden_size} 
  \end{subfigure}%%
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/cifar10_num_layers} 
  \end{subfigure} 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/cifar100_hidden_size} 
  \end{subfigure}%%
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/landscape-hessian/figs_extraction/cifar100_num_layers} 
  \end{subfigure} 
  \caption{Зависимость абсолютного значения разности функций потерь от доступного размера выборки, извлечение признаков изображений. Графики слева показывают уменьшение значений с увеличением размерности скрытого слоя. Графики справа показывают увеличение значений с увеличением количества слоев. Результаты для различных наборов данных: FashionMNIST, CIFAR10 и CIFAR100.}
  \label{chapter:complex:experiment:fig:additional-exp-extraction} 
\end{figure}

Для проверки полученных теоретических оценок проводится вычислительный эксперимент.
В данном разделе представлены результаты обучения полносвязной нейронной сети для задачи классификации изображений.
Основной целью этих экспериментов является эмпирическое подтверждение сходимости ландшафта функции потерь с увеличением размера выборки.
Для достижения этой цели обучена полносвязная нейронная сеть на полном наборе данных и получены соответствующие параметры $\hat{\boldsymbol{\theta}}$ как точка вблизи минимума.

Для вычислительного эксперимента используется библиотека \texttt{pytorch}~\cite{pytorch2019} в качестве Python-фреймворка для обучения нейронных сетей.
Архитектура сети была единообразной и состоит из нескольких линейных слоев с функцией активации ReLU после каждого слоя, за исключением последнего.
Размер~$h$ зафиксирован для всех скрытых слоев $L$.
Обучение сети происходит в течение нескольких эпох с использованием оптимизатора \texttt{Adam}~\cite{adam2015} с постоянной скоростью обучения $10^{-3}$.
Для обучения используются различные наборы данных для классификации изображений, доступные в библиотеке~\texttt{torchvision}.
Для процесса обучения выбран размер батча (англ. batch size) 64.
Эксперименты проводятся на GPU Tesla A100 80GB с 16 CPU ядрами и 243 GB оперативной памяти.

В первом эксперименте используются значения пикселей изображений в качестве входных данных.
На Рис.~\ref{chapter:complex:experiment:fig:mnist} представлены результаты, полученные при анализе 10 000 объектов из набора данных MNIST \cite{deng2012mnist}, при этом сеть обучалась в течение 10 эпох.
Соответствующий размер входа составляет 784, а размер выхода~---~$10$.
Графики слева были получены при фиксированном количестве слоев~$L=5$ в сети.
Размер скрытого слоя во всех слоях варьировался от~$4$ до~$64$.
В то же время, график справа иллюстрирует поведение разности потерь при изменении количества скрытых слоев от~$1$ до~$10$, при фиксированном размере скрытого слоя~$h=16$.
Эта последовательность была повторена~$100$ раз для усреднения.
К полученным результатам было применено экспоненциальное скользящее среднее с коэффициентом сглаживания~$0{,}99$.
Из наблюдаемых зависимостей видно, что, хотя изменение и не является значительным, добавление большего количества слоев приводит к большей разнице в функциях потерь.
И наоборот, увеличение размера скрытого слоя приводит к меньшей разнице между функциями потерь.
Кроме того, на практике константа $M$, ограничивающая величину весов, оказывается относительно небольшой.
Более того, поскольку задача классификации набора данных MNIST считается относительно простой, неглубокая, но широкая нейронная сеть может давать хорошие результаты классификации.
Следовательно, наблюдалось, что значения функции потерь ниже для больших значений $h$, и поэтому их разница также меньше.

В отличие от предыдущего эксперимента, в этой части используется предварительно обученный экстрактор признаков изображений.
Полносвязная сеть используется в качестве многоклассового классификатора.
В качестве модели выбрана модель Vision Transformer (ViT)~\cite{wu2020visual} от Google.
Аналогичным образом выбраются случайным образом~$10\,000$ объектов из набора данных MNIST и варьировали размер скрытого слоя и количество слоев.
Результаты согласуются с наблюдениями при прямой классификации изображений.
Эта согласованность подтверждает, что представленная сходимость не зависит от природы пространства исходных объектов $\mathcal{X}$.
Для наблюдения сходимости ландшафта функции потерь достаточно ограниченности этого пространства.

Эксперимент подтверждает сходимость, доказанную в Теореме~\ref{chapter:complex:theorem-kiselev-loss}.
Кроме того, верхняя оценка скорости этой сходимости остается верной.
Действительно, изменение параметров нейронной сети, таких как количество слоев и размер слоя, приводит к незначительному изменению разности функций потерь.

Приведем расширенную версию проведенных экспериментов.
Ниже в Таблице~\ref{chapter:complex:experiment:table:datasets} представлено описание используемых наборов данных.
Выбирается четыре набора данных из библиотеки \texttt{torchivison}: MNIST~\cite{deng2012mnist}, FashionMNIST~\cite{xiao2017fashionmnistnovelimagedataset}, CIFAR10 и CIFAR100~\cite{krizhevsky2009learning}.
Единственной предобработкой данных являлась нормализация для приведения значений к диапазону~$[-1; 1]$.

На Рис.~\ref{chapter:complex:experiment:fig:additional-exp} графики слева получены при фиксированном количестве слоев $L=5$ в сети.
При изменения размера скрытого слоя на всех уровнях от $4$ до $64$.
В то же время, график справа демонстрирует поведение разности потерь при изменении количества скрытых слоев от $1$ до $10$, при сохранении размера $h = 16$ неизменным.
Данная последовательность была повторена $100$ раз для усреднения.
Для полученных результатов применяется экспоненциальное скользящее среднее с коэффициентом сглаживания $0.99$.

Аналогично Рис.~\ref{chapter:complex:experiment:fig:mnist-extraction} на Рис.~\ref{chapter:complex:experiment:fig:additional-exp-extraction} результаты подтверждают сходимость, доказанную в Теореме~\ref{chapter:complex:theorem-kiselev-loss}.
Также верна верхняя оценка скорости этой сходимости.
В частности, изменение параметров нейронной сети: количества слоев и размера слоя приводит к изменению разности функций потерь.

\subsection{Сверточные модели глубокого обучения}
\begin{figure}[h!t]
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/mnist_change_layers} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_layers.pdf} 
  \end{subfigure} 
  \caption{Переменное количество скрытых сверточных слоев $L$ с фиксированным размером ядра $k = 3$ и количеством каналов $C = 6$. Анализ полученных графиков показывает немонотонный характер зависимости выходных значений от количества слоев.}
   \label{chapter:complex:experiment:matricez:fig:layers}
\end{figure}

\begin{figure}[h!t]
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/mnist_change_kers} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_kers} 
  \end{subfigure} 
  \caption{Изменение размера ядра $k$ при фиксированном количестве сверточных слоев $L$ и количестве каналов $C = 6$. Данные демонстрируют немонотонный характер зависимости относительно размера ядра.}
   \label{chapter:complex:experiment:matricez:fig:kernels}
\end{figure}

\begin{figure}[h!t]
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/mnist_change_channels.pdf} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_channels.pdf} 
  \end{subfigure} 
  \caption{Изменение количества каналов $C$ при фиксированном количестве сверточных слоев $L$ и размере ядра $k=3$. Зависимость значения от количества каналов имеет монотонный характер.}
   \label{chapter:complex:experiment:matricez:fig:channels}
\end{figure}

\begin{figure}[h!t]
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_maxpool_pos} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_avgpool_pos} 
  \end{subfigure} 
  \caption{Изменение количества каналов $C$ при фиксированном количестве сверточных слоев $L$ и размере ядра $k=3$. График показывает монотонную зависимость значения от позиции пулинга в сети.}
   \label{chapter:complex:experiment:matricez:fig:pool_pos}
\end{figure}

В данном разделе представлены результаты обучения сверточных сетей с различными параметрами.
Основной целью экспериментов является демонстрация зависимости ландшафта функции потерь от таких параметров, как количество слоев, размер ядра, количество каналов, позиции пулинга, и наблюдение того, как скорость сходимости зависит от этих параметров.
Для достижения этой цели обучаются сверточные сети и получены параметры $\hat{\boldsymbol{\theta}}$ вблизи оптимума.
В качестве модели используется сверточная архитектура с функцией активации ReLU после каждого слоя.
Чтобы проследить влияние заданного параметра на сходимость, фиксируется все другие параметры нейронной сети, а заданный параметр вариаруется и происходит обучения заданной модели.
Далее исследуется зависимость между средним абсолютным различием между значениями средней функции потерь и доступным размером выборки.
Далее, для каждой модели, чтобы получить более надежные результаты, производится усреднения разность потерь по перемешанным выборкам.
Дополнительно, для улучшения визуализации, используется экспоненциальное сглаживание с коэффициентом $0.995$.
Для данного эксперимента используется числовое представление пикселей изображений в качестве входных данных.
Результаты получены на основе анализа выборок из баз данных MNIST~\cite{deng2012mnist}, FashionMNIST~\cite{xiao2017fashionmnistnovelimagedataset} и CIFAR10~\cite{krizhevsky2009learning}.
Во всех экспериментах использовались следующие гиперпараметры: постоянная скорость обучения 1e-3, оптимизатор Adam, мини-пакеты размером 64, обучение проводилось в течение 10 эпох на наборах данных MNIST и Fashion-MNIST и 15 эпох на наборе данных CIFAR-10.
Если параметр не варьировался, он сохранялся одинаковым во всех слоях.

\subsection{Трансформер модели глубокого обучения}

\begin{figure}[h!t]\center
    \includegraphics[width=0.5\textwidth]{thesis/figures/chapter-1/transformer/loss_landscape_convergence}
    \caption{Абсолютная разность потерь в зависимости от количества обучающих примеров в наборе данных, отображенная в двойном логарифмическом масштабе. \textcolor{blue}{Синяя} линия представляет экспоненциальное скользящее среднее (EMA) желаемой зависимости, а \textcolor{gray}{серая} линия соответствует линейному тренду.}
    \label{chapter:complex:experiment:fig:loss_landscape_convergence}
\end{figure}

Для более глубокого изучения зависимости между функцией потерь и ее гессианом проводится эксперимент, соответствующий Теореме~\ref{chapter:complex:theorem-transformer-loss}.
Здесь используется другая конфигурацию модели на наборе данных CIFAR-100~\cite{krizhevsky2009learning}.
По сравнению с аналогичной моделью для набора данных MNIST, эта модель имеет в $8$ раз больше блоков Трансформера, а также скрытые слои в $8$ раз шире.
Во время обучения модель также обучалась в течение ряда эпох для достижения точности >50\% на валидационном наборе данных.
Результаты представлены на Рис.~\ref{chapter:complex:experiment:fig:loss_landscape_convergence}.
Настройка эксперимента следующая:
\begin{enumerate}
    \item Модель обучается до сходимости и сохраняется вектор параметров~$\mathbf{w*}.$
    \item Начиная с пустого набора данных, добавляются данные батч за батчем (англ. batch) и вычисляется среднее значение потерь по просмотренным батчам;
    \item Вычисляется абсолютная разность в соответствии с выражением:
        \[
            \left| \mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w}) \right|.
        \]
\end{enumerate}

\section{Заключение по главе}
В данной главе проведено комплексное исследование сходимости ландшафта функции потерь при увеличении объема выборки для различных архитектур нейронных сетей.

Теоретический анализ и эмпирические результаты для полносвязных нейронных сетей демонстрируют, что абсолютная разность между средними значениями функции потерь при добавлении нового объекта в выборку стремится к нулю при увеличении количества доступных объектов до бесконечности.
Это достигнуто за счет доказательства теоремы о верхней оценке нормы гессиана. Полученные результаты подтверждают сходимость поверхности функции потерь для задачи классификации изображений как при прямом использовании исходных представлений, так и при работе с предобученным экстрактором признаков.

Для сверточных нейронных сетей установлено, что абсолютная разность между средними значениями функции потерь демонстрирует монотонную зависимость от размера слоя и позиции пулинга, в то время как зависимость от размера ядра и количества слоев имеет немонотонный характер.
Это указывает на преобладающее влияние значения функции потерь в точке оптимума.
Предложенный метод оценки нормы гессиана и его использования для анализа сходимости ландшафта потерь предоставляет вклад в теорию анализа локальной геометрии ландшафтов функции потерь.

В случае трансформерных моделей работа восполняет ключевой пробел в анализе путем явного вывода якобианов и гессианов для LayerNorm и FFN.
Теоретические результаты показывают гетерогенность гессиана по блокам, причем наибольший вклад вносят компоненты, связанные с Values и Keys.
Установленное неравенство сходимости $|\mathcal{L}_{k+1}(\mathbf{w})-\mathcal{L}_{k}(\mathbf{w})| \le 2L/(k+1) + M\|\mathbf{w}-\mathbf{w}^*\|_2^2/(k+1)$ объясняет наблюдаемую стабилизацию ландшафта потерь с ростом объема данных.

Основные ограничения исследования включают детерминистический характер анализа, предположение о существовании единой точки минимума для последовательных размеров выборки, а также возможность улучшения верхних оценок за счет учета специфики разреженных матриц.
Полученные результаты открывают перспективы для разработки методов определения достаточного размера выборки и алгоритмов обучения.