\begin{lemma}[Производная умножения матричнозначных функций]\label{lemma:matrix_funcs_product_derivative}
    Пусть заданы~$\mathbf{A}(\mathbf{X}) \in \mathbb{R}^{p \times r}$ и~$\mathbf{B}(\mathbf{X}) \in \mathbb{R}^{r \times q}$ матрицезначные функции переменной~$\mathbf{X}$, тогда
    \begin{equation}
        \frac{\partial\mathbf{A}(\mathbf{X})\mathbf{B}(\mathbf{X})}{\partial \mathbf{X}} = \left(\mathbf{A} \otimes \mathbf{I}_q 
     \right) \frac{\partial\mathbf{B}}{\partial\mathbf{X}} + \left( \mathbf{I}_p \otimes \mathbf{B}^\top\right) \frac{\partial\mathbf{A}}{\partial\mathbf{X}}
    \end{equation}
\end{lemma}
\begin{proof}
Применить цепное правило для вычисления производной сложной функции, а затем объединим его со свойством~\ref{prop:matrix_product_derivative}
    \begin{align}
        \frac{\partial\mathbf{A}(\mathbf{X}) \mathbf{B}(\mathbf{X})}{\partial \mathbf{X}} &= \frac{\partial \mathbf{A}\mathbf{B}}{\partial\mathbf{B}}\frac{\partial\mathbf{B}}{\partial \mathbf{X}} + \frac{\partial \mathbf{A}\mathbf{B}}{\partial\mathbf{A}}\frac{\partial\mathbf{A}}{\partial \mathbf{X}} =\\
        &=\frac{\partial \mathbf{A}\mathbf{B}\mathbf{I}_q}{\partial\mathbf{B}}\frac{\partial\mathbf{B}}{\partial \mathbf{X}} + \frac{\partial \mathbf{I}_p\mathbf{A}\mathbf{B}}{\partial\mathbf{A}}\frac{\partial\mathbf{A}}{\partial \mathbf{X}} = \\
        &= \left( \mathbf{A} \otimes \mathbf{I}_q \right) \frac{\partial \mathbf{B}}{\partial \mathbf{X}} + \left( \mathbf{I}_p \otimes \mathbf{B}^\top \right) \frac{\partial \mathbf{A}}{\partial \mathbf{X}}
    \end{align}
\end{proof}

\begin{lemma}[Теорема идентификации для построчной векторизации] \label{lemma:identification_theorem_vec_r}

Пусть отображение~$\mathbf{F}: \mathbb{R}^{m\times n} \rightarrow \mathbb{R}^{p, q}$ является дифференциальной матричнозначной функцией от~$\mathbf{X} \in \mathbb{R}^{m \times n}$.
Если дифференциал функции~$\mathbf{F}$ может быть записан в виде:
\begin{equation}
    d \mathrm{vec}_r(\mathbf{F}(\mathbf{X})) = \mathbf{J} \cdot d \mathrm{vec}_r(\mathbf{X}),
\end{equation}
где матрица~$\mathbf{J} \in \mathbb{R}^{pq \times mn}$ является, некоторой константной матрицей относительно переменной~$d\mathbf{X}$, тогда матрица~$\mathbf{J}$ является матрицей Якоби преобразования $\mathbf{F}(\mathbf{X})$ относительно построчной векторизации. Обозначим это в следующем виде:
\begin{equation}
    \frac{\partial \mathbf{F}(\mathbf{X})}{\partial \mathbf{X}} := \frac{\partial \mathrm{vec}_r(\mathbf{F}(\mathbf{X}))}{\partial (\mathrm{vec}_r(\mathbf{X}))^\top} = \mathbf{J}
\end{equation}
    
\end{lemma}
\begin{proof}
Это построчный $\mathrm{vec}_r$-аналог первой теоремы об идентификации (англ. first identification theorem) в главе 12~\cite{magnus1988matrix} для векторизации по столбцам.
\end{proof}

\begin{lemma}[Производная квадрата Адамара]\label{lemma:hadamard_square_derivative}
Для матрицы~$\mathbf{A} \in \mathbb{R}^{m \times n}$, производная поэлементного квадрата равна
\[
    \frac{\partial \mathbf{A}^{\circ 2}}{\partial \mathbf{A}}
    = 2 \cdot \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{A})\big).
\]
\end{lemma}
\begin{proof}
Используя определение~\ref{def:vec_elem_ops}, а именно $(\mathbf{A}^{\circ 2}){ij} = (\mathbf{A}{ij})^2$.
Выполняя поэлеметное взятие дифференциала, получаем, что:
\[
    d(\mathbf{A}^{\circ 2}) = 2\mathbf{A} \circ d\mathbf{A}.
\]
Далее, применяя оператор~$\mathrm{vec}_r$ и используя свойство~\ref{prop:vec_r_hadamard_product} получаем выражение:
\[
    \mathrm{vec}_r(d(\mathbf{A}^{\circ 2})) = 2 \textit{diag}(\mathrm{vec}_r(\mathbf{A})) \mathrm{vec}_r (d\mathbf{A}),
\]
причем, используя построчный аналог первой теоремы об идентификации~\ref{lemma:identification_theorem_vec_r} получаем следующий вид:
\[
    \frac{\partial \mathbf{A}^{\circ 2}}{\partial \mathbf{A}}
    = \frac{\partial \mathrm{vec}_r(\mathbf{A}^{\circ 2})}{\partial \mathrm{vec}_r(\mathbf{A})} = 2 \cdot \mathrm{diag}\!\big(\mathrm{vec}_r(\mathbf{A})\big).
\]
\end{proof}

\begin{lemma}[Производная корня Адамара]\label{lemma:hadamard_root_derivative}
Для матричнозначной функции~$\mathbf{A} \in \mathbb{R_+}^{m \times n}$ с положительными элементами, производная поэлементного корня равна
\[
    \frac{\partial \mathbf{A}^{\circ \frac{1}{2}}}{\partial \mathbf{A}}
    = \tfrac{1}{2} \mathrm{diag}^{-1}\!\big(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{A})\big).
\]
\end{lemma}
\begin{proof}
Аналогично, доказательству леммы~\ref{lemma:hadamard_square_derivative} получаем~$d(\mathbf{A}^{\circ 1/2}) = \frac{1}{2}\mathbf{A}^{\circ -1/2} \circ d\mathbf{A},$  откуда в векторном виде получаем, следующее выражение:
\[
    \frac{\partial \mathbf{A}^{\circ \frac{1}{2}}}{\partial \mathbf{A}}
    = \frac{\partial \mathrm{vec}_r(\mathbf{A}^{\circ \frac{1}{2}})}{\partial \mathrm{vec}_r(\mathbf{A})} = \tfrac{1}{2} \mathrm{diag}^{-1}\!\big(\mathrm{vec}_r^{\circ \frac{1}{2}}(\mathbf{A})\big).
\]
\end{proof}

\begin{lemma}[Производная обратной матрицы]\label{lemma:invert_derivative}
Пусть задана обратимая квадратная матрица~$\mathbf{D} \in \mathbb{R}^{n \times n}$, тогда производная операции обращения равно:
\[
    \frac{\partial \mathbf{D}^{-1}}{\partial \mathbf{D}} 
    = -\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top}.
\]
\end{lemma}
\begin{proof}
По определению из~\cite{petersen2012matrix} и~\cite{magnus1988matrix}:
\[
    d(\mathbf{D}^{-1}) = -\mathbf{D}^{-1} (d\mathbf{D}) \mathbf{D}^{-1}.
\] 
Используя оператор~$\mathrm{vec}_r$ и свойство~\ref{prop:vec_r_matrix_product}, получаем 
\[
    \mathrm{vec}_r(-\mathbf{D}^{-1} (d\mathbf{D}) \mathbf{D}^{-1}) = (-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top}) \mathrm{vec}_r(d\mathbf{D}),
\]
причем, используя лемму~\ref{lemma:identification_theorem_vec_r} получаем:
\[
    \mathrm{vec}_r(d\mathbf{D}^{-1}) = \frac{\partial \mathrm{vec}_r \mathbf{D}^{-1}}{\partial\mathrm{vec}_r\mathbf{D}} \mathrm{vec}_r(d\mathbf{D}).
\]
Следовательно получаем выражение, которое заканчивает доказательство леммы~ $\frac{\partial \mathrm{vec}_r \mathbf{D}^{-1}}{\partial\mathrm{vec}_r\mathbf{D}} = (-\mathbf{D}^{-1} \otimes \mathbf{D}^{-\top})$
\end{proof}

\begin{lemma}[Производная $\mathrm{diag}(\cdot)$]\label{lemma:diag_derivative}
Для вектора~$\mathbf{v} \in \mathbb{R}^{L \times 1}$, производная оператора диагонализации является:
\[
    \frac{\partial \mathrm{diag}(\mathbf{v})}{\partial \mathbf{v}}
    = \big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots \quad \mathbf{e}_L \otimes \mathbf{e}_L\big),
\]
где вектора~$\mathbf{e}_i$ являются базисными в пространстве~$\mathbb{R}^L$.
\end{lemma}

\begin{proof}
По определению~\ref{def:vec_elem_ops} оператор~$\mathrm{diag}(\mathbf{v})$ отображает элемент~$v_i,$ в позицию~$(i,i)$ результирующей диагональной матрицы.
Тогда производная оператора~$\mathrm{diag}(\mathbf{v})$ является матрицей~$\mathbf{E}_{ii} = \mathbf{e}_i \mathbf{e}_i^\top,$ в которой~$1$ в позиции~$(i,i)$~и $0$ иначе. Причем используя свойство~\ref{prop:vec_r_matrix_product}, применяя оператор построчной векторизации, получаем:
\[
    \mathrm{vec}_r(\mathbf{E}_{i,i}) = \mathbf{e}_i \otimes \mathbf{e}_i.
\]

Итого, применяя для всех~$i=1,\dots,L$, матрица Якоби принимает вид:
\[
    \frac{\partial \mathrm{diag}(\mathbf{v})}{\partial \mathbf{v}}
    = \big(\mathbf{e}_1 \otimes \mathbf{e}_1 \quad \dots \quad \mathbf{e}_L \otimes \mathbf{e}_L\big).
\]
\end{proof}

\begin{lemma}[Производная транспонированной матрицы]\label{lemma:transposed_matrix_derivative}
    Пусть задана матрица~$\mathbf{A} \in \mathbb{R}^{m \times n}$, тогда справедливо следующее равенство:
    \begin{equation}
        \frac{\partial \mathbf{A}^\top}{\partial \mathbf{A}} = \mathbf{K}_{n, m},
    \end{equation}
    где матрица~$\mathbf{K}_{n, m}$ является коммутационной матрицей (англ. commutation matrix) описанной в определении~\ref{def:commutation_matrix}.
\end{lemma} 
\begin{proof}
    Объединяя аналогичное свойство из~\cite{magnus1988matrix} для постолбцовой векторизации с правилом соединения столбцов и строк~\ref{prop:vec_relation} и \ref{def:commutation_matrix}, получаем утверждение теоремы.
\end{proof}


\begin{lemma}[Производная произведения Кронекера матричнозначных функций]\label{lemma:matrix_funcs_kronecker_product_derivative}
    Пусть заданы матричнозначные функции~$\mathbf{A}(\mathbf{X}) \in \mathbb{R}^{n \times q}$ и~$\mathbf{B}(\mathbf{X}) \in \mathbb{R}^{p \times r}$ матрицы $\mathbf{X}$, тогда
    \begin{equation}
        \frac{\partial\mathbf{A}(\mathbf{X}) \otimes \mathbf{B}(\mathbf{X})}{\partial \mathbf{X}} = (\mathbf{I}_n \otimes \mathbf{K}_{p,q} \otimes \mathbf{I}_r) \left( \left( \mathrm{vec}_r \mathbf{A} \otimes \mathbf{I}_{pr} \right) \frac{\partial\mathbf{B}}{\partial \mathbf{X}} + \left( \mathbf{I}_{nq} \otimes \mathrm{vec}_r \mathbf{B} \right) \frac{\partial\mathbf{A}}{\partial \mathbf{X}}\right).
    \end{equation}
\end{lemma}
\begin{proof}
    Применить цепное правило для вычисления производной сложной функции, а затем объединим его со свойством~\ref{prop:kronecker_product_derivative}
    \begin{align}
        \frac{\partial\mathbf{A}(\mathbf{X}) \otimes \mathbf{B}(\mathbf{X})}{\partial \mathbf{X}} &= \frac{\partial \mathbf{A}\otimes\mathbf{B}}{\partial\mathbf{B}}\frac{\partial\mathbf{B}}{\partial \mathbf{X}} + \frac{\partial \mathbf{A}\otimes\mathbf{B}}{\partial\mathbf{A}}\frac{\partial\mathbf{A}}{\partial \mathbf{X}} = \\
        &= (\mathbf{I}_n \otimes \mathbf{K}_{p,q} \otimes \mathbf{I}_r) \left( \mathrm{vec}_r \mathbf{A} \otimes \mathbf{I}_{pr} \right) \frac{\partial\mathbf{B}}{\partial \mathbf{X}} +\\
        &\quad+(\mathbf{I}_n \otimes \mathbf{K}_{p,q} \otimes \mathbf{I}_r) \left( \mathbf{I}_{nq} \otimes \mathrm{vec}_r \mathbf{B} \right) \frac{\partial\mathbf{A}}{\partial \mathbf{X}} = \\
        &= (\mathbf{I}_n \otimes \mathbf{K}_{p,q} \otimes \mathbf{I}_r) \left( \left( \mathrm{vec}_r \mathbf{A} \otimes \mathbf{I}_{pr} \right) \frac{\partial\mathbf{B}}{\partial \mathbf{X}} + \left( \mathbf{I}_{nq} \otimes \mathrm{vec}_r \mathbf{B} \right) \frac{\partial\mathbf{A}}{\partial \mathbf{X}}\right).
    \end{align}
\end{proof}

\begin{lemma}\label{lemma:attention_phi_from_functional_hessian}
Рассмотрим слой внимания следующего вида:
\[
    \mathbf{F}(\mathbf{X}) = \mathbf{A}(\mathbf{T})  \mathbf{X}\mathbf{W}_V,
    \qquad
    \mathbf{T} = \frac{1}{\sqrt{d_K}} \mathbf{X}\mathbf{W}_Q \mathbf{W}_K^\top \mathbf{X}^\top,
\]
где $\mathbf{X} \in \mathbb{R}^{L \times d_V}$, $\mathbf{W}_Q,\mathbf{W}_K \in \mathbb{R}^{d_V \times d_K}$, $\mathbf{W}_V \in \mathbb{R}^{d_V \times d_V}$.
Матрица внимания $\mathbf{A}(\cdot)$ применяет построчный softmax.
Используем построчную векторизацию~$\mathrm{vec}_r(\cdot)$ и матрицы перестановки~$\mathbf{K}_{m,n}$ из определения~\ref{def:commutation_matrix}.

Определим блоки обобщенного функционального гессиана, используя результаты~\cite{ormaniec2024attentionhessian} в наших обозначениях $\mathrm{vec}_r$ как
\[
    \mathbf{H}_{\mathrm{f}}(\mathbf{W}_i,\mathbf{W}_j)
    = \big( \tfrac{\partial \ell}{\partial \mathbf{F}} \otimes \mathbf{I}_{p_i q_i} \big)  \frac{\partial^2 \mathbf{F}}{\partial \mathbf{W}_i \partial \mathbf{W}_j},
\]
где $p_i q_i$~--- размер матрицы~$\mathbf{W}_i$, а $\tfrac{\partial \ell}{\partial \mathbf{F}} \in \mathbb{R}^{L \times d_V}$~--- градиент функции потерь.

Для квадратичной функции потерь~$\ell(\mathbf{F})=\tfrac{1}{2}\|\mathbf{F}-\mathbf{Target}\|_F^2$ имеем~$\tfrac{\partial \ell}{\partial \mathbf{F}}=\mathbf{F}-\mathbf{Target}$ и матрицу построчного свертывания
\[
    \mathbf{R}_m := \mathrm{vec}_r\big(\mathbf{F}(\mathbf{X}) - \mathbf{Target}\big)^\top \otimes \mathbf{I}_m \in \mathbb{R}^{m \times (m \cdot L d_V)}.
\]
Тогда для~$i \in \{V,Q,K\}$ с $n_i:=p_i q_i$ блоки функционального гессиана могут быть факторизованы как
\[
    \mathbf{H}_{\mathrm{f}}(\mathbf{W}_i,\mathbf{W}_j) = \mathbf{R}_{n_i} \boldsymbol{\Phi}_{ij},
    \qquad
    \boldsymbol{\Phi}_{ij} := \frac{\partial^2 \mathbf{F}}{\partial \mathbf{W}_i \partial \mathbf{W}_j}
    \in \mathbb{R}^{(L d_V \cdot n_i) \times n_j}.
\]
В частности, блоки кривизны модели~$\boldsymbol{\Phi}_{ij}$ получаются из соответствующих выражений в~\cite[Теорема.~3.2]{ormaniec2024attentionhessian} удалением левого свертывания $\mathbf{R}_{n_i}$.

Теперь перечислим явные блоки, необходимые для вывода. Определим фиксированный оператор изменения формы
\[
    \mathbf{S} := \big(\mathbf{I}_{d_V} \otimes \mathbf{K}_{d_V, d_V}\big) \big(\mathrm{vec}_r \mathbf{I}_{d_V} \otimes \mathbf{I}_{d_V}\big)
    \in \mathbb{R}^{d_V^2 \times d_V},
\]
и операторы производных softmax
\begin{align}
    \mathbf{Z}_1 &:= (\mathbf{I}_L \otimes \mathbf{X}^\top)(\partial\mathbf{A}/\partial\mathbf{T})(\mathbf{X} \otimes \mathbf{X}) \in \mathbb{R}^{Ld_V \times d_V^2}, \\
    \mathbf{Z}_2 &:= \big(\mathbf{I}_L \otimes \mathbf{X}^\top \otimes \mathbf{X}^\top \otimes \mathbf{X}^\top\big)
    \frac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2}
    (\mathbf{X} \otimes \mathbf{X})
    \in \mathbb{R}^{L d_V^3 \times d_V^2},
\end{align}
где~$\tfrac{\partial^2 \mathbf{A}}{\partial \mathbf{T}^2}$ обозначает тензор вторых производных softmax (построчный), согласованный с $\mathrm{vec}_r$ и произведениями Кронекера, как указано выше, а $\mathbf{Z}_1$ — линейный оператор первой производной softmax, используемый в~\cite{ormaniec2024attentionhessian}.

Тогда вторые производные внимания имеют вид:
\begin{align}
    \boldsymbol{\Phi}_{VV} &= \mathbf{0}_{(L d_V \cdot d_V^2) \times d_V^2}, \\
    \boldsymbol{\Phi}_{QQ} &= \frac{2}{L d_V d_K}
    \big(\mathbf{I}_L \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V} \otimes \mathbf{W}_K^\top\big)
    \mathbf{Z}_2
    \big(\mathbf{I}_{d_V} \otimes \mathbf{W}_K\big)
    \in \mathbb{R}^{(L d_V \cdot d_V d_K) \times d_V d_K}, \\
    \boldsymbol{\Phi}_{VQ} &= \frac{2}{L d_V \sqrt{d_K}}
    \big(\mathbf{I}_L \otimes \mathbf{S}\big)
    \mathbf{Z}_1
    \big(\mathbf{I}_{d_V} \otimes \mathbf{W}_K\big)
    \in \mathbb{R}^{(L d_V \cdot d_V^2) \times d_V d_K}, \\
    \boldsymbol{\Phi}_{QK}
    &= \frac{2}{L d_V d_K}
    \big(\mathbf{I}_L \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V} \otimes \mathbf{W}_K^\top\big)
    \mathbf{Z}_2
    \big(\mathbf{W}_Q \otimes \mathbf{I}_{d_V}\big) \mathbf{K}_{d_K, d_V} \\
    &\qquad + \frac{2}{L d_V \sqrt{d_K}}
    \big(\mathbf{I}_{d_V} \otimes \mathbf{W}_V^\top \otimes \mathbf{I}_{d_V}\big)
    \big(\mathbf{Z}_1 \otimes \mathbf{I}_{d_V}\big) \mathbf{S} \otimes \mathbf{I}_{d_K} \in \mathbb{R}^{(L d_V \cdot d_V d_K) \times d_V d_K}.
\end{align}

Более того, в силу симметрии вторых производных, $\boldsymbol{\Phi}_{KQ}$ равен $\boldsymbol{\Phi}_{QK}$ с переставленными $\mathbf{W}_Q,\mathbf{W}_K$ и корректировкой перестановки с помощью $\mathbf{K}_{\cdot,\cdot}$.
Аналогичные симметричные соотношения дают $\boldsymbol{\Phi}_{QV}$ и $\boldsymbol{\Phi}_{KV}$ из $\boldsymbol{\Phi}_{VQ}$.
\end{lemma}
\begin{proof}  
По определению обобщенного функционального гессиана в \cite{ormaniec2024attentionhessian},
\[
    \mathbf{H}_{\mathrm{f}}(\mathbf{W}_i,\mathbf{W}_j)
    = \big( \tfrac{\partial \ell}{\partial \mathbf{F}} \otimes \mathbf{I}_{p_i q_i} \big)
    \frac{\partial^2 \mathbf{F}}{\partial \mathbf{W}_i \partial \mathbf{W}_j}.
\]
Для квадратичной функции потерь~$\tfrac{\partial \ell}{\partial \mathbf{F}}=\mathbf{R}_{p_i q_i}$, определенную выше, а следовательно
\[
    \mathbf{H}_{\mathrm{f}}(\mathbf{W}_i,\mathbf{W}_j) = \mathbf{R}_{n_i} \boldsymbol{\Phi}_{ij},
\]
где $\boldsymbol{\Phi}_{ij} = \tfrac{\partial^2 \mathbf{F}}{\partial \mathbf{W}_i \partial \mathbf{W}_j}$.

Явные формы для~$\mathbf{H}_{\mathrm{f}}$ описаны в~\cite[Thm.~3.2]{ormaniec2024attentionhessian}. Используя выражения для~$\mathbf{H}_{\mathrm{f}}$ получаем выражения~$\boldsymbol{\Phi}_{ij}$ просто удаляя ведущей метрицы~$\mathbf{R}_{n_i}$.
\end{proof}

\begin{lemma}\label{lemma:Y_S_norm_bounds}
Пусть заданы матрицы~$\mathbf{X}\in\mathbb{R}^{L\times d_V}$, $\mathbf{Y}=\mathrm{LayerNorm}(\mathbf{F}(\mathbf{X})+\mathbf{X})\in\mathbb{R}^{L\times d_V}$ и задана сеть
\[
    \mathrm{FFN}(\mathbf{Y})=\sigma(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2,\qquad\mathbf{W}_1\in\mathbb{R}^{d_V\times d_{ff}},\quad\mathbf{W}_2\in\mathbb{R}^{d_{ff}\times d_V},
\]
пусть также задана~$\mathbf{S}=\mathbf{Y}+\mathrm{FFN}(\mathbf{Y})\in\mathbb{R}^{L\times d_V}$.
Тогда выполняются следующие оценки спектральных норм:
\begin{align}
    \|\mathbf{Y}\|_2 &\le \|\mathbf{Y}\|_F = \sqrt{Ld_V}, 
    \label{eq:Y_norm_bound}
    \\
    \|\mathrm{FFN}(\mathbf{Y})\|_2 &\le \sqrt{\min(L,d_{ff})} \|\mathbf{Y}\|_2\|\mathbf{W}_1\|_2\|\mathbf{W}_2\|_2,
    \label{eq:FFN_norm_bound}
    \\
    \|\mathbf{S}\|_2 \le \|\mathbf{Y}\|_2 + \|\mathrm{FFN}(\mathbf{Y})\|_2 
    &\le \sqrt{Ld_V}\Big(1+\sqrt{\min(L,d_{ff})}\|\mathbf{W}_1\|_2\|\mathbf{W}_2\|_2\Big).
    \label{eq:S_norm_bound}
\end{align}
\end{lemma}
\begin{proof}
Оценим~$\|\mathbf{Y}\|_2$. Согласно определению LayerNorm в рамках Теоремы~\ref{thm:layernorm_derivative} получаем:
\[
    \mathbf{Y} = \mathbf{P}(\mathbf{S}_0)\mathbf{M}(\mathbf{S}_0), \qquad \mathbf{S}_0:=\mathbf{F}(\mathbf{X})+\mathbf{X},
\]
где~$\mathbf{M}(\mathbf{S}_0)=\mathbf{S}_0-\tfrac{1}{d_V}\mathbf{S}_0\mathbf{1}_{d_V}\mathbf{1}_{d_V}^\top$ и~$\mathbf{P}=\mathrm{diag}^{-1}(\sigma)$ с $\sigma=\tfrac{1}{\sqrt{d_V}}(\mathbf{M}^{\circ 2}\mathbf{1})^{\circ 1/2}$, применяемым построчно.
Для любой строки~$i$ обозначим~$\mathbf{m}_i$ как~$i$-ю строку~$\mathbf{M}$ и~$\sigma_i=\tfrac{1}{\sqrt{d_V}}\|\mathbf{m}_i\|_2$.
Тогда~$i$-я строка~$\mathbf{Y}$ имеет вид~$\mathbf{y}_i=\mathbf{m}_i/\sigma_i$, а следовательно
\[
    \|\mathbf{y}_i\|_2^2 = \frac{\|\mathbf{m}_i\|_2^2}{\sigma_i^2}= \frac{\|\mathbf{m}_i\|_2^2}{(1/d_V)\|\mathbf{m}_i\|_2^2}= d_V.
\]
Таким образом, каждая строка~$\mathbf{Y}$ имеет евклидову норму $\sqrt{d_V}$. Получаем:
\[
    \|\mathbf{Y}\|_F^2 = \sum_{i=1}^L \|\mathbf{y}_i\|_2^2 = Ld_V,\qquad\text{откуда}\qquad\|\mathbf{Y}\|_F = \sqrt{Ld_V}.
\]
Используя из свойства~\ref{prop:matrix_norm_inequalities} неравенство для норм~$\|\mathbf{A}\|_2 \le \|\mathbf{A}\|_F$, получаем~\eqref{eq:Y_norm_bound}.

Оценим~$\|\mathrm{FFN}(\mathbf{Y})\|_2$.
Используя свойство~\ref{prop:matrix_product_norm} получаем следующую оценку:
\[
    \|\mathrm{FFN}(\mathbf{Y})\|_2= \|\text{ReLU}(\mathbf{Y}\mathbf{W}_1)\mathbf{W}_2\|_2\le \|\text{ReLU}(\mathbf{Y}\mathbf{W}_1)\|_2 \|\mathbf{W}_2\|_2,
\]
далее, используя свойство~\ref{prop:matrix_norm_inequalities}, получаем
\[
    \|\text{ReLU}(\mathbf{Y}\mathbf{W}_1)\|_2 \le \|\text{ReLU}(\mathbf{Y}\mathbf{W}_1)\|_F,
\]
причем согласно определению~\ref{def:matrix_norms}, норма~$\|\cdot\|_F^2$ представляет собой сумму квадратов.
Поэлементно~$\sigma(\cdot)$ удовлетворяет условию~$0\le \sigma(a)\le |a|$, а следовательно~$\sigma(a)^2 \le a^2$ для каждого элемента~$a \in \mathbb{R}$.
Поэтому получаем:
\[
    \|\sigma(\mathbf{Y}\mathbf{W}_1)\|_F \le \|\mathbf{Y}\mathbf{W}_1\|_F.
\]
Используя неравенство~$\|\cdot\|_F \le \sqrt{d}\|\cdot\|_2$ с $d=\operatorname{rank}(\cdot)$ из свойства~\ref{prop:matrix_norm_inequalities} получаем:
\[
    \|\mathbf{Y}\mathbf{W}_1\|_F \le \sqrt{\operatorname{rank}(\mathbf{Y}\mathbf{W}_1)}\|\mathbf{Y}\mathbf{W}_1\|_2,
\]
так как~$\mathbf{Y}\mathbf{W}_1 \in \mathbb{R}^{L\times d_{ff}}$, $\operatorname{rank}(\mathbf{Y}\mathbf{W}_1)\le \min(L,d_{ff})$. Таким образом получаем используя свойство~\ref{prop:matrix_product_norm}:
\[
    \|\mathbf{Y}\mathbf{W}_1\|_F \le \sqrt{\min(L,d_{ff})}\|\mathbf{Y}\mathbf{W}_1\|_2\le \sqrt{\min(L,d_{ff})}\|\mathbf{Y}\|_2 \|\mathbf{W}_1\|_2
\]
Собирая все вместе получаем:
\[
    \|\mathrm{FFN}(\mathbf{Y})\|_2\le \|\sigma(\mathbf{Y}\mathbf{W}_1)\|_F \|\mathbf{W}_2\|_2\le \sqrt{\min(L,d_{ff})}\|\mathbf{Y}\|_2\|\mathbf{W}_1\|_2\|\mathbf{W}_2\|_2,
\]
что заканчивает оценку~\eqref{eq:FFN_norm_bound}.

Оценим~$\|\mathbf{S}\|_2$.
Используя из свойства~\ref{prop:matrix_sum_norm} неравенство для нормы суммы, получаем:
\[
    \|\mathbf{S}\|_2=\|\mathbf{Y}+\mathrm{FFN}(\mathbf{Y})\|_2\le \|\mathbf{Y}\|_2 + \|\mathrm{FFN}(\mathbf{Y})\|_2,
\]
откуда подставляя~\eqref{eq:Y_norm_bound} и~\eqref{eq:FFN_norm_bound}, получаем~\eqref{eq:S_norm_bound}.
\end{proof}

\begin{lemma}\label{lemma:layernorm_deriv_hessian_norm}
Пусть заданы матрицы~$\mathbf{X} \in \mathbb{R}^{m \times n}$.
Производная LayerNorm $\mathbf{J}_{\mathrm{LN}}(\mathbf{X}) = \frac{\partial \text{LayerNorm}(\mathbf{X})}{\partial \mathbf{X}}$ вычисляется в соответствии с Теоремой~\ref{thm:layernorm_derivative}, а ее гессиан $\mathbf{H}_{\mathrm{LN}}(\mathbf{X}) = \frac{\partial^2 \text{LayerNorm}(\mathbf{X})}{\partial \mathbf{X}^2}$ вычисляется как в Теореме \ref{thm:layernorm_second_derivative}.

Тогда выполняются следующие оценки:
\begin{align}
    \big\|\mathbf{J}_{\mathrm{LN}}(\mathbf{X})\big\|_2
    &\le \frac{1}{\sigma_{\min}} + \frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3}, \label{eq:ln_jac_norm_bound}\\[4pt]
    \big\|\mathbf{H}_{\mathrm{LN}}(\mathbf{X})\big\|_2
    &\le \frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3}\Big(1+\sqrt{\tfrac{m}{n}}\Big)
    + \frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3}
    + \frac{3\|\mathbf{X}\|_2^3}{n\sigma_{\min}^5},
    \label{eq:ln_hess_norm_bound}
\end{align}
где $\sigma_{\min}$ обозначает $\min \limits_i \|\mathbf{M}_i \|_2$, где $\mathbf{M}(\mathbf{X}) = \mathbf{X}(\mathbf{I}_n - \tfrac{1}{n}\mathbf{1}_n\mathbf{1}_n^\top)$
\end{lemma}
\begin{proof}
Согласно Теореме~\ref{thm:layernorm_derivative}:
\[
    \mathbf{J}_{\mathrm{LN}}(\mathbf{X})= (\mathbf{P}\otimes \mathbf{I}_n)\mathbf{G}+ (\mathbf{I}_m\otimes \mathbf{M}^\top)\mathbf{H},
\]
где $\mathbf{G}=\mathbf{I}_{mn}-\tfrac{1}{n}(\mathbf{I}_m\otimes \mathbf{1}_{n\times n})$,
$\mathbf{H}=\tfrac{\partial \mathbf{P}}{\partial \mathbf{X}}$, и $\mathbf{P}=\mathrm{diag}^{-1}(\boldsymbol{\sigma})$.
Используя cвойства~\ref{prop:kronecker_product_norm}, \ref{prop:matrix_product_norm}, \ref{prop:matrix_sum_norm} получаем:
\begin{align}
    \|\mathbf{J}_{\mathrm{LN}}(\mathbf{X})\|_2 &\le \|\mathbf{P}\otimes \mathbf{I}_n\|_2\|\mathbf{G}\|_2 + \|\mathbf{I}_m\otimes \mathbf{M}^\top\|_2\|\mathbf{H}\|_2 =\\
    &=\|\mathbf{P}\|_2\|\mathbf{G}\|_2 + \|\mathbf{M}\|_2\|\mathbf{H}\|_2.
\end{align}
Оценим каждый множитель по отдельности. Множитель~$\|\mathbf{G}\|_2\le 1$, поскольку~$\tfrac{1}{n}\mathbf{1}_{n\times n}$ является проекцией, следовательно~$\|\mathbf{I}_n-\tfrac{1}{n}\mathbf{1}_{n\times n}\|_2\le 1$, и произведение Кронекера сохраняет оценку спектральной нормы согласно свойств~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, а также Леммы~\ref{lemma:1_spectral_norm}. Множитель~$\|\mathbf{P}\|_2 = \|\mathbf{D}^{-1}\|_2 = 1/\sigma_{\min}$, где $\mathbf{D}=\mathrm{diag}(\boldsymbol{\sigma})$. Множитель~$\|\mathbf{M}\|_2 \le \|\mathbf{X}\|_2$, потому что $\mathbf{M}(\mathbf{X}) = \mathbf{X}(\mathbf{I}_n - \tfrac{1}{n}\mathbf{1}_n\mathbf{1}_n^\top)$, и правый множитель является проектором с нормой $\le 1$ согласно свойства~\ref{prop:matrix_product_norm}. Для $\|\mathbf{H}\|_2=\big\|\tfrac{\partial \mathbf{P}}{\partial \mathbf{X}}\big\|_2$, Теорема~\ref{thm:layernorm_derivative} вместе с Леммами~\ref{lemma:invert_derivative}, \ref{lemma:diag_derivative}, \ref{lemma:hadamard_square_derivative}, \ref{lemma:hadamard_root_derivative} и свойствами~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm} дают оценку:
\begin{align}
    \Big\|\frac{\partial \mathbf{P}}{\partial \mathbf{X}}\Big\|_2 &\le \frac{1}{\sqrt{n}}\|\mathbf{D}^{-1}\otimes \mathbf{D}^{-\top}\|_2\Big\|\mathrm{diag}^{-1}\!\big(\mathrm{vec}_r^{\circ 1/2}(\mathbf{M}^{\circ 2}\mathbf{1}_n)\big)\Big\|_2 \cdot\\
    &\quad\cdot\|\mathbf{I}_m\otimes \mathbf{1}_n^\top\|_2\|\mathrm{diag}(\mathrm{vec}_r(\mathbf{M}))\|_2\Big\|\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\Big\|_2.
\end{align}
Используя свойство~\ref{prop:matrix_norm_inequalities} получаем оценки:
\begin{align}
    \|\mathbf{D}^{-1}\otimes \mathbf{D}^{-\top}\|_2 &=\|\mathbf{D}^{-1}\|_2^2=\frac{1}{\sigma_{\min}^{2}},\\
    \big\|\mathrm{diag}^{-1}(\cdot)\big\|_2 &= \frac{1}{\min_i\sqrt{\sum_{v} M_{i,v}^2}} = \frac{1}{\sqrt{n}\sigma_{\min}},\\
    \|\mathbf{I}_m\otimes \mathbf{1}^\top\|_2 &=\sqrt{n},\\
    \|\mathrm{diag}(\mathrm{vec}_r(\mathbf{M}))\|_2 &=\|\mathbf{M}\|_{\max}\le \|\mathbf{M}\|_2,\\
    \big\|\tfrac{\partial \mathbf{M}}{\partial \mathbf{X}}\big\|_2 &\le 1,
\end{align}
откуда получаем оценку:
\[
    \|\mathbf{H}\|_2 \le \frac{1}{\sqrt{n}\sigma_{\min}^{2}}\cdot \frac{1}{\sqrt{n}\sigma_{\min}}\cdot \sqrt{n}\cdot \|\mathbf{M}\|_2\cdot 1\le \frac{\|\mathbf{X}\|_2}{\sqrt{n}\sigma_{\min}^3}.
\]
Собирая все полученные оценки, получаем \eqref{eq:ln_jac_norm_bound}:
\[
    \|\mathbf{J}_{\mathrm{LN}}(\mathbf{X})\|_2
    \le \frac{1}{\sigma_{\min}}\cdot 1 + \|\mathbf{X}\|_2\cdot \frac{\|\mathbf{X}\|_2}{\sqrt{n}\sigma_{\min}^3}
    = \frac{1}{\sigma_{\min}} + \frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3}.
\]

Из Теоремы~\ref{thm:layernorm_second_derivative} (с $m,n$), используя $\tfrac{\partial^2 \mathbf{M}}{\partial \mathbf{X}^2}=0$,
\[
    \mathbf{H}_{\mathrm{LN}}(\mathbf{X})=(\mathbf{I}_{mn}\otimes \mathbf{G}^\top)\frac{\partial (\mathbf{P}\otimes \mathbf{I}_n)}{\partial \mathbf{X}}+ \big( (\mathbf{I}_m\otimes \mathbf{M}^\top)\otimes \mathbf{I}_{mn}\big)\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}+ (\mathbf{I}_{mn}\otimes \mathbf{H}^\top)\frac{\partial (\mathbf{I}_m\otimes \mathbf{M}^\top)}{\partial \mathbf{X}}.
\]
Оценим три слагаемых отдельно с помощью свойств~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}.
Первое слагаемое. По Предложению~\ref{prop:kronecker_product_derivative} получаем:
\[
    \frac{\partial (\mathbf{P}\otimes \mathbf{I}_n)}{\partial \mathbf{X}}=(\mathbf{I}_m \otimes \mathbf{K}_{n,m} \otimes \mathbf{I}_n)(\mathbf{I}_{m^2} \otimes \mathrm{vec}_r(\mathbf{I}_n))\frac{\partial \mathbf{P}}{\partial \mathbf{X}},
\]
а следовательно
\begin{align}
    \Big\|(\mathbf{I}_{mn}\otimes \mathbf{G}^\top)\frac{\partial (\mathbf{P}\otimes \mathbf{I}_n)}{\partial \mathbf{X}}\Big\|_2 &\le \|\mathbf{G}\|_2 \|\mathbf{I}_{m^2} \otimes \mathrm{vec}_r(\mathbf{I}_n)\|_2 \Big\|\frac{\partial \mathbf{P}}{\partial \mathbf{X}}\Big\|_2 =\\
    &=1 \cdot \sqrt{n} \cdot \frac{\|\mathbf{X}\|_2}{\sqrt{n}\sigma_{\min}^3}= \frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3}.
\end{align}
Второе слагаемое. Используя $\|\mathbf{I}_m\otimes \mathbf{M}^\top\|_2=\|\mathbf{M}\|_2\le \|\mathbf{X}\|_2$ и оценку для $\big\|\tfrac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\big\|_2$ получаем:
\[
    \Big\|\big( (\mathbf{I}_m\otimes \mathbf{M}^\top)\otimes \mathbf{I}_{mn}\big)\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\Big\|_2\le \|\mathbf{X}\|_2 \Big\|\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\Big\|_2.
\]
Далее оценим~$\big\|\tfrac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\big\|_2$, следуя той же цепочке, что и в доказательстве Теоремы~\ref{thm:layernorm_second_derivative}
запишем~$\tfrac{\partial \mathbf{P}}{\partial \mathbf{X}}=\tfrac{1}{\sqrt{n}}\mathbf{A}_1(\mathbf{X})\mathbf{E}\mathbf{B}_1(\mathbf{X})$ и продифференцируем, используя свойство~\ref{prop:matrix_product_norm} с Леммами~\ref{lemma:invert_derivative}, \ref{lemma:diag_derivative}, \ref{lemma:hadamard_square_derivative}, \ref{lemma:hadamard_root_derivative} и свойствами~\ref{prop:matrix_product_norm}, \ref{prop:kronecker_product_norm}, \ref{prop:matrix_norm_inequalities}. Получаем:
\[
    \Big\|\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\Big\|_2\le\frac{1}{\sqrt{n}\sigma_{\min}^3}\|\mathbf{X}\|_2+ \frac{3}{n\sigma_{\min}^5}\|\mathbf{X}\|_2^2,
\]
а следовательно,
\[
    \Big\|\big( (\mathbf{I}_m\otimes \mathbf{M}^\top)\otimes \mathbf{I}_{mn}\big)\frac{\partial^2 \mathbf{P}}{\partial \mathbf{X}^2}\Big\|_2\le \frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3}+ \frac{3\|\mathbf{X}\|_2^3}{n\sigma_{\min}^5}.
\]

Третье слагаемое. По свойству~\ref{prop:kronecker_product_derivative} и Лемме~\ref{lemma:transposed_matrix_derivative} получаем:
\[
    \frac{\partial (\mathbf{I}_m\otimes \mathbf{M}^\top)}{\partial \mathbf{X}}=(\mathbf{I}_m \otimes \mathbf{K}_{n,m} \otimes \mathbf{I}_m)(\mathrm{vec}_r(\mathbf{I}_m)\otimes \mathbf{I}_{mn})\frac{\partial \mathbf{M}}{\partial \mathbf{X}},
\]
откуда получаем:
\begin{align}
    \Big\|(\mathbf{I}_{mn}\otimes \mathbf{H}^\top)\frac{\partial (\mathbf{I}_m\otimes \mathbf{M}^\top)}{\partial \mathbf{X}}\Big\|_2 &\le \|\mathbf{H}\|_2 \|\mathrm{vec}_r(\mathbf{I}_m)\otimes \mathbf{I}_{mn}\|_2 \Big\|\frac{\partial \mathbf{M}}{\partial \mathbf{X}}\Big\|_2 =\\
    &=\frac{\|\mathbf{X}\|_2}{\sqrt{n}\sigma_{\min}^3}\cdot \sqrt{m}\cdot 1= \frac{\sqrt{m}}{\sqrt{n}}\frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3}.
\end{align}

Суммируя все слагаемые с помощью свойства~\ref{prop:matrix_sum_norm}  получаем~\eqref{eq:ln_hess_norm_bound}:
\begin{align}
    \|\mathbf{H}_{\mathrm{LN}}(\mathbf{X})\|_2 &\le \frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3} + \Big(\frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3} + \frac{3\|\mathbf{X}\|_2^3}{n\sigma_{\min}^5}\Big) + \frac{\sqrt{m}}{\sqrt{n}}\frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3} = \\
    &= \frac{\|\mathbf{X}\|_2}{\sigma_{\min}^3}\Big(1+\sqrt{\tfrac{m}{n}}\Big) + \frac{\|\mathbf{X}\|_2^2}{\sqrt{n}\sigma_{\min}^3} + \frac{3\|\mathbf{X}\|_2^3}{n\sigma_{\min}^5}.
\end{align}
\end{proof}

\begin{lemma}\label{lemma:1_spectral_norm}
    Пусть задана единичная матрица~$\mathbf{A} = \mathbf{1}_{L \times L}$. Тогда ее спектральная норма принимает следующее значение:
    \begin{equation}
        \|\mathbf{A}\|_2 = L
    \end{equation}
\end{lemma}
\begin{proof}
Используя основные свойства линейной алгебры, получаем~$\mathrm{tr}(\mathbf{A}) = L$ и~$\mathrm{rank}(\mathbf{A}) = 1 = \mathrm{dim}(\mathrm{Im}(\mathbf{X}))$.
Следовательно, используя~$\mathrm{dim}(\mathrm{Im}(\mathbf{X})) + \mathrm{dim}(\mathrm{Ker}(\mathbf{X})) = L$, получаем~$\mathrm{dim}(\mathrm{Ker}(\mathbf{X})) = L - 1$.
Таким образом, для~$i \in \{2, \dots L\}$ имеем~$\lambda_i = 0$, а~$\lambda_1 = L$.
Тогда единственное ненулевое сингулярное число матрицы~$\mathbf{A}$ равно $\sqrt{L^2} = L$.
Следовательно, получаем, что~$\|\mathbf{A}\|_2 = L$, в соответствии с определением~\ref{def:matrix_norms}.
\end{proof}