\documentclass[10pt,pdf]{beamer}

\mode<presentation>
{
\usetheme{boxes}
\beamertemplatenavigationsymbolsempty

\setbeamertemplate{footline}[page number]
\setbeamersize{text margin left=1.5em, text margin right=1.5em}
}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{bm}
\usepackage{multirow}
\usepackage{ragged2e}
\usepackage{indentfirst}
\usepackage{multicol}
\usepackage{subfig}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{comment}
\usepackage{multicol}
\usepackage{color}
\definecolor{python-green}{RGB}{55,126,33}

\usepackage[all]{xy}

\usepackage{tikz}
\usetikzlibrary{positioning,arrows}

\usepackage{caption}
\captionsetup{skip=0pt,belowskip=0pt}

\newtheorem{rustheorem}{Теорема}
\newtheorem{rusdefinition}{Определение}
\newtheorem{rusassumption}{Предположение}
\newtheorem{rusremark}{Замечание}
\newtheorem{ruscorollary}{Следствие}


% colors
\definecolor{darkgreen}{rgb}{0.0, 0.2, 0.13}
\definecolor{darkcyan}{rgb}{0.0, 0.55, 0.55}

\AtBeginEnvironment{figure}{\setcounter{subfigure}{0}}%

%----------------------------------------------------------------------------------------------------------

\title[О сложности моделей и данных]{О сложности моделей и данных \\в современных моделях глубокого обучения}
\author{А.В. Грабовой}

\institute[]{Диссертация на соискание ученой степени\\
доктора физико-математических наук\\~\\1.2.1~--- Искусственный интеллект и машинное обучение\\~\\Научный консультант: профессор РАН К.\,В. Воронцов}
\date[2026]{\small 2026\,г.}

%---------------------------------------------------------------------------------------------------------
\begin{document}

\begin{frame}
\titlepage
\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Актуальность и постановка задачи}

\begin{frame}{Актуальность проблемы}
\justifying
\begin{itemize}
    \justifying
    \item Экспоненциальный рост производительности вычислительных систем: от терафлопсов в начале XXI века до экзафлопсов в настоящий момент;
    \item Сопоставимый рост сложности моделей: от тысяч параметров в начале нулевых до миллиардов и сотен миллиардов в двадцатых, с прогнозируемым переходом к триллионам параметров;
    \item Особую остроту проблема приобретает при разработке больших языковых моделей (LLM), обучение которых требует значительных вычислительных, энергетических и финансовых ресурсов;
    \item Отсутствие строгой теоретической основы для предсказания поведения моделей при масштабировании делает процесс разработки экономически и энергетически неэффективным;
    \item Современные исследования преимущественно опираются на эмпирические корреляции, что приводит к необоснованным и противоречивым результатам.
\end{itemize}
\end{frame}

\begin{frame}{Проблемы существующих подходов}
\justifying
\begin{block}{Классические подходы (VC, PAC, Радемахер)}
\begin{itemize}
    \justifying
    \item Разработаны для моделей с ограниченным числом параметров;
    \item Дают слишком консервативные оценки для перепараметризованных нейронных сетей;
    \item Не учитывают специфику архитектур и особенности процесса оптимизации.
\end{itemize}
\end{block}

\begin{block}{Современные подходы}
\begin{itemize}
    \justifying
    \item Эмпирические законы масштабирования не имеют строгого теоретического обоснования;
    \item Оценка сложности данных сводится только к объему выборки;
    \item Методы снижения сложности не опираются на строгие теоретические оценки.
\end{itemize}
\end{block}

\begin{block}{Главная проблема}
    \justifying
    Отсутствует единый теоретический аппарат для описания сложности моделей и данных с формальными критериями соответствия между сложностью модели и сложностью данных.
\end{block}
\end{frame}

\begin{frame}{Объект и предмет исследования}
\justifying
\begin{block}{Объект исследования}
    \justifying
    Параметрические семейства функций, задаваемые суперпозициями линейных и нелинейных преобразований, а также конечные выборки данных, применяемые для оценки параметров указанных семейств в рамках задачи минимизации эмпирического риска.
\end{block}

\begin{block}{Предмет исследования}
    \justifying
    Разработка теоретического аппарата для оценки и анализа сложности моделей глубокого обучения и сложности данных, а также установление формальных критериев соответствия между сложностью модели и сложностью выборки, необходимой для ее обучения.
\end{block}
\end{frame}

\begin{frame}{Цель и задачи исследования}
\justifying
\begin{block}{Цель исследования}
\justifying
    Построение единого теоретического аппарата для оценки сложности моделей глубокого обучения и сложности данных, а также установление формальных критериев соответствия между сложностью модели и сложностью выборки, необходимой для ее обучения.
\end{block}

\begin{block}{Задачи исследования}
\begin{enumerate}
    \justifying
    \item Введение формальных определений мер сложности моделей и данных в рамках теории мер и установление критерия обучаемости модели на выборке;
    \item Получение теоретических оценок спектральных норм матриц Гессе для полносвязных, сверточных и трансформерных архитектур моделей глубокого обучения;
    \item Построение ландшафтной меры сложности модели на основе анализа матриц Гессе и установление ее связи с условной сложностью выборки;
    \item Построение методов оценки достаточного объема выборки на основе анализа стабильности функции потерь и близости апостериорных распределений параметров;
    \item Построение методов снижения сложности моделей глубокого обучения на основе анализа матриц Гессе и методов дистилляции знаний;
    \item Демонстрация практического применения построенного теоретического аппарата в задачах многозадачного обучения, нейровизуализации и детекции машинно-генерированного контента.
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Методы исследования}
\justifying
Для решения поставленных задач в диссертации используются:
\begin{itemize}
    \justifying
    \item Методы теории мер, линейной алгебры, матричного анализа (матричное дифференцирование и спектральный анализ матриц);
    \item Методы теории вероятностей и математической статистики (байесовский анализ, анализ сходимости случайных процессов);
    \item Методы теории оптимизации и анализа функций многих переменных;
    \item Методы статистической теории обучения.
\end{itemize}
\end{frame}

\begin{frame}{Основные решения и подходы}
\justifying
\begin{block}{Основная идея}
    \justifying
    Теоретический анализ нейросетевых моделей, опирающийся на анализ их матриц Гессе. Матрица Гессе функции потерь содержит информацию о локальной кривизне оптимизационного ландшафта.
\end{block}

\begin{block}{Подход}
\begin{itemize}
    \justifying
    \item Разработанный математический аппарат для анализа сложности на основе анализа ландшафта функции потерь через матрицы Гессе;
    \item Теоретические оценки матриц Гессе для различных архитектур моделей глубокого обучения;
    \item Анализ связи объема и сложности выборки со сложностью модели;
    \item Частные случаи общей теории сложности для практических оценок.
\end{itemize}
\end{block}
\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Общая схема подхода}
\begin{frame}{Место работы в теории сложности моделей}
\justifying
\small
\centering
\begin{tikzpicture}[node distance=0.9cm and 1.8cm, auto, scale=0.8, transform shape]
    % Контекст: существующие подходы (левый столбец, серый)
    \node[draw, rectangle, rounded corners, fill=gray!10, text width=2.2cm, text centered, minimum height=0.7cm, font=\tiny] (context1) at (0,3) {Классические:\\VC, PAC,\\Радемахер};
    \node[draw, rectangle, rounded corners, fill=gray!10, text width=2.2cm, text centered, minimum height=0.7cm, font=\tiny] (context2) at (0,1.5) {Современные:\\Эмпирический\\анализ Гессе};
    \node[draw, rectangle, rounded corners, fill=gray!10, text width=2.2cm, text centered, minimum height=0.7cm, font=\tiny] (context3) at (0,0) {Эмпирические\\scaling laws};
    
    % Проблемы (правый столбец, красноватый)
    \node[draw, rectangle, rounded corners, fill=red!10, text width=2.2cm, text centered, minimum height=0.7cm, font=\tiny] (problem1) at (8,3) {Не учитывают\\перепараметризацию};
    \node[draw, rectangle, rounded corners, fill=red!10, text width=2.2cm, text centered, minimum height=0.7cm, font=\tiny] (problem2) at (8,1.5) {Нет связи\\модели и данных};
    \node[draw, rectangle, rounded corners, fill=red!10, text width=2.2cm, text centered, minimum height=0.7cm, font=\tiny] (problem3) at (8,0) {Эмпирические,\\нет строгих\\оценок};
    
    % Вклад диссертации (центральная часть, яркие цвета)
    \node[draw, rectangle, rounded corners, fill=blue!30, text width=2.5cm, text centered, minimum height=1cm, font=\small] (complexity) at (4,2.5) {\textbf{Единый аппарат}\\\textbf{теории мер}};
    \node[draw, rectangle, rounded corners, fill=green!30, text width=2.2cm, text centered, minimum height=0.8cm, font=\scriptsize, below left=0.8cm and 0.2cm of complexity] (landscape) {\textbf{Ландшафтная}\\\textbf{мера}};
    \node[draw, rectangle, rounded corners, fill=orange!30, text width=2.2cm, text centered, minimum height=0.8cm, font=\scriptsize, below right=0.8cm and 0.2cm of complexity] (hessian) {\textbf{Оценки}\\\textbf{матриц Гессе}};
    \node[draw, rectangle, rounded corners, fill=red!25, text width=2.2cm, text centered, minimum height=0.8cm, font=\scriptsize, below=0.8cm of landscape] (sample) {\textbf{Достаточный}\\\textbf{объем выборки}};
    \node[draw, rectangle, rounded corners, fill=purple!25, text width=2.2cm, text centered, minimum height=0.8cm, font=\scriptsize, below=0.8cm of hessian] (reduction) {\textbf{Снижение}\\\textbf{сложности}};
    \node[draw, rectangle, rounded corners, fill=yellow!30, text width=2.2cm, text centered, minimum height=0.8cm, font=\scriptsize, below=0.8cm of sample, xshift=1.1cm] (applications) {\textbf{Практические}\\\textbf{применения}};
    
    % Стрелки от контекста к проблемам (пунктирные, серые)
    \draw[->, dashed, gray!50, thin] (context1) -- (problem1);
    \draw[->, dashed, gray!50, thin] (context2) -- (problem2);
    \draw[->, dashed, gray!50, thin] (context3) -- (problem3);
    
    % Стрелки от проблем к вкладу (пунктирные, красные)
    \draw[->, dashed, red!40, thick] (problem1) -- (complexity);
    \draw[->, dashed, red!40, thick] (problem2) -- (complexity);
    \draw[->, dashed, red!40, thick] (problem3) -- (hessian);
    
    % Стрелки внутри вклада диссертации (сплошные, черные)
    \draw[->, thick] (complexity) -- (landscape);
    \draw[->, thick] (complexity) -- (hessian);
    \draw[->, thick] (landscape) -- (hessian);
    \draw[->, thick] (landscape) -- (sample);
    \draw[->, thick] (hessian) -- (reduction);
    \draw[->, thick] (sample) -- (applications);
    \draw[->, thick] (reduction) -- (applications);
\end{tikzpicture}

\vspace{0.3cm}
\begin{block}{Вклад диссертации}
\justifying
\small
\textbf{Разработанный теоретический аппарат}, решающий проблемы существующих подходов:
\begin{itemize}
    \item Формализация в рамках теории мер: критерий обучаемости $\mu_f(f) \leq \mu_D(D)$
    \item Ландшафтная мера через матрицы Гессе: строгие оценки для конкретных архитектур
    \item Вычислимо осуществимые методы без прямого вычисления полных матриц Гессе
\end{itemize}
\end{block}
\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Общее введение теории ландшафтной меры}
\begin{frame}{Мера сложности выборки}
\justifying

\begin{rusdefinition}
    \justifying
    Генеральной совокупностью данных~$\Gamma$ назовем произвольное множество объектов, которые исследуются в рамках той либо иной задаче.
\end{rusdefinition}

\begin{rusdefinition}
    \justifying
    Генеральную совокупность~$\Gamma$ назовем однородной, если все объекты генеральной совокупности порождаются из одного распределения. В противном случае выборку назовем~$k$-родной.
\end{rusdefinition}


Рассмотрим кольцо выборок: $\mathfrak{D} = \{D_\Gamma^i\}, \quad D_\Gamma^i \subset \Gamma.$

\begin{rusdefinition}
    \justifying
    Мерой сложности выборки назовем отображение~$\mu_D,$ такое, что: 
    \[
        \mu_D(D_i) : \mathfrak{D}_\Gamma \to \mathbb{R}_+,
    \]
    удовлетворяющее свойству:
    \begin{align}
        \mu_D(D_i\cup D_j) \leq \mu_D(D_i)+\mu_D(D_j),
    \end{align}
    где равенство достигается при условии~$D_i\cap D_j=\emptyset.$
\end{rusdefinition}

\end{frame}

\begin{frame}{Мера сложности модели и критерий обучаемости}
\justifying
Задано множество моделей: $\mathfrak{F} = \left\{f_i\right\}.$

\begin{rusdefinition}
    \justifying
    Мерой сложности модели~$f$ назовем отображение~$\mu_f(f_i)$:
    \[
        \mu_f(f_i) : \mathfrak{F} \to \mathbb{R}_+.
    \]
\end{rusdefinition}

\begin{block}{Ключевой критерий обучаемости}
\justifying
Модель $f\in\mathfrak{F}$ \textbf{обучаема} на выборке $D\in\mathfrak{D}$, если 
\[
    \mu_f(f)\leq \mu_D(D).
\]
\end{block}

\begin{rustheorem}
    \justifying
    Если для исходной выборки~$D\in\mathfrak{D}$ выполняется условие~$\mu_f(f) \leq \mu_D(D)$, тогда для новой выборки~$D'\in\mathfrak{D}$ модель дообучаема при условии:
    \[
        \mu_f(f) - \mu_D(D) \leq \mu_D(D').
    \]
\end{rustheorem}

\begin{rusremark}
    \justifying
    Критерий обучаемости формализует необходимое условие предотвращения переобучения и устанавливает связь между сложностью модели и сложностью данных.
\end{rusremark}

\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Достаточный объем выборки, как мера сложности данных}
\begin{frame}{Условная сложность и однородность выборки}
\justifying
\begin{rusdefinition}
    \justifying
    Условной сложностью выборки~$D$ относительно заданной параметрической модели~$f$ назовем отображение~$\mu_D(D|f) : \mathfrak{D} \to \mathbb{R}_+,$ определяющиеся выражением:
    $$\mu_D(D|f) = \inf \{ \mu_D(D') : D' \subseteq D, \quad \mu_f(f) \leq \mu_D(D') \}.$$
\end{rusdefinition}

\begin{rusdefinition}
    \justifying
    Однородную генеральную совокупность~$\Gamma_C$ назовем простой, если она состоит из объектов одинаковой сложности~$C.$
\end{rusdefinition}

\begin{rustheorem}
    \justifying
    Для простой генеральной совокупности, мера сложности любой выборки $D \subset \Gamma$ равна ее объему:
    \[
        \mu_D(D) = C\cdot|D|.
    \]
\end{rustheorem}
\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Ландшафтная мера}
\begin{frame}{От общей теории к конкретной мере сложности}
\justifying
\begin{block}{Необходимость конкретизации}
Общие определения мер сложности требуют конкретизации для практического применения к моделям глубокого обучения.
\end{block}

\begin{block}{Ландшафтная мера}
Анализ изменения ландшафта функции потерь при вариации выборки через матрицы Гессе.
\end{block}

\vspace{0.3cm}
\textbf{Основная идея:} Если добавление нового объекта данных существенно изменяет ландшафт оптимизации, то модель недостаточно обучена на текущей выборке.
\end{frame}

\begin{frame}{Изменения ландшафта при вариации выборки}
\justifying
Пусть задана выборка из~$\Gamma_C$:
\begin{equation}
    D = \left\{ (\mathbf{x}_i, \mathbf{y}_i) \right\}, \quad i = 1, \ldots, m, \quad \mathbf{x} \in \mathcal{X}, \ \mathbf{y} \in \mathcal{Y}, \quad D\subset \Gamma_C.
\end{equation}

Пусть, используется метод минимизации эмпирического риска:
\[
    \hat{\boldsymbol{\theta}} = \arg\min_{\boldsymbol{\theta}} \mathcal{L}_m(\boldsymbol{\theta}) = \arg\min_{\boldsymbol{\theta}}\frac{1}{m} \sum\limits_{i=1}^{m} \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), \mathbf{y}_i) \approx \arg\min_{\boldsymbol{\theta}}\mathsf{E}_{(\mathbf{x}, \mathbf{y})} \left[ \ell(f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y}) \right].
\]

Рассмотрим изменение функции~$\mathcal{L}_m(\boldsymbol{\theta})$:
\[
    \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) = \frac{1}{k+1} \left(\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \mathcal{L}_{k}(\boldsymbol{\theta})\right),
\]
где~$\mathcal{L}_{k+1}(\boldsymbol{\theta})$ и~$\mathcal{L}_k(\boldsymbol{\theta})$ вычислены на выборках размера~$k+1$ и $k,$ различающимися ровно в~$1$ объекте.
\end{frame}

\begin{frame}{Предположение устойчивости локального минимума}
\justifying
\begin{rusassumption}
    \justifying
    Пусть $\boldsymbol{\theta}^*$ является локальным минимумом обеих эмпирических функций потерь $\mathcal{L}_{k}(\boldsymbol{\theta})$ и $\mathcal{L}_{k+1}(\boldsymbol{\theta})$, т.е: $\nabla \mathcal{L}_{k}(\boldsymbol{\theta}^*) = \nabla \mathcal{L}_{k+1}(\boldsymbol{\theta}^*) = \mathbf{0}.$
\end{rusassumption}

В рамках предположения: $\mathcal{L}_{k}(\boldsymbol{\theta}) \approx \mathcal{L}_{k}(\boldsymbol{\theta}^*) + \frac{1}{2} (\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top \mathbf{H}^{(k)}(\boldsymbol{\theta}^*) (\boldsymbol{\theta} - \boldsymbol{\theta}^*).$

\begin{align*}
    \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) &= \frac{1}{k+1} \left( \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right) +\\
    &\quad+ \frac{1}{k+1} (\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top \left( \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right) (\boldsymbol{\theta} - \boldsymbol{\theta}^*).
\end{align*}

\begin{rusremark}
    Первое слагаемое может быть ограничено константой,
поскольку сама функция потерь принимает ограниченные значения. Основной интерес представляет выражение с матрицами Гессе.
\end{rusremark}

\end{frame}

\begin{frame}{Ландшафтная мера сложности}
\justifying

\begin{rusdefinition}
    \justifying
    \textbf{Ландшафтной мерой} сложности параметрической функции~$f$ назовем:
    \[
        \mu_f(f|D) = \mathsf{E}_{\mathbf{x}_i\in D}\left\| \mathbf{H}_{i}(\boldsymbol{\theta}^*) -  \mathsf{E}_{\mathbf{x}_i\in D}\mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
    \]
\end{rusdefinition}

\begin{block}{Интерпретация}
Ландшафтная мера характеризует \textbf{вариативность} матриц Гессе по объектам выборки. Большая вариативность означает, что различные объекты по-разному влияют на ландшафт оптимизации.
\end{block}

\begin{block}{Связь с изменением функции потерь}
\[
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{M_l}{k+1}+ \frac{\mu_f(f|D)}{k+1}\left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2.
\]
\end{block}

\begin{rusremark}
    \justifying
    Ландшафтная мера связывает сложность модели со сложностью данных через анализ матриц Гессе, что позволяет получить вычислимо осуществимые оценки без прямого вычисления полных матриц Гессе.
\end{rusremark}
\end{frame}

\begin{frame}{Экспериментальная проверка ландшафтной меры}
\justifying
\begin{block}{Экспериментальная постановка}
Исследование зависимости между средним абсолютным различием значений функции потерь $|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta})|$ и доступным размером выборки $k$ для различных архитектур.
\end{block}

\begin{figure}[h!t]\center
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-1/landscape-hessian/figs/mnist_hidden_size}
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-1/landscape-hessian/figs/mnist_num_layers}
    \caption{Зависимость разности функций потерь от размера выборки для полносвязных сетей на MNIST. Слева: зависимость от размера скрытого слоя при $L=5$. Справа: зависимость от количества слоев при $h=16$.}
\end{figure}

\begin{block}{Результаты}
\begin{itemize}
    \item Эксперименты на наборах данных MNIST, FashionMNIST, CIFAR10 подтверждают теорему о сходимости;
    \item Наблюдается монотонное убывание разности потерь с ростом размера выборки;
    \item Скорость сходимости зависит от параметров архитектуры, что согласуется с теоретическими предсказаниями.
\end{itemize}
\end{block}
\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Ландшафтные меры для некоторых семейств}
\begin{frame}{Ландшафтная мера полносвязной сетевой модели}
\justifying
\begin{rustheorem}
    Пусть параметры $\boldsymbol{\theta}$ выбраны так, что $\left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \leqslant R^2$ для некоторого $R > 0$. Если существует неотрицательная константа $M_{\ell}$ такая, что $\left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| \leqslant M_{\ell}$ для всех объектов $i = 1, \ldots, m$ в наборе данных справедливо:
    \begin{align*}
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{2}{k+1}\left( M_{\ell} + \left( L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \frac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1} \right) R^2 \right),
    \end{align*}
    причем:~$
        \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \to 0~\text{при}~k \to \infty.$
\end{rustheorem}

\begin{ruscorollary}
    Ландшафтная мера сложности параметрической функции~$f$ полносвязной нейросетевой модели глубокого обучения имеет асимптотику:
    \[
        \mu_f(f|D) \propto L(hM)^{2L},
    \]
    где~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{ruscorollary}
\end{frame}

\begin{frame}{Экспериментальная проверка для полносвязных сетей}
\justifying
\begin{block}{Экспериментальная постановка}
Исследование зависимости разности потерь от параметров архитектуры: количества слоев $L$ и размера скрытого слоя $h$.
\end{block}

\begin{figure}[h!t]\center
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-1/landscape-hessian/figs/mnist_num_layers}
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-1/landscape-hessian/figs/mnist_hidden_size}
    \caption{Слева: зависимость от количества слоев при $h=16$. Справа: зависимость от размера скрытого слоя при $L=5$.}
\end{figure}

\begin{block}{Результаты}
\begin{itemize}
    \item Эксперименты подтверждают теоретическую оценку: $\mu_f(f|D) \propto L(hM)^{2L}$;
    \item Увеличение глубины $L$ приводит к экспоненциальному росту разности потерь;
    \item Увеличение ширины $h$ приводит к полиномиальному росту разности потерь.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Ландшафтная мера сверточной сетевой модели}
\justifying
\begin{rustheorem}
    Пусть параметры $\boldsymbol{\theta}$ находятся в $R$-окрестности оптимума: $\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\| \leqslant R.$
    Также функция потерь ограничена некоторой константой~$\exists \; W_l > 0: \; \forall i\; |\ell_i| \leqslant W_l.$
    Пусть все объекты в наборе данных ограничены~$\exists W_x\; \forall i \; \|{x_i}\| \leqslant W_x.$
    
    Тогда справедливо:
    \begin{align*}
        \big|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_{k}(\boldsymbol{\theta})\big| &\leqslant \frac{2}{k+1}W_{\ell} +\\
        & + \frac{2}{k+1}R^2\sqrt{2}C^4k^4m^2n^2W_{x}^2(L+1)(C^2k^2w^2mn)^L.
    \end{align*}
\end{rustheorem}

\begin{ruscorollary}
    Ландшафтная мера сложности параметрической функции~$f$ сверточной нейросетевой модели глубокого обучения имеет асимптотику, зависящую от параметров свертки и глубины сети.
\end{ruscorollary}
\end{frame}

\begin{frame}{Экспериментальная проверка для сверточных сетей}
\justifying
\begin{block}{Экспериментальная постановка}
Исследование влияния параметров сверточной сети (количество слоев $L$, размер ядра $k$, количество каналов $C$, позиция пулинга) на ландшафтную меру.
\end{block}

\begin{figure}[h!t]\center
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-1/matricized-networks/mnist_change_channels}
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_maxpool_pos}
    \caption{Слева: зависимость от количества каналов. Справа: влияние позиции пулинга на разность потерь.}
\end{figure}

\begin{block}{Результаты}
\begin{itemize}
    \item Увеличение количества каналов $C$ приводит к монотонному росту разности потерь;
    \item Более раннее применение пулинга снижает разность потерь, что согласуется с теоретическими оценками;
    \item Результаты подтверждают теоретические оценки для сверточных архитектур.
\end{itemize}
\end{block}
\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Оценка матриц Гессе для нейросетевых моделей}
\begin{frame}{От ландшафтной меры к оценкам матриц Гессе}
\justifying
\begin{block}{Постановка задачи}
Для практического применения ландшафтной меры необходимо получить оценки матриц Гессе для различных архитектур нейронных сетей.
\end{block}

\begin{block}{Подход}
\begin{itemize}
    \item Декомпозиция матрицы Гессе на G- и H-компоненты
    \item Факторизация для вычислимо осуществимых оценок
    \item Получение асимптотических оценок для основных архитектур
\end{itemize}
\end{block}

\vspace{0.3cm}
\textbf{Результат:} Строгие теоретические оценки для полносвязных, сверточных и трансформерных сетей.
\end{frame}

\begin{frame}{Декомпозиция матрицы Гессе на G и H компоненты}
\justifying

Матрица Гессе для функции $\mathcal{L}(\boldsymbol{\theta})$ от $n$ параметров:
\[
    \mathbf{H}(\mathcal{L})_{ij} = \frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j} = \sum_{i=1}^{l}\frac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \theta_i \partial \theta_j},
\]

Используя цепное правило, матрица Гессе представима в виде двух слагаемых:
\[
    \mathbf{H}_{i}(\boldsymbol{\theta}) = \underbrace{\nabla_{\boldsymbol{\theta}} \mathbf{z}_i \dfrac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_{i}^2} \nabla_{\boldsymbol{\theta}} \mathbf{z}_i^{\text{T}} }_{\text{G-компонента}} + \underbrace{\sum\limits_{k=1}^{K} \dfrac{\partial \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial z_{ik}} \nabla^2_{\boldsymbol{\theta}} z_{ik}}_{\text{H-компонента}}.
\]

\begin{ruscorollary}
    \justifying
    Эмперически установлено, что H-компонента близка к нулю, а особый интерес представляет G-компонента:
\[
    \mathbf{H}_{i}(\boldsymbol{\theta}) \approx \nabla_{\boldsymbol{\theta}} \mathbf{z}_i \dfrac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_{i}^2} \nabla_{\boldsymbol{\theta}} \mathbf{z}_i^{\text{T}}.
\]
\end{ruscorollary}
\end{frame}

\begin{frame}{Оценка матрицы Гессе для полносвязной сети}
\begin{rustheorem}
    \justifying
    Для~$L$-слойной полносвязной нейронную сеть с функцией активации ReLU, применяемую для решения задачи классификации на~$K$ классов.
    Пусть: $\| \mathbf{W}^{(p)} \|_2 \leqslant M_{\mathbf{W}}, \| \mathbf{x}_i \|_2 \leqslant M_{\mathbf{x}},$
    для всех слоев~$p = 1, \ldots, L$ в сети и для всех объектов~$i = 1, \ldots, m.$
    Тогда для любого объекта~$i = 1, \ldots, m$ выполняется следующее неравенство: $\left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \dfrac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1}.$
\end{rustheorem}

\begin{rustheorem}
    \justifying
    Пусть все параметры модели ограничены некоторой константой~$M > 0$, то есть для всех $i, j = 1, \ldots, h$ и для всех слоев $p = 1, \ldots, L$ выполняется условие~$|w_{ij}^{(p)}| \leqslant M,$ тогда при выполнении условий предыдущей теоремы справедлива оценка: $\left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 (hM)^{2L} + \sqrt{2} \dfrac{(hM)^2 ((hM)^{2L} - 1)}{(hM)^2 - 1}.$
\end{rustheorem}

\begin{ruscorollary}
    \justifying
    Частным случаем является оценка на матрицу Гессе для однослойной сети~$L=1$ вида: $\left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant \sqrt{2} M_{\mathbf{W}}^{2} \left( M_{\mathbf{x}}^2 + 1 \right).$
\end{ruscorollary}
\end{frame}

\begin{frame}{Факторизация матричных моделей глубокого обучения}
\justifying
Пусть $f_{\boldsymbol{\theta}}(\mathbf{x})$ является суперпозицией линейных операторов:
\begin{equation*}
    f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)}\dots \boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)}\mathbf{x},
\end{equation*}
где~$\boldsymbol{\theta} = \mathrm{col}(\mathbf{W}^{(L+1)}, \dots, \mathbf{W}^{(1)}).$

\begin{rustheorem}
    \justifying
    Пусть функция нейронной сети~$f_{\boldsymbol{\theta}}(\mathbf{x})$ представима в виде суперпозиции линейных операторов, тогда матрица Гессе функции потерь относительно параметров модели представляется в факторизованной форме:
    $\mathbf{H}(\boldsymbol{\theta}) \approx \mathbf{Q}^{T}\mathbf{F}^{T}\mathbf{A}\mathbf{F}\mathbf{Q}$, описывающие G-компоненту матрицы Гессе.
\end{rustheorem}

\begin{rustheorem}
    \justifying
    Пусть функция нейронной сети~$f_{\boldsymbol{\theta}}(\mathbf{x})$ представима в виде суперпозиции линейных операторов.
    Пусть для всех $p$ выполняется: $\left\|\mathbf{Q}^{(p)}\right\| \leqslant q, \left\|\mathbf{T}^{(p)}\right\|^2 \leqslant w_{\mathbf{T}}^2.$
    Тогда справедлива оценка: $\left\|\mathbf{H}_O\right\| \leqslant \sqrt{2}q^2\left\|\mathbf{x}\right\|^2(L+1)w_{\mathbf{T}}^{2L}.$
\end{rustheorem}

\end{frame}

\begin{frame}{Сверточные модели глубокого обучения}
\justifying
\begin{rustheorem}
    \justifying
    Пусть задана функция нейронной сети~$f_{\boldsymbol{\theta}}x = C_{\mathbf{W}^{(L+1)}} \circ \sigma \circ \dots \circ \sigma \circ C_{\mathbf{W}^{(1)}},$
    где~$C_{\mathbf{W}^{(i)}}$~--- одномерная свертка с ядром~$\mathbf{W}^{(i)}$. Пусть заданы следующие верхние оценки на параметры:~$C_l \leqslant C, k_i \leqslant k, d_i \leqslant d_1:=d, |\mathbf{W}^{(p)}_{i,j,k}|^2 \leqslant w^2.$
    Тогда норма матрицы Гессе имеет следующую верхнюю оценку: $\left\|\mathbf{H}\right\| \leqslant \sqrt{2}\left\|x\right\|^2d^2(L+1)(C^2w^2kd)^L.$
\end{rustheorem}

\begin{rustheorem}
    \justifying
    Пусть задана функция нейронной сети~$f_{\boldsymbol{\theta}}\mathbf{x} = C_{\mathbf{W}^{(L+1)}} \circ \dots \circ C_{\mathbf{W}^{(1)}},$
    где $C_{\mathbf{W}^{(l)}}$~--- двумерная свертка с ядром $\mathbf{W}^{(i)}$.
    Пусть заданы следующие верхние оценки на параметры:~$C_l \leqslant C, k_i \leqslant k, m_i \leqslant m_1:=m, n_i \leqslant n_1:=n, |\mathbf{W}^{(p)}_{i,j,k}|^2 \leqslant w^2.$
    Тогда норма матрицы Гессе имеет следующую верхнюю оценку:~$\left\|\mathbf{H}_{O}\right\| \leqslant \sqrt{2}\left\|\mathbf{x}\right\|^2C^4k^4m^2n^2(L+1)(C^2k^2w^2mn)^{L}.$
\end{rustheorem}

\begin{rusremark}
    \justifying
    Полученные оценки имеют недостаток связанный с тем, что они не учитывают уменьшение размеров после сверточных операций и зависят только от верхних границ параметров.
\end{rusremark}
\end{frame}

\begin{frame}{Снижение сложности в сверточных моделях}
\justifying
\begin{rustheorem}
    \justifying
    Для сверточных сетей с операциями пулинга (MaxPool2D или AvgPool2D) норма матрицы Гессе уменьшается за счет множителя:
    \[
        \left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}
    \]
    где $k_{\mathrm{pool}}$~--- размер ядра пулинга, $L$~--- глубина сети, $l$~--- позиция слоя пулинга.
\end{rustheorem}

\begin{block}{Практический вывод}
Операции пулинга снижают сложность модели и действуют как \textbf{механизм регуляризации} в глубоких сверточных нейронных сетях.
\end{block}

\begin{rusremark}
    \justifying
    Это объясняет, почему пулинг-слои улучшают обобщающую способность моделей: они уменьшают сложность оптимизационного ландшафта.
\end{rusremark}

\end{frame}

\begin{frame}{Сводка оценок матриц Гессе}
\justifying
\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|}
\hline
\textbf{Архитектура} & \textbf{Асимптотика нормы $\|\mathbf{H}\|_2$} \\
\hline
Полносвязная сеть & $L(hM)^{2L}$ \\
\hline
1D свертка & $L(C^2w^2kd)^L$ \\
\hline
2D свертка & $L(C^2k^2w^2mn)^L$ \\
\hline
С пулингом & $L \cdot \left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} \cdot (\text{базовая оценка})$ \\
\hline
\end{tabular}
\end{table}

\begin{block}{Ключевые наблюдения}
\begin{itemize}
    \item Экспоненциальный рост с глубиной сети $L$
    \item Полиномиальный рост с размером слоев
    \item Пулинг снижает сложность
    \item Оценки позволяют предсказывать сложность без обучения модели
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Экспериментальная проверка для трансформеров}
\justifying
\begin{block}{Экспериментальная постановка}
Исследование зависимости разности функций потерь от количества обучающих примеров для трансформерной модели на CIFAR-100.
\end{block}

\begin{figure}[h!t]\center
    \includegraphics[width=0.6\textwidth]{thesis/figures/chapter-1/transformer/loss_landscape_convergence}
    \caption{Зависимость $|\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w})|$ от $k$ в двойном логарифмическом масштабе.}
\end{figure}

\begin{block}{Результаты}
\begin{itemize}
    \item Эксперименты подтверждают теорему о сходимости для трансформерных моделей;
    \item Наблюдается стабилизация ландшафта функции потерь с ростом объема данных;
    \item Результаты демонстрируют применимость теоретических оценок к трансформерным архитектурам.
\end{itemize}
\end{block}
\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Достаточный объем выборки}
\begin{frame}{Оценка достаточного объема выборки}
\justifying
\begin{block}{Постановка задачи}
Теоретические оценки сложности требуют практических методов определения достаточного объема выборки для конкретных моделей и задач.
\end{block}

\begin{block}{Подходы}
\begin{enumerate}
    \item Сэмплирование эмпирической функции ошибки (D- и M-достаточность)
    \item Анализ близости апостериорных распределений (KL- и S-достаточность)
    \item Классические статистические и байесовские методы
\end{enumerate}
\end{block}

\vspace{0.3cm}
\textbf{Результат:} Практические алгоритмы оценки достаточного объема выборки с теоретическими гарантиями сходимости.
\end{frame}

\begin{frame}{Достаточный объем выборки}
\justifying

\begin{rustheorem}
    \justifying
    Для простой генеральной совокупности, мера сложности любой выборки $D \subset \Gamma$ равна ее объему:
    \[
        \mu_D(D) = C\cdot|D|.
    \]
\end{rustheorem}

\begin{rusdefinition}
    \justifying
    Размер выборки $m^*$ называется \textit{достаточным} согласно критерию $T$, если $T$ выполняется для всех $k \geqslant m^*$.
\end{rusdefinition}

\end{frame}

\begin{frame}{Cэмлирования эмпирической функции ошибки}
\justifying
\begin{rusdefinition}
    \justifying
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ назовем D-достаточным, если для всех~$k\geqslant m^*$ выполняется условие:~$D(k) = \mathbb{D}_{\hat{\mathbf{w}}_{k}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \leqslant \varepsilon.$
\end{rusdefinition}

\begin{rusdefinition}
    \justifying
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ назовем M-достаточным, если для всех~$k\geqslant m^*$ выполняется условие:~$M(k) = \left| \mathbb{E}_{\hat{\mathbf{w}}_{k+1}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E}_{\hat{\mathbf{w}}_{k}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \right| \leqslant \varepsilon.$
\end{rusdefinition}

\begin{rustheorem}
    \justifying
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и~$\|\bm{\Sigma}_{k+1} - \bm{\Sigma}_k\|_{F}\to 0$ при~$k\to \infty$. 
    Тогда в модели линейной регрессии определение M-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$M(k)\leqslant\varepsilon$.
\end{rustheorem}

\begin{ruscorollary}
    \justifying
    Пусть~$\|\mathbf{m}_k - \mathbf{w}\|_2\to 0$ и~$\|\mathbf{\Sigma}_k - \left[k\mathcal{I}(\mathbf{w})\right]^{-1}\|_{F}\to 0$ при~$k \to \infty$.
    Тогда в модели линейной регрессии определение M-достаточного размера выборки корректно.
\end{ruscorollary}

\end{frame}

\begin{frame}{KL-близость апостериорных распределений}
\justifying
\begin{rusdefinition}
    \justifying
    Подвыборки~$\mathfrak{D}^1$ и~$\mathfrak{D}^2$ назовем близкими, если~$\mathcal{I}_2$ может быть получено из~$\mathcal{I}_1$ путем удаления, замены или добавления одного элемента, то есть
    \[
        \left| \mathcal{I}_1 \triangle \mathcal{I}_2 \right| = \left| \left( \mathcal{I}_1 \setminus \mathcal{I}_2 \right) \cup \left( \mathcal{I}_2 \setminus \mathcal{I}_1 \right) \right| = 1.
    \]
\end{rusdefinition}

\begin{rusdefinition}
    \justifying
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ называется KL-достаточным, если для всех~$k\geqslant m^*$
    \[
        KL(k) = D_{KL}(p_k \| p_{k+1}) = \int p_k(\mathbf{w}) \log{\frac{p_k(\mathbf{w})}{p_{k+1}(\mathbf{w})}} d\mathbf{w} \leqslant \varepsilon.
    \]
\end{rusdefinition}

\begin{rustheorem}
    \justifying
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и~$\|\bm{\Sigma}_{k+1} - \bm{\Sigma}_k\|_{F}\to 0$ при~$k\to \infty$.
    Тогда в модели с нормальным апостериорным распределением параметров определение KL-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$KL(k)\leqslant\varepsilon$.
\end{rustheorem}

\end{frame}

\begin{frame}{S-близость апостериорных распределений}
\justifying
\begin{rusdefinition}
    \justifying
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ называется S-достаточным, если для всех~$k\geqslant m^*$
    \[
        S(k) = \text{s-score}(p_k, p_{k+1}) \geqslant 1-\varepsilon.
    \]
\end{rusdefinition}

\begin{rustheorem}
    \justifying
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2\to 0$ при~$k \to \infty$.
    Тогда в модели с нормальным апостериорным распределением параметров определение S-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$S(k)\geqslant 1-\varepsilon$.
\end{rustheorem}

\end{frame}

\begin{frame}{KL-достаточность против S-достаточности}
\justifying
\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{thesis/figures/chapter-3/posterior-distribution/sufficient-vs-threshold}
\end{figure}

Зависимость достаточного размера выборки от порогового параметра.
Для S-достаточного размера выборки требуются более низкие значения порога.
Таким образом, он оказывается более требовательным к этому значению.

\end{frame}

\begin{frame}{Экспериментальная проверка KL- и S-достаточности}
\justifying
\begin{block}{Экспериментальная постановка}
Исследование сходимости предложенных функций $KL(k)$ и $S(k)$ к предельным значениям на синтетических данных и Liver Disorders.
\end{block}

\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{thesis/figures/chapter-3/posterior-distribution/dependence_on_available_sample_set}
    \caption{Зависимость $m^*$ от $m$ для классических методов и методов KL- и S-достаточности на Boston Housing.}
\end{figure}

\begin{block}{Результаты}
\begin{itemize}
    \item Критерий KL-достаточности является наиболее консервативным;
    \item S-достаточность указывает на минимальные размеры выборки;
    \item Наблюдается сходимость всех методов, подтверждающая теоретические результаты.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Экспериментальная проверка методов оценки достаточного объема}
\justifying
\begin{block}{Экспериментальная постановка}
Сравнение различных методов определения достаточного размера выборки на наборах данных Boston Housing, Diabetes, Forest Fires, Servo и NBA.
\end{block}

\begin{figure}[h!t]\center
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-3/statical/cross}
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-3/statical/kl}\\
    \includegraphics[width=0.85\textwidth]{thesis/figures/chapter-3/statical/graphs}
    \caption{Зависимость статистических значений методов от размера подвыборки и зависимость $m^*$ от $m$ для Boston Housing.}
\end{figure}

\begin{block}{Результаты}
\begin{itemize}
    \item Все представленные функции монотонны и асимптотически стремятся к константе;
    \item Методы демонстрируют сходимость с низкой дисперсией оценок;
    \item Результаты подтверждают теоретические оценки сходимости для всех методов.
\end{itemize}
\end{block}
\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Снижение сложности моделей}
\begin{frame}{Методы снижения сложности моделей}
\justifying
\begin{block}{Постановка задачи}
Снижение сложности моделей без потери качества для эффективного использования вычислительных ресурсов.
\end{block}

\begin{block}{Методы}
\begin{enumerate}
    \item \textbf{Прореживание (pruning)} на основе анализа матриц Гессе и ковариационных матриц градиентов
    \item \textbf{Дистилляция знаний} для передачи знаний от сложных моделей к простым
    \item \textbf{Мультидоменная дистилляция} для работы с несколькими доменами данных
    \item \textbf{Анти-дистилляция} для наращивания сложности модели с использованием информации из простых моделей
\end{enumerate}
\end{block}

\vspace{0.3cm}
\textbf{Основа:} Все методы опираются на разработанный теоретический аппарат оценки сложности.
\end{frame}

\begin{frame}{Дистилляция на многих генеральных совокупностях}
\justifying

\begin{figure}[h!t]\center
    {\includegraphics[width=0.4 \textwidth]{thesis/figures/chapter-4/multidomain-distillation/Distillation_only}}
    {\includegraphics[width=0.4 \textwidth]{thesis/figures/chapter-4/multidomain-distillation/Distillation_DA}}
\end{figure}

\begin{rusdefinition}
    \justifying
    Генеральная совокупность~$B$ называется близкой к генеральной совокупности~$A$, если существует инъективное отображение $\varphi: A \rightarrow B$.
\end{rusdefinition}

Заданы выборки из двух близких генеральных совокупностей отображением~$\varphi$:
\[
    \mathfrak{D}_{A}=\{(\mathbf{x_i},\mathbf{y_i})\}_{i=1}^n,
\mathbf{x_i} \in \mathbb{X}_{A},
\mathbf{y_i} \in \mathbb{Y}, \qquad \mathfrak{D}_{B}=\{(\mathbf{x_i},\mathbf{y_i})\}_{i=1}^m, \mathbf {x_i} \in \mathbb{X}_{B}, \mathbf{y_i} \in \mathbb{Y}.
\]
Оптимизационная задача для дистилляции:
\begin{align*}
    \mathcal{L}(\mathbf{w,X,Y,f,\varphi}) =&
    -\lambda\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{ R}\mathbb{I}[y_{i}=r]\log{g^{r}(\mathbf{x}_{i},\mathbf{w})} -\\
    &-(1-\lambda)\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{R}(f\circ \varphi)^{r}(\mathbf{x }_{i})\log{g^{r}(\mathbf{x}_{i},\mathbf{w})}.
\end{align*}

\end{frame}

\begin{frame}{Экспериментальная проверка методов снижения сложности}
\justifying
\begin{block}{Метод Белсли для анализа мультиколлинеарности}
Анализ ковариационной матрицы градиентов функции ошибки для выявления релевантных параметров.
\end{block}

\begin{figure}[h!t]\center
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-4/belsli/Cov}
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-4/belsli/BelslyImage}
    \caption{Слева: матрица ковариации параметров. Справа: дисперсионные доли, демонстрирующие вклад каждого признака.}
\end{figure}

\begin{block}{Результаты}
\begin{itemize}
    \item Метод позволяет идентифицировать параметры с высокой мультиколлинеарностью;
    \item Удаление нерелевантных параметров снижает сложность модели без существенной потери качества;
    \item Эксперименты подтверждают эффективность подхода на различных архитектурах.
\end{itemize}
\end{block}
\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Применение теоретических оценок в прикладных задачах}
\begin{frame}{Практические применения разработанного аппарата}
\justifying
\begin{block}{Области применения}
\begin{enumerate}
    \item \textbf{Многозадачное обучение}~--- оценка радемахеровской сложности, анализ LoRA-адаптеров
    \item \textbf{Нейровизуализация}~--- декодирование фМРТ-изображений, снижение размерности данных
    \item \textbf{Детекция машинно-генерированного контента}~--- оценка качества данных, анализ устойчивости детекторов
\end{enumerate}
\end{block}

\begin{block}{Ключевые результаты}
\begin{itemize}
    \item Теоретические гарантии для многозадачного обучения
    \item Эффективные методы предобработки данных
    \item Метрики оценки качества данных на основе топологической статистики
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Радамахеровская сложность многозадачного обучения}
\justifying
\begin{rustheorem}
    \justifying
    Пусть $S_t=\{x_i\}_{i=1}^n$~--- выборка фиксированной целевой задачи $t$ с $\|x_i\|_2 \le R$.
    Пусть $\phi(\cdot;w)$~--- энкодер, и рассмотрим линейные головы
    $f_{w,h}(x)=h^\top \phi(x;w)$ с $\|h\|_2 \le B_{\mathrm{head}}$.
    Предположим:
    \begin{enumerate}
        \item Ограничение на признаки: для всех $x,w$ 
        $\|\phi(x;w)\|_2 \le L\,\|w\|\,\|x\|_2$.
        \item Энкодер STL удовлетворяет $\|w_{\mathrm{enc}}\|\le B_{\mathrm{enc}}$,
        общий энкодер MTL удовлетворяет $\|w_{\mathrm{shared}}\|\le B_{\mathrm{shared}}$.
        \item Многозадачное масштабирование: $B_{\mathrm{shared}} \le B_{\mathrm{enc}}/\sqrt{T}$.
    \end{enumerate}

    Тогда справедлива оценка сложности: $\widehat{\mathfrak R}_n\big(\mathcal{F}_{\mathrm{MTL}}^{(t)};S_t\big) \le \frac{1}{\sqrt{T}}\widehat{\mathfrak R}_n\big(\mathcal{F}_{\mathrm{STL}}^{(t)};S_t\big).$
\end{rustheorem}

\begin{rusremark}
    \justifying
    В постановке Бакстера, количество задач $T$ само по себе способствует оценке общего индуктивного смещения: с ростом $T$ сложность данных на задачу уменьшается пропорционально $1/T$.
    В отличие от этого, наш анализ сохраняет размер выборки целевой задачи $n$ фиксированным и сравнивает STL и MTL на одном и том же $n$, поэтому улучшение проявляется как множитель $1/\sqrt{T}$ в сложности Радемахера на задачу.
\end{rusremark}

\end{frame}

\begin{frame}{Экспериментальная проверка: LoRA-адаптеры в многозадачном обучении}
\justifying
\begin{block}{Экспериментальная постановка}
Исследование зависимости метрики качества от ранга $r$ адаптеров LoRA для модели DeBERTa-v3-base на задаче детекции машинно-генерированного текста.
\end{block}

\begin{figure}[h!t]\center
    \includegraphics[width=0.7\textwidth]{thesis/figures/chapter-5/rademacher/lora_rank_vs_f1-1}
    \caption{Зависимость метрики $F_1$ от ранга $r$ адаптеров LoRA.}
\end{figure}

\begin{block}{Результаты}
\begin{itemize}
    \item Наилучшая производительность достигается при $r=8$, что подтверждает эффективность низкоранговой параметризации;
    \item Снижение отдачи при увеличении ранга согласуется с теоретическими оценками;
    \item Результаты подтверждают теорему о статистической состоятельности LoRA-адаптеров.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Экспериментальная проверка: декодирование фМРТ-изображений}
\justifying
\begin{block}{Экспериментальная постановка}
Применение методов снижения сложности данных для декодирования фМРТ-изображений из видеопоследовательностей с использованием предварительного сжатия данных.
\end{block}

\begin{figure}[h!t]\center
    \includegraphics[width=0.8\textwidth]{thesis/figures/chapter-5/fmri/scheme}
    \caption{Схема метода декодирования фМРТ-изображений с использованием ResNet152 и линейной регрессии.}
\end{figure}

\begin{block}{Результаты}
\begin{itemize}
    \item Предварительное сжатие данных обеспечивает существенное сокращение времени обучения;
    \item Качество реконструкции сохраняется на высоком уровне;
    \item Метод демонстрирует эффективность применения теории сложности данных в практических задачах.
\end{itemize}
\end{block}
\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Выносится на защиту}
\begin{frame}{Основные положения, выносимые на защиту}
\justifying
\begin{block}{1. Единый теоретический аппарат}
Единый теоретический аппарат оценки сложности моделей глубокого обучения и сложности данных на основе теории мер и анализа ландшафта оптимизационной задачи, включающий:
\begin{itemize}
    \item Формальные определения мер сложности выборки и модели
    \item Критерий обучаемости модели на выборке: $\mu_f(f) \leq \mu_D(D)$
\end{itemize}
\end{block}

\begin{block}{2. Ландшафтная мера}
Ландшафтная мера сложности модели через спектральные свойства матриц Гессе функции потерь и ее связь с условной сложностью выборки.
\end{block}
\end{frame}

\begin{frame}{Основные положения, выносимые на защиту (продолжение)}
\justifying
\begin{block}{3. Оценки матриц Гессе}
Теоретические оценки спектральных норм матриц Гессе для основных архитектур моделей глубокого обучения:
\begin{itemize}
    \item Полносвязные сети: $\|\mathbf{H}\|_2 \propto L(hM)^{2L}$
    \item Сверточные сети: оценки для 1D и 2D сверток
    \item Унифицированный подход на основе матричной факторизации
\end{itemize}
\end{block}

\begin{block}{4. Методы оценки достаточного объема выборки}
Методы оценки достаточного объема выборки на основе:
\begin{itemize}
    \item Анализа стабильности функции потерь (D- и M-достаточность)
    \item Близости апостериорных распределений (KL- и S-достаточность)
    \item Теоретические оценки сходимости для всех методов
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Основные положения, выносимые на защиту (завершение)}
\justifying
\begin{block}{5. Методы снижения сложности}
Методы снижения сложности моделей глубокого обучения на основе:
\begin{itemize}
    \item Анализа матриц Гессе и градиентов для прореживания
    \item Дистилляции и анти-дистилляции для передачи знаний
    \item Мультидоменной дистилляции для работы с несколькими доменами
\end{itemize}
\end{block}

\begin{block}{Практическая значимость}
Все методы экспериментально подтверждены на реальных задачах:
\begin{itemize}
    \item Компьютерное зрение
    \item Обработка естественного языка
    \item Классификация и регрессия
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Основные публикации}
\justifying
	\begin{enumerate}
	\justifying
        \item Киселев Н. С., Грабовой А. В. Раскрытие Гессиана: ключ к плавной сходимости поверхности функции потерь // Доклады Российской академии наук. Математика, информатика, процессы управления.~--- 2024.~--- Т. 520, № 2.~--- С. 57–70.
        \item Грабовой А. В., Стрижов В. В. Вероятностная интерпретация задачи дистилляции // Автоматика и телемеханика.~--- 2022.~--- № 1.~--- С. 150–168. 
        \item Numerical methods of sufficient sample size estimation for generalised linear models / A. V. Grabovoy, T. T. Gadaev, A. P. Motrenko, V. V. Strijov // Lobachevskii Journal of Mathematics.~--- 2022.~--- Vol. 43, no. 9.~--- P. 2453–2462.
        \item Грабовой А. В., Стрижов В. В. Байесовская дистилляция моделей глубокого обучения // Автоматика и телемеханика.~--- 2021.~--- № 11.~--- С. 16–29.
        \item Грабовой\;А.\;В., Бахтеев\;О.\;Ю., Стрижов\;В.\;В. Определение релевантности параметров нейросети // Информатика и ее применения.~--- 2019. 
	\end{enumerate}
\end{frame}

\begin{frame}{Публикации по теме исследований}
\justifying
	\begin{itemize}
	\justifying
        \item Всего публикаций: \textbf{90}.
        \item Публикаций по теме диссертации: \textbf{43}.
        \item Публикации в списке ВАК: \textbf{24}.
        \item Журнальные публикации в WoS, Scopus: \textbf{18}.
        \item Публикации из RSCI: \textbf{17}.
        \item Конференции в WoS, Scopus: \textbf{20}.
        \item Тезисы докладов и другие публикации: \textbf{32}.
	\end{itemize}
\end{frame}

\end{document}