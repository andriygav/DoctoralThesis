\documentclass[10pt,pdf,hyperref={unicode}]{beamer}

\mode<presentation>
{
\usetheme{boxes}
\beamertemplatenavigationsymbolsempty

\setbeamertemplate{footline}[page number]
\setbeamersize{text margin left=1em, text margin right=1em}
}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{bm}
\usepackage{multirow}
\usepackage{ragged2e}
\usepackage{indentfirst}
\usepackage{multicol}
\usepackage{subfig}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{comment}
\usepackage{multicol}
\usepackage{color}
\definecolor{python-green}{RGB}{55,126,33}

\usepackage[all]{xy}

\usepackage{tikz}
\usetikzlibrary{positioning,arrows}

\usepackage{caption}
\captionsetup{skip=0pt,belowskip=0pt}

\newtheorem{rustheorem}{Теорема}
\newtheorem{rusdefinition}{Определение}
\newtheorem{rusassumption}{Предположение}
\newtheorem{rusremark}{Замечание}
\newtheorem{ruscorollary}{Следствие}


% colors
\definecolor{darkgreen}{rgb}{0.0, 0.2, 0.13}
\definecolor{darkcyan}{rgb}{0.0, 0.55, 0.55}

\AtBeginEnvironment{figure}{\setcounter{subfigure}{0}}%

%----------------------------------------------------------------------------------------------------------

\title[О сложности моделей и данных]{О сложности моделей и данных \\в современных моделях глубокого обучения}
\author{А.В. Грабовой}

\institute[]{Диссертация на соискание ученой степени\\
доктора физико-математических наук\\~\\1.2.1~--- Искусственный интеллект и машинное обучение\\~\\Научный консультант: профессор РАН К.\,В. Воронцов}
\date[2026]{\small 2026\,г.}

%---------------------------------------------------------------------------------------------------------
\begin{document}

\begin{frame}
\titlepage
\end{frame}

%----------------------------------------------------------------------------------------------------------
\section{Актуальность, задачи и методы исследований}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Актуальность}
\justifying
\begin{itemize}
    \justifying
    \item Экспоненциальный рост сложности моделей: от тысяч до сотен миллиардов параметров, с прогнозируемым переходом к триллионам.
    \item Особую остроту проблема приобретает при разработке больших языковых моделей (LLM), требующих огромных вычислительных, энергетических и финансовых ресурсов.
    \item Отсутствие строгой теоретической основы для предсказания поведения моделей при масштабировании делает процесс разработки экономически и энергетически неэффективным.
    \item Современные исследования преимущественно опираются на эмпирические корреляции, что приводит к необоснованным и противоречивым результатам.
    \item Классические подходы (VC, PAC, Радемахер) дают слишком консервативные оценки для перепараметризованных нейронных сетей и не учитывают специфику современных архитектур.
\end{itemize}

\vspace{0.3cm}
\textit{Ключевая проблема:} Отсутствует единый теоретический аппарат для описания сложности моделей и данных с формальными критериями их соответствия.
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Цель и задачи исследования}
\justifying
\textbf{Цель:} Построение единого теоретического аппарата для оценки сложности моделей глубокого обучения и сложности данных, а также установление формальных критериев соответствия между сложностью модели и сложностью выборки, необходимой для ее обучения.

\vspace{0.3cm}
\textbf{Задачи:}
\begin{enumerate}
    \justifying
    \item Введение формальных определений мер сложности моделей и данных в рамках теории мер и установление критерия обучаемости модели на выборке.
    \item Получение теоретических оценок спектральных норм матриц Гессе для полносвязных, сверточных и трансформерных архитектур.
    \item Построение ландшафтной меры сложности модели на основе анализа матриц Гессе и установление ее связи с условной сложностью выборки.
    \item Построение методов оценки достаточного объема выборки на основе анализа стабильности функции потерь и близости апостериорных распределений параметров.
    \item Построение методов снижения сложности моделей глубокого обучения на основе анализа матриц Гессе и методов дистилляции знаний.
    \item Демонстрация практического применения построенного теоретического аппарата в прикладных задачах.
\end{enumerate}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Методы исследования}
\justifying
Для решения поставленных задач в диссертации используются:

\begin{itemize}
    \justifying
    \item Методы теории мер, линейной алгебры, матричного анализа (матричное дифференцирование и спектральный анализ матриц);
    \item Методы теории вероятностей и математической статистики (байесовский анализ, анализ сходимости случайных процессов);
    \item Методы теории оптимизации и анализа функций многих переменных;
    \item Методы статистической теории обучения.
\end{itemize}

\vspace{0.3cm}
\textbf{Ключевой инструмент:} анализ матриц Гессе функции потерь, содержащих информацию о локальной кривизне оптимизационного ландшафта.
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Научная новизна}
\justifying
\begin{itemize}
    \justifying
    \item Впервые введены формальные определения меры сложности выборки $\mu_D(D)$ и меры сложности модели $\mu_f(f)$ в рамках теории мер, установлен критерий обучаемости $\mu_f(f) \le \mu_D(D)$.
    \item Введена ландшафтная мера сложности модели $\mu_f(f|D) = \mathbb{E}\|\mathbf{H}_i - \mathbb{E}\mathbf{H}_i\|$, определяемая через спектральные свойства матриц Гессе.
    \item Впервые получены строгие теоретические оценки спектральных норм матриц Гессе для полносвязных, сверточных и трансформерных архитектур (включая явные выражения для компонентов трансформера).
    \item Разработаны новые методы оценки достаточного объема выборки (D, M, KL, S-достаточность) с доказательством сходимости.
    \item Предложены оригинальные методы снижения сложности: прореживание на основе ковариации градиентов, мультидоменная дистилляция и анти-дистилляция.
    \item Получены теоремы о состоятельности LoRA-адаптеров и снижении сложности Радемахера в многозадачном обучении.
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Теоретическая и практическая значимость}
\justifying
\textbf{Теоретическая значимость:}
\begin{itemize}
    \justifying
    \item Разработан единый формальный язык для описания сложности моделей и данных, расширяющий классическую теорию статистического обучения на случай перепараметризованных нейронных сетей.
    \item Установлены прямые связи между архитектурными параметрами моделей (глубина, ширина, тип слоев) и сложностью оптимизационного ландшафта через спектр Гессе.
    \item Полученные оценки матриц Гессе открывают новые направления в теории оптимизации и анализа обобщающей способности глубоких сетей.
\end{itemize}

\vspace{0.2cm}
\textbf{Практическая значимость:}
\begin{itemize}
    \justifying
    \item Методы оценки достаточного объема выборки позволяют экономить вычислительные ресурсы при планировании экспериментов.
    \item Методы снижения сложности обеспечивают сжатие моделей без потери качества для развертывания в ресурсно-ограниченных средах.
    \item Разработанный аппарат применен к реальным задачам: многозадачное обучение, декодирование фМРТ-изображений, детекция машинно-генерированного контента.
    \item Экспериментально подтверждена эффективность всех предложенных методов на общедоступных наборах данных.
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\section{Диссертация в контексте теории сложности}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Диссертация в контексте теории сложности}
    \centering
    \begin{enumerate}
        \item Эволюция теории сложности
        \item Ограничения существующих подходов
        \item Место диссертационной работы
    \end{enumerate}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Эволюция теории сложности обучения}
\justifying

\textbf{Статистическая теория сложности (1970–2010)}
\begin{itemize}
    \justifying
    \item Вапник, Червоненкис, 1974~--- введение VC-размерности как меры емкости класса; равномерная сходимость частот к вероятностям.
    \item Valiant, 1984~--- формализация понятия обучаемости; гарантии с заданной точностью и надежностью.
    \item Воронцов, 2010~--- развитие VC-теории; учет структуры данных и локализации алгоритмов для снижения консервативности оценок.
\end{itemize}

\textbf{Аппроксимационная теория (1989–2015)}
\begin{itemize}
    \justifying
    \item Cybenko, 1989~--- однослойная сеть с сигмоидой может аппроксимировать любую непрерывную функцию.
    \item Hastad, 1987; Bengio, 2007; Cohen, 2015~-- экспоненциальный рост выразительной способности с увеличением числа слоев.
\end{itemize}

\textbf{Современные эмпирические подходы (2017–2022)}
\begin{itemize}
    \justifying
    \item Sagun, 2017; Keskar, 2016~--- эмпирическое исследование спектра Гессе; связь ``плоских'' минимумов с обобщением.
    \item Kaplan, 2020; Hoffmann, 2022~--- степенные зависимости качества от числа параметров и объема данных; оптимальные соотношения для вычислительных бюджетов.
\end{itemize}

\textit{\small{Направления развивались независимо. Отсутствует единая теория, связывающая сложность модели и сложность данных.}}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Ограничения существующих подходов}
\justifying

\textbf{Статистическая теория сложности}
\begin{itemize}
    \justifying
    \item Ориентация на worst-case анализ $\Rightarrow$ оценки существенно завышены для реальных данных.
    \item Не объясняет феномен перепараметризации: модели с высокой VC-размерностью успешно обобщают данный.
    \item Сложность модели и данных рассматриваются изолированно, без формального критерия соответствия.
\end{itemize}

\textbf{Аппроксимационная теория}
\begin{itemize}
    \justifying
    \item Характеризует принципиальную возможность представления функций, но не дает количественных оценок сложности обучения.
    \item Не учитывает влияние конечности выборки и процедуры оптимизации.
\end{itemize}

\textbf{Эмпирический анализ ландшафта}
\begin{itemize}
    \justifying
    \item Результаты носят описательный характер, отсутствуют строгие аналитические оценки для конкретных архитектур.
\end{itemize}

\textbf{Законы масштабирования}
\begin{itemize}
    \justifying
    \item Эмпирические зависимости не имеют строгого теоретического обоснования.
    \item Не объясняют механизмы, определяющие оптимальные соотношения между параметрами и данными.
\end{itemize}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Диссертация заполняет следующие пробелы}
\justifying

\begin{itemize}
    \justifying
    \item \textbf{Отсутствие единого формального языка.} Введение мер сложности $\mu_D(D)$ и $\mu_f(f)$; критерий обучаемости $\mu_f(f) \le \mu_D(D)$.
    \item \textbf{Нет аналитических оценок сложности.} Ландшафтная мера $\mu_f(f|D) = \mathbb{E}\| \mathbf{H}_i - \mathbb{E}\mathbf{H}_i \|$; строгие оценки $\|\mathbf{H}\|_2$ для полносвязных, сверточных и трансформерных сетей.
    \item \textbf{Невозможность определить необходимый объем данных.} Методы D, M, KL, S-достаточности с доказанной сходимостью (теоремы 28–31).
    \item \textbf{Отсутствие теоретически обоснованных методов.} Прореживание на основе ковариации градиентов; мультидоменная дистилляция; анти-дистилляция.
    \item \textbf{Эмпирический характер законов масштабирования.} Теоремы о состоятельности LoRA; снижение сложности Радемахера в многозадачном обучении в $1/\sqrt{T}$ раз.
\end{itemize}

\vspace{0.2cm}
\small{Работа создает единый теоретический аппарат, связывающий классическую теорию сложности с современными архитектурами и эмпирическими наблюдениями.}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Диссертация в контексте теории сложности}
\centering
\begin{tikzpicture}[node distance=1.2cm and 2.2cm, auto, scale=1.0, transform shape]
    % Контекст: существующие подходы (левый столбец)
    \node[draw, rectangle, rounded corners, fill=gray!10, text width=2.2cm, text centered, minimum height=0.8cm, font=\footnotesize] (context1) at (0,3.5) {Классические:\\VC, PAC,\\Радемахер};
    \node[draw, rectangle, rounded corners, fill=gray!10, text width=2.2cm, text centered, minimum height=0.8cm, font=\footnotesize] (context2) at (0,1.8) {Современные:\\эмпирический\\анализ Гессе};
    \node[draw, rectangle, rounded corners, fill=gray!10, text width=2.2cm, text centered, minimum height=0.8cm, font=\footnotesize] (context3) at (0,0.1) {Эмпирические\\scaling laws};
    
    % Проблемы (правый столбец)
    \node[draw, rectangle, rounded corners, fill=red!10, text width=2.2cm, text centered, minimum height=0.8cm, font=\footnotesize] (problem1) at (9,3.5) {Не учитывают\\перепараметризацию};
    \node[draw, rectangle, rounded corners, fill=red!10, text width=2.2cm, text centered, minimum height=0.8cm, font=\footnotesize] (problem2) at (9,1.8) {Нет связи\\модели и данных};
    \node[draw, rectangle, rounded corners, fill=red!10, text width=2.2cm, text centered, minimum height=0.8cm, font=\footnotesize] (problem3) at (9,0.1) {Эмпирические,\\нет строгих оценок};
    
    % Вклад диссертации (центральная часть)
    \node[draw, rectangle, rounded corners, fill=blue!30, text width=2.8cm, text centered, minimum height=1.2cm, font=\small] (complexity) at (4.5,3.0) {\textbf{Единый аппарат}\\теории мер};
    \node[draw, rectangle, rounded corners, fill=green!30, text width=2.4cm, text centered, minimum height=0.9cm, font=\footnotesize, below left=0.9cm and 0.2cm of complexity] (landscape) {\textbf{Ландшафтная}\\мера};
    \node[draw, rectangle, rounded corners, fill=orange!30, text width=2.4cm, text centered, minimum height=0.9cm, font=\footnotesize, below right=0.9cm and 0.2cm of complexity] (hessian) {\textbf{Оценки}\\матриц Гессе};
    \node[draw, rectangle, rounded corners, fill=red!25, text width=2.4cm, text centered, minimum height=0.9cm, font=\footnotesize, below=0.9cm of landscape] (sample) {\textbf{Достаточный}\\объем выборки};
    \node[draw, rectangle, rounded corners, fill=purple!25, text width=2.4cm, text centered, minimum height=0.9cm, font=\footnotesize, below=0.9cm of hessian] (reduction) {\textbf{Снижение}\\сложности};
    \node[draw, rectangle, rounded corners, fill=yellow!30, text width=2.4cm, text centered, minimum height=0.9cm, font=\footnotesize, below=0.9cm of sample, xshift=1.2cm] (applications) {\textbf{Практические}\\применения};
    
    % Стрелки от контекста к проблемам
    \draw[->, dashed, gray!50, thin] (context1) -- (problem1);
    \draw[->, dashed, gray!50, thin] (context2) -- (problem2);
    \draw[->, dashed, gray!50, thin] (context3) -- (problem3);
    
    % Стрелки от проблем к вкладу
    \draw[->, dashed, red!40, thick] (problem1) -- (complexity);
    \draw[->, dashed, red!40, thick] (problem2) -- (complexity);
    \draw[->, dashed, red!40, thick] (problem3) -- (hessian);
    
    % Внутренние связи вклада
    \draw[->, thick] (complexity) -- (landscape);
    \draw[->, thick] (complexity) -- (hessian);
    \draw[->, thick] (landscape) -- (hessian);
    \draw[->, thick] (landscape) -- (sample);
    \draw[->, thick] (hessian) -- (reduction);
    \draw[->, thick] (sample) -- (applications);
    \draw[->, thick] (reduction) -- (applications);
\end{tikzpicture}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\section{Общее введение теории ландшафтной меры}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Общее введение теории ландшафтной меры}
    \centering
    \begin{enumerate}
        \item Меры сложности выборки и модели
        \item Критерий обучаемости
        \item Условная сложность и достаточный объем данных
        \item Ландшафтная мера: определение и интерпретация
    \end{enumerate}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Мера сложности выборки}
\justifying

\begin{rusdefinition}
    \justifying
    Генеральной совокупностью данных~$\Gamma$ назовем произвольное множество объектов, которые исследуются в рамках той либо иной задаче.
\end{rusdefinition}

\begin{rusdefinition}
    \justifying
    Генеральную совокупность~$\Gamma$ назовем однородной, если все объекты генеральной совокупности порождаются из одного распределения. В противном случае выборку назовем~$k$-родной.
\end{rusdefinition}


Рассмотрим кольцо выборок: $\mathfrak{D} = \{D_\Gamma^i\}, \quad D_\Gamma^i \subset \Gamma.$

\begin{rusdefinition}
    \justifying
    Мерой сложности выборки назовем отображение~$\mu_D,$ такое, что: 
    \[
        \mu_D(D_i) : \mathfrak{D}_\Gamma \to \mathbb{R}_+,
    \]
    удовлетворяющее свойству:
    \[
        \mu_D(D_i\cup D_j) \leq \mu_D(D_i)+\mu_D(D_j),
    \]
    где равенство достигается при условии~$D_i\cap D_j=\emptyset.$
\end{rusdefinition}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Мера сложности модели}
\justifying

\begin{rusdefinition}
\justifying
Для параметрического семейства функций $\mathfrak{F} = \{f_i\}$ вводится отображение
\[
\mu_f(f_i) : \mathfrak{F} \to \mathbb{R}_+,
\]
называемое \textbf{мерой сложности модели}.
\end{rusdefinition}

\vspace{0.3cm}
\textbf{Примеры мер:}
\begin{itemize}
    \item Число параметров модели.
    \item Норма вектора весов $\|\mathbf{w}\|$.
    \item Эффективная емкость, оцениваемая через след гессиана.
    \item В дальнейшем будет введена \textit{ландшафтная мера} $\mu_f(f|D)$, учитывающая данные.
\end{itemize}

\vspace{0.3cm}
\textit{Примечание:} Конкретный вид меры зависит от задачи и модели; важно лишь, что она задает числовую характеристику сложности.
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Критерий обучаемости и дообучение}
\justifying

\begin{rusdefinition}
\justifying
Модель $f$ \textbf{обучаема} на выборке $D$, если
\[
\mu_f(f) \;\le\; \mu_D(D).
\]
Интуиция: сложность модели не должна превышать сложность данных, иначе неизбежно переобучение.
\end{rusdefinition}

\begin{rustheorem}[Грабовой, 2025]
\justifying
Если модель уже обучаема на исходной выборке $D$, то для добавления новой выборки $D'$ достаточно выполнения условия
\[
\mu_f(f) - \mu_D(D) \;\le\; \mu_D(D').
\]
\end{rustheorem}
Это означает, что остаточная емкость модели должна покрываться сложностью новых данных.

\textit{Примечание:} Критерии дают формальную основу для планирования экспериментов и оценки необходимости сбора дополнительных данных.

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Условная сложность выборки}
\justifying

Мера $\mu_D(D)$ оценивает абсолютную сложность данных, но для конкретной модели важна \textit{относительная} сложность.

\begin{rusdefinition}
    \justifying
    Условная сложность выборки $D$ относительно модели $f$:
    \[
        \mu_D(D|f) = \inf\{\mu_D(D'): D'\subseteq D,\; \mu_f(f) \le \mu_D(D')\}.
    \]
\end{rusdefinition}
Это минимальная сложность подвыборки, достаточная для выполнения критерия обучаемости.

\begin{block}{Свойства}
    \begin{itemize}
        \item $\mu_D(D|f) \le \mu_D(D)$
        \item Если $\mu_f(f) \le \mu_D(D)$, то $\mu_D(D|f) \le \mu_D(D)$
        \item Зависит как от данных, так и от архитектуры модели
    \end{itemize}
\end{block}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Простая генеральная совокупность}
\justifying

\begin{rusdefinition}
    \justifying
    Однородную генеральную совокупность $\Gamma_C$ назовем \textbf{простой}, если все ее объекты имеют одинаковую сложность $C$:
    \[
        \forall \gamma\in\Gamma_C\; \mu_D(\{\gamma\}) = C.
    \]
\end{rusdefinition}

\begin{rustheorem}[Грабовой, 2025]
    Для простой совокупности мера сложности любой конечной выборки пропорциональна ее размеру:
    \[
        \mu_D(D) = C\cdot|D|.
    \]
\end{rustheorem}

\textit{Примечание:} Условная сложность превращается в минимальный размер подвыборки:
\[
    \mu_D(D|f) = C\cdot m^*, \quad 
    m^* = \min\{k: \mu_f(f) \le Ck\}.
\]
$m^*$~--- минимальное количество объектов для обучения модели $f$.
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{От общей теории к конкретной мере сложности}
\justifying

\begin{figure}[h!t]\center
    \includegraphics[width=0.9\textwidth]{thesis/figures/chapter-1/losses_difference.pdf}  
\end{figure}

\vspace{0.3cm}
Если добавление нового объекта данных существенно изменяет ландшафт оптимизации, то модель недостаточно обучена на текущей выборке.

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Изменения ландшафта при вариации выборки}
\justifying

Пусть задана выборка из~$\Gamma_C$:
\[
    D = \left\{ (\mathbf{x}_i, \mathbf{y}_i) \right\}, \quad i = 1, \ldots, m, \quad \mathbf{x} \in \mathcal{X}, \ \mathbf{y} \in \mathcal{Y}, \quad D\subset \Gamma_C.
\]

Рассмотрим эмпирический риск на выборках размера $k$ и $k+1$:
\[
\mathcal{L}_k(\boldsymbol{\theta}) = \frac{1}{k}\sum_{i=1}^k \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), \mathbf{y}_i), \quad 
\mathcal{L}_{k+1}(\boldsymbol{\theta}) = \frac{1}{k+1}\sum_{i=1}^{k+1} \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), \mathbf{y}_i).
\]

Их разность:
\[
\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) = 
\frac{1}{k+1}\Bigl(\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \mathcal{L}_k(\boldsymbol{\theta})\Bigr).
\]

\vspace{0.2cm}
\textit{Примечание:} при $k\to\infty$ эта разность стремится к нулю. 
Скорость убывания определяется тем, насколько новый объект ``не похож'' на уже имеющиеся.
Далее эта скорость оценивается через спектральные свойства матрицы Гессе.
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Предположение о стационарности точки минимума}
\justifying

\begin{rusassumption}
\justifying
Пусть $\boldsymbol{\theta}^*$~--- локальный минимум обеих функций потерь $\mathcal{L}_k(\boldsymbol{\theta})$ и $\mathcal{L}_{k+1}(\boldsymbol{\theta})$, т.е.
\[
    \nabla\mathcal{L}_k(\boldsymbol{\theta}^*) = \nabla\mathcal{L}_{k+1}(\boldsymbol{\theta}^*) = \mathbf{0}.
\]
\end{rusassumption}

В окрестности минимума функции аппроксимируются квадратично:
\[
\mathcal{L}_k(\boldsymbol{\theta}) \approx \mathcal{L}_k(\boldsymbol{\theta}^*) + \frac{1}{2}(\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top \mathbf{H}^{(k)}(\boldsymbol{\theta}^*)(\boldsymbol{\theta} - \boldsymbol{\theta}^*).
\]

Тогда разность потерь принимает вид:
\[
\begin{aligned}
\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) &\approx 
\frac{1}{k+1}\Bigl(\ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \mathcal{L}_k(\boldsymbol{\theta}^*)\Bigr) \\
&\quad + \frac{1}{k+1}(\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top 
\Bigl(\mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k}\sum_{i=1}^{k}\mathbf{H}_i(\boldsymbol{\theta}^*)\Bigr)(\boldsymbol{\theta} - \boldsymbol{\theta}^*).
\end{aligned}
\]

\begin{rusremark}
\justifying
Первое слагаемое ограничено константой, не зависящей от $k$. Второе содержит разность матриц Гессе~--- именно она определяет скорость сходимости ландшафта и используется в основе ландшафтной меры.
\end{rusremark}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Ландшафтная мера сложности модели}
\justifying

\begin{rusdefinition}
\justifying
    \textbf{Ландшафтной мерой} сложности параметрической функции $f$ называется
    \[
        \mu_f(f|D) = \mathbb{E}_{\mathbf{x}_i\in D}\left\| \mathbf{H}_i(\boldsymbol{\theta}^*) - \mathbb{E}_{\mathbf{x}_i\in D}\mathbf{H}_i(\boldsymbol{\theta}^*) \right\|_2,
    \]
    где $\mathbf{H}_i(\boldsymbol{\theta}^*)$~--- матрица Гессе для $i$-го объекта в точке минимума $\boldsymbol{\theta}^*$.
\end{rusdefinition}

Ландшафтная мера характеризует \textbf{вариативность} гессианов по объектам выборки:
\begin{itemize}
    \item Малая вариативность~--- объекты одинаково влияют на ландшафт, модель насытилась данными.
    \item Большая вариативность~--- разные объекты по-разному искривляют ландшафт, данных недостаточно.
\end{itemize}

\begin{block}{Связь с изменением функции потерь}
\[
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{M_l}{k+1}+ \frac{\mu_f(f|D)}{k+1}\left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2.
\]
\end{block}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\section{Ландшафтные меры для некоторых семейств}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Ландшафтные меры для разных архитектур}
    \centering

    \begin{enumerate}
        \item Полносвязные нейронные сети
        \item Сверточные нейронные сети
        \item Экспериментальное подтверждение
    \end{enumerate}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Ландшафтная мера полносвязной сети}
\justifying

\begin{rustheorem}[Грабовой, 2025]
Для $L$-слойной полносвязной сети с ReLU верна оценка скорости сходимости ландшафта:
\[
\bigl|\mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta})\bigr| \le 
\frac{C_1}{k+1} + \frac{C_2 R^2}{k+1}\cdot L(hM)^{2L},
\]
где $C_1, C_2$~--- константы, $R$~--- радиус окрестности оптимума, $h$~--- ширина слоя, $M$~--- граница параметров.
\end{rustheorem}

\begin{ruscorollary}
Ландшафтная мера сложности полносвязной модели: $\mu_f(f|D) \propto L(hM)^{2L}.$
\end{ruscorollary}

\textbf{Интерпретация:} сложность ландшафта растет \textbf{экспоненциально} с глубиной $L$ и \textbf{полиномиально} с шириной $h$. Это объясняет, почему глубокие сети требуют больше данных для стабилизации.

\begin{rusremark}
    Для однослойной сети ($L=1$) имеем $\mu_f \propto h^2$~--- квадратичная зависимость от ширины.
\end{rusremark}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Эксперимент по полносвязной сети}
\justifying

\textbf{Цель:} проверить, что разность потерь $|\mathcal{L}_{k+1}-\mathcal{L}_k|$ убывает с ростом выборки $k$, а скорость убывания зависит от архитектуры (как предсказывает ландшафтная мера $\mu_f(f|D)$).

\begin{figure}[h!t]\center
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-1/landscape-hessian/figs/mnist_hidden_size}
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-1/landscape-hessian/figs/mnist_num_layers}
\end{figure}

\begin{itemize}
    \item Монотонное убывание подтверждено для MNIST, FashionMNIST, CIFAR10.
    \item Увеличение глубины $L$ дает более медленную сходимость (больше $\mu_f$).
    \item Увеличение ширины $h$ слабее влияет на сходимость~--- согласуется с асимптотикой $\mu_f\propto L(hM)^{2L}$.
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Ландшафтная мера сверточной сети}
\justifying

\begin{rustheorem}[Грабовой, 2025]
$\mu_f(f|D) \;\propto\; L\,(C^2 w^2 k d)^L,$
где $C$~--- число каналов, $k$~--- размер ядра, $d$~--- длина последовательности.
\end{rustheorem}
\begin{rustheorem}[Грабовой, 2025]
$
\mu_f(f|D) \;\propto\; C^2 k^2 L\,(C^2 k^2 w^2 m n)^L,
$
где $m\times n$~--- пространственные размеры входа.
\end{rustheorem}

\begin{rustheorem}[Грабовой, 2025]
Операция пулинга (MaxPool/AvgPool) снижает сложность мультипликативно на
$
\left(\frac{1}{k_{\text{pool}}^2}\right)^{L-l+2},
$
где $l$~--- позиция слоя пулинга.
\end{rustheorem}

\textbf{Интерпретация:} сложность ландшафта растет \textbf{экспоненциально} с глубиной $L$ и \textbf{полиномиально} с числом каналов, размером ядра и разрешением. Пулинг действует как эффективный регуляризатор, уменьшая ландшафтную меру.

\begin{rusremark}
    Полученные оценки объясняют, почему увеличение числа каналов или глубины требует больше данных, а пулинг облегчает обучение.
\end{rusremark}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Экспериментальная проверка для сверточных сетей}
\justifying

\textbf{Цель:} исследовать влияние параметров сверточной архитектуры (количество каналов $C$, позиция пулинга) на разность потерь $|\mathcal{L}_{k+1}-\mathcal{L}_k|$ и сопоставить с теоретической асимптотикой $\mu_f(f|D)$.

\begin{figure}[h!t]\center
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-1/matricized-networks/mnist_change_channels}
    \includegraphics[width=0.45\textwidth]{thesis/figures/chapter-1/matricized-networks/cifar10_change_maxpool_pos}
\end{figure}

\begin{itemize}
    \item Увеличение числа каналов~$C$ монотонно увеличивает разность потерь, что согласуется с полиномиальным ростом сложности.
    \item Раннее применение пулинга снижает разность потерь~--- подтверждает теоретическую оценку о регуляризующем эффекте пулинга.
    \item Эксперименты на MNIST и CIFAR10 подтверждают теоретические асимптотики для сверточных архитектур.
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\section{Оценка матриц Гессе для нейросетевых моделей}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Оценка матриц Гессе для нейросетевых моделей}
    \centering

    \begin{enumerate}
        \item Декомпозиция на G и H компоненты
        \item Полносвязные сети: оценки нормы
        \item Матричная факторизация
        \item Сверточные сети и влияние пулинга
        \item Сводная таблица асимптотических оценок
    \end{enumerate}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Декомпозиция матрицы Гессе на G и H компоненты}
\justifying

Матрица Гессе для функции $\mathcal{L}(\boldsymbol{\theta})$ от $n$ параметров:
\[
    \mathbf{H}(\mathcal{L})_{ij} = \frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j} = \sum_{i=1}^{l}\frac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \theta_i \partial \theta_j},
\]

Используя цепное правило, матрица Гессе представима в виде двух слагаемых:
\[
    \mathbf{H}_{i}(\boldsymbol{\theta}) = \underbrace{\nabla_{\boldsymbol{\theta}} \mathbf{z}_i \dfrac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_{i}^2} \nabla_{\boldsymbol{\theta}} \mathbf{z}_i^{\text{T}} }_{\text{G-компонента}} + \underbrace{\sum\limits_{k=1}^{K} \dfrac{\partial \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial z_{ik}} \nabla^2_{\boldsymbol{\theta}} z_{ik}}_{\text{H-компонента}}.
\]

\begin{ruscorollary}
    \justifying
    Эмперически установлено, что H-компонента близка к нулю, а особый интерес представляет G-компонента:
\[
    \mathbf{H}_{i}(\boldsymbol{\theta}) \approx \nabla_{\boldsymbol{\theta}} \mathbf{z}_i \dfrac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_{i}^2} \nabla_{\boldsymbol{\theta}} \mathbf{z}_i^{\text{T}}.
\]
\end{ruscorollary}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Оценка матрицы Гессе для полносвязной сети}
\begin{rustheorem}[Грабовой-Киселев, 2024]
    \justifying
    Для~$L$-слойной полносвязной нейронную сеть с функцией активации ReLU, применяемую для решения задачи классификации на~$K$ классов.
    Пусть: $\| \mathbf{W}^{(p)} \|_2 \leqslant M_{\mathbf{W}}, \| \mathbf{x}_i \|_2 \leqslant M_{\mathbf{x}},$
    для всех слоев~$p = 1, \ldots, L$ в сети и для всех объектов~$i = 1, \ldots, m.$
    Тогда для любого объекта~$i = 1, \ldots, m$ выполняется следующее неравенство: $\left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \dfrac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1}.$
\end{rustheorem}

\begin{rustheorem}[Грабовой-Киселев, 2024]
    \justifying
    Пусть все параметры модели ограничены некоторой константой~$M > 0$, то есть для всех $i, j = 1, \ldots, h$ и для всех слоев $p = 1, \ldots, L$ выполняется условие~$|w_{ij}^{(p)}| \leqslant M,$ тогда при выполнении условий предыдущей теоремы справедлива оценка: $\left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 (hM)^{2L} + \sqrt{2} \dfrac{(hM)^2 ((hM)^{2L} - 1)}{(hM)^2 - 1}.$
\end{rustheorem}

\begin{ruscorollary}
    \justifying
    Частным случаем является оценка на матрицу Гессе для однослойной сети~$L=1$ вида: $\left\| \mathbf{H}_i(\boldsymbol{\theta}) \right\|_2 \leqslant \sqrt{2} M_{\mathbf{W}}^{2} \left( M_{\mathbf{x}}^2 + 1 \right).$
\end{ruscorollary}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Факторизация матричных моделей глубокого обучения}
\justifying
Пусть $f_{\boldsymbol{\theta}}(\mathbf{x})$ является суперпозицией линейных операторов:
\begin{equation*}
    f_{\boldsymbol{\theta}}(\mathbf{x}) = \mathbf{T}^{(L+1)}\boldsymbol{\Lambda}^{(L)}\dots \boldsymbol{\Lambda}^{(1)}\mathbf{T}^{(1)}\mathbf{x},
\end{equation*}
где~$\boldsymbol{\theta} = \mathrm{col}(\mathbf{W}^{(L+1)}, \dots, \mathbf{W}^{(1)}).$

\begin{rustheorem}[Грабовой-Киселев-Мешков, 2024]
    \justifying
    Пусть функция нейронной сети~$f_{\boldsymbol{\theta}}(\mathbf{x})$ представима в виде суперпозиции линейных операторов, тогда матрица Гессе функции потерь относительно параметров модели представляется в факторизованной форме:
    $\mathbf{H}(\boldsymbol{\theta}) \approx \mathbf{Q}^{T}\mathbf{F}^{T}\mathbf{A}\mathbf{F}\mathbf{Q}$, описывающие G-компоненту матрицы Гессе.
\end{rustheorem}

\begin{rustheorem}[Грабовой-Киселев-Мешков, 2024]
    \justifying
    Пусть функция нейронной сети~$f_{\boldsymbol{\theta}}(\mathbf{x})$ представима в виде суперпозиции линейных операторов.
    Пусть для всех $p$ выполняется: $\left\|\mathbf{Q}^{(p)}\right\| \leqslant q, \left\|\mathbf{T}^{(p)}\right\|^2 \leqslant w_{\mathbf{T}}^2.$
    Тогда справедлива оценка: $\left\|\mathbf{H}_O\right\| \leqslant \sqrt{2}q^2\left\|\mathbf{x}\right\|^2(L+1)w_{\mathbf{T}}^{2L}.$
\end{rustheorem}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Сверточные модели глубокого обучения}
\justifying
\begin{rustheorem}[Грабовой-Киселев-Мешков, 2024]
    \justifying
    Пусть задана функция нейронной сети~$f_{\boldsymbol{\theta}}x = C_{\mathbf{W}^{(L+1)}} \circ \sigma \circ \dots \circ \sigma \circ C_{\mathbf{W}^{(1)}},$
    где~$C_{\mathbf{W}^{(i)}}$~--- одномерная свертка с ядром~$\mathbf{W}^{(i)}$. Пусть заданы следующие верхние оценки на параметры:~$C_l \leqslant C, k_i \leqslant k, d_i \leqslant d_1:=d, |\mathbf{W}^{(p)}_{i,j,k}|^2 \leqslant w^2.$
    Тогда норма матрицы Гессе имеет следующую верхнюю оценку: $\left\|\mathbf{H}\right\| \leqslant \sqrt{2}\left\|x\right\|^2d^2(L+1)(C^2w^2kd)^L.$
\end{rustheorem}

\begin{rustheorem}[Грабовой-Киселев-Мешков, 2024]
    \justifying
    Пусть задана функция нейронной сети~$f_{\boldsymbol{\theta}}\mathbf{x} = C_{\mathbf{W}^{(L+1)}} \circ \dots \circ C_{\mathbf{W}^{(1)}},$
    где $C_{\mathbf{W}^{(l)}}$~--- двумерная свертка с ядром $\mathbf{W}^{(i)}$.
    Пусть заданы следующие верхние оценки на параметры:~$C_l \leqslant C, k_i \leqslant k, m_i \leqslant m_1:=m, n_i \leqslant n_1:=n, |\mathbf{W}^{(p)}_{i,j,k}|^2 \leqslant w^2.$
    Тогда норма матрицы Гессе имеет следующую верхнюю оценку:~$\left\|\mathbf{H}_{O}\right\| \leqslant \sqrt{2}\left\|\mathbf{x}\right\|^2C^4k^4m^2n^2(L+1)(C^2k^2w^2mn)^{L}.$
\end{rustheorem}

\begin{rusremark}
    \justifying
    Полученные оценки имеют недостаток связанный с тем, что они не учитывают уменьшение размеров после сверточных операций и зависят только от верхних границ параметров.
\end{rusremark}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Снижение сложности в сверточных моделях}
\justifying
\begin{rustheorem}[Грабовой-Киселев-Мешков, 2024]
    \justifying
    Для сверточных сетей с операциями пулинга (MaxPool2D или AvgPool2D) норма матрицы Гессе уменьшается за счет множителя:
    \[
        \left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}
    \]
    где $k_{\mathrm{pool}}$~--- размер ядра пулинга, $L$~--- глубина сети, $l$~--- позиция слоя пулинга.
\end{rustheorem}

\begin{block}{Практический вывод}
Операции пулинга снижают сложность модели и действуют как \textbf{механизм регуляризации} в глубоких сверточных нейронных сетях.
\end{block}

\begin{rusremark}
    \justifying
    Это объясняет, почему пулинг-слои улучшают обобщающую способность моделей: они уменьшают сложность оптимизационного ландшафта.
\end{rusremark}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Сводная таблица асимптотических оценок}
\justifying
\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|}
\hline
\textbf{Архитектура} & \textbf{Асимптотика нормы $\|\mathbf{H}\|_2$} \\
\hline
Полносвязная сеть & $L(hM)^{2L}$ \\
\hline
1D свертка & $L(C^2w^2kd)^L$ \\
\hline
2D свертка & $L(C^2k^2w^2mn)^L$ \\
\hline
С пулингом & $L \cdot \left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} \cdot (\text{базовая оценка})$ \\
\hline
\end{tabular}
\end{table}

\begin{block}{Ключевые наблюдения}
\begin{itemize}
    \item Экспоненциальный рост с глубиной сети $L$
    \item Полиномиальный рост с размером слоев
    \item Пулинг снижает сложность
    \item Оценки позволяют предсказывать сложность без обучения модели
\end{itemize}
\end{block}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\section{Достаточный размер выборки}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Достаточный размер выборки}
    \centering

    \begin{enumerate}
        \item Статистические и байесовские методы
        \item Сэмплирование эмпирической функции ошибки (D- и M-достаточность)
        \item Близость апостериорных распределений (KL- и S-достаточность)
        \item Экспериментальное сравнение
    \end{enumerate}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Cэмлирования эмпирической функции ошибки}
\justifying
\begin{rusdefinition}
    \justifying
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ назовем D-достаточным, если для всех~$k\geqslant m^*$ выполняется условие:~$D(k) = \mathbb{D}_{\hat{\mathbf{w}}_{k}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \leqslant \varepsilon.$
\end{rusdefinition}

\begin{rusdefinition}
    \justifying
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ назовем M-достаточным, если для всех~$k\geqslant m^*$ выполняется условие:~$M(k) = \left| \mathbb{E}_{\hat{\mathbf{w}}_{k+1}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E}_{\hat{\mathbf{w}}_{k}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \right| \leqslant \varepsilon.$
\end{rusdefinition}

\begin{rustheorem}[Грабовой-Киселев, 2023]
    \justifying
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и~$\|\bm{\Sigma}_{k+1} - \bm{\Sigma}_k\|_{F}\to 0$ при~$k\to \infty$. 
    Тогда в модели линейной регрессии определение M-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$M(k)\leqslant\varepsilon$.
\end{rustheorem}

\begin{ruscorollary}
    \justifying
    Пусть~$\|\mathbf{m}_k - \mathbf{w}\|_2\to 0$ и~$\|\mathbf{\Sigma}_k - \left[k\mathcal{I}(\mathbf{w})\right]^{-1}\|_{F}\to 0$ при~$k \to \infty$.
    Тогда в модели линейной регрессии определение M-достаточного размера выборки корректно.
\end{ruscorollary}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{KL-близость апостериорных распределений}
\justifying
\begin{rusdefinition}
    \justifying
    Подвыборки~$\mathfrak{D}^1$ и~$\mathfrak{D}^2$ назовем близкими, если~$\mathcal{I}_2$ может быть получено из~$\mathcal{I}_1$ путем удаления, замены или добавления одного элемента, то есть
    \[
        \left| \mathcal{I}_1 \triangle \mathcal{I}_2 \right| = \left| \left( \mathcal{I}_1 \setminus \mathcal{I}_2 \right) \cup \left( \mathcal{I}_2 \setminus \mathcal{I}_1 \right) \right| = 1.
    \]
\end{rusdefinition}

\begin{rusdefinition}
    \justifying
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ называется KL-достаточным, если для всех~$k\geqslant m^*$
    \[
        KL(k) = D_{KL}(p_k \| p_{k+1}) = \int p_k(\mathbf{w}) \log{\frac{p_k(\mathbf{w})}{p_{k+1}(\mathbf{w})}} d\mathbf{w} \leqslant \varepsilon.
    \]
\end{rusdefinition}

\begin{rustheorem}[Грабовой-Киселев, 2023]
    \justifying
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и~$\|\bm{\Sigma}_{k+1} - \bm{\Sigma}_k\|_{F}\to 0$ при~$k\to \infty$.
    Тогда в модели с нормальным апостериорным распределением параметров определение KL-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$KL(k)\leqslant\varepsilon$.
\end{rustheorem}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{S-близость апостериорных распределений}
\justifying
\begin{rusdefinition}
    \justifying
    Пусть задано некоторое~$\varepsilon > 0$.
    Размер выборки~$m^*$ называется S-достаточным, если для всех~$k\geqslant m^*$
    \[
        S(k) = \text{s-score}(p_k, p_{k+1}) \geqslant 1-\varepsilon.
    \]
\end{rusdefinition}

\begin{rustheorem}[Грабовой-Киселев, 2023]
    \justifying
    Пусть~$\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2\to 0$ при~$k \to \infty$.
    Тогда в модели с нормальным апостериорным распределением параметров определение S-достаточного размера выборки корректно.
    А именно, для любого~$\varepsilon > 0$ существует такой~$m^*$, что для всех~$k\geqslant m^*$ выполняется~$S(k)\geqslant 1-\varepsilon$.
\end{rustheorem}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{KL-достаточность против S-достаточности}
\justifying
\begin{figure}[h!t]\center
    \includegraphics[width=\textwidth]{thesis/figures/chapter-3/posterior-distribution/sufficient-vs-threshold}
\end{figure}

Зависимость достаточного размера выборки от порогового параметра.
Для S-достаточного размера выборки требуются более низкие значения порога.
Таким образом, он оказывается более требовательным к этому значению.

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Экспериментальная проверка KL- и S-достаточности}
\justifying

\textbf{Цель:} исследовать сходимость функций $KL(k)$ и $S(k)$ к предельным значениям и сопоставить с классическими методами оценки достаточного объема выборки.

\begin{figure}[h!t]\center
    \includegraphics[width=0.8\textwidth]{thesis/figures/chapter-3/posterior-distribution/dependence_on_available_sample_set}
\end{figure}

\begin{itemize}
    \item Функции $KL(k)$ и $S(k)$ сходятся к предельным значениям.
    \item KL-достаточность дает наиболее консервативные оценки.
    \item S-достаточность указывает на меньшие достаточные объемы, оставаясь строго обоснованной.
    \item Наблюдается согласие с классическими методами при больших $m$.
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Экспериментальное сравнение методов оценки}
\justifying

\textbf{Цель:} сопоставить поведение различных методов определения достаточного объема выборки на реальных данных.

\begin{columns}[c]
\column{0.35\textwidth}
\centering
\includegraphics[width=1.0\columnwidth]{thesis/figures/chapter-3/statical/cross} \\

\includegraphics[width=1.0\columnwidth]{thesis/figures/chapter-3/statical/kl}

\column{0.65\textwidth}
\centering
\includegraphics[width=1.0\columnwidth]{thesis/figures/chapter-3/statical/graphs}
\end{columns}

\begin{itemize}
    \item Все исследованные методы демонстрируют \textbf{монотонное} поведение.
    \item Значения функций с ростом $k$ асимптотически стремятся к константе, что подтверждает корректность определений достаточного объема.
    \item Дисперсия оценок $m^*$ мала, методы устойчивы к выбросам.
    \item Полученные результаты согласуются с теоремами о сходимости предложенных критериев.
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\section{Снижение сложности моделей}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Снижение сложности моделей}
    \centering

    \begin{enumerate}
        \item Удаление параметров (pruning) на основе анализа градиентов
        \item Мультидоменная дистилляция знаний
        \item Анти-дистилляция: наращивание сложности с регуляризацией
        \item Экспериментальная оценка методов
    \end{enumerate}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Дистилляция на многих генеральных совокупностях}
\justifying

\begin{figure}[h!t]\center
    {\includegraphics[width=0.4 \textwidth]{thesis/figures/chapter-4/multidomain-distillation/Distillation_only}}
    {\includegraphics[width=0.4 \textwidth]{thesis/figures/chapter-4/multidomain-distillation/Distillation_DA}}
\end{figure}

\begin{rusdefinition}
    \justifying
    Генеральная совокупность~$B$ называется близкой к генеральной совокупности~$A$, если существует инъективное отображение $\varphi: A \rightarrow B$.
\end{rusdefinition}

Заданы выборки из двух близких генеральных совокупностей отображением~$\varphi$:
\[
    \mathfrak{D}_{A}=\{(\mathbf{x_i},\mathbf{y_i})\}_{i=1}^n,
\mathbf{x_i} \in \mathbb{X}_{A},
\mathbf{y_i} \in \mathbb{Y}, \qquad \mathfrak{D}_{B}=\{(\mathbf{x_i},\mathbf{y_i})\}_{i=1}^m, \mathbf {x_i} \in \mathbb{X}_{B}, \mathbf{y_i} \in \mathbb{Y}.
\]
Оптимизационная задача для дистилляции:
\begin{align*}
    \mathcal{L}(\mathbf{w,X,Y,f,\varphi}) =&
    -\lambda\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{ R}\mathbb{I}[y_{i}=r]\log{g^{r}(\mathbf{x}_{i},\mathbf{w})} -\\
    &-(1-\lambda)\sum\limits_{i=1}^{m}\sum\limits_{r=1}^{R}(f\circ \varphi)^{r}(\mathbf{x }_{i})\log{g^{r}(\mathbf{x}_{i},\mathbf{w})}.
\end{align*}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Прореживание нейросетей на основе анализа градиентов}
\justifying

\textbf{Идея:} оценить важность каждого параметра и удалить наименее значимые без потери качества.

\begin{itemize}
    \item По \textbf{дисперсии градиента}~--- параметры с малой дисперсией слабо влияют на обучение.
    \item По \textbf{мультиколлинеарности}~--- параметры, линейно зависимые с другими, избыточны.
\end{itemize}

\begin{figure}[h!t]\center
    \includegraphics[width=0.35\textwidth]{thesis/figures/chapter-4/belsli/Cov}
    \includegraphics[width=0.35\textwidth]{thesis/figures/chapter-4/belsli/BelslyImage}
\end{figure}

\begin{itemize}
    \item Удаление до $80\%$ параметров не снижает качества на простых моделях классификации.
    \item Метод Белсли эффективен при наличии мультиколлинеарности.
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Антидистилляция: передача знаний от простой модели к сложной}
\justifying

\textbf{Идея:} модель-учитель ($g_{\text{tr}}$) обучена на простой задаче $\mathfrak{D}_1$; нужно обучить более сложную модель-ученика ($f_{\text{st}}$) на сложной задаче $\mathfrak{D}_2$, используя знания учителя.

\textbf{Проблема:} размерности моделей разные ($N_{\text{tr}} < N_{\text{st}}$). Решение~--- составная функция потерь при инициализации:

\[
\mathcal{L}(\mathbf{w}) = \lambda_1\mathcal{L}_{\text{ce}}(\mathbf{w},\mathfrak{D}_1) + \lambda_2\|\mathbf{u}-\mathbf{Pr}[\mathbf{w}]\|^2 + \lambda_3\mathcal{L}_3^{\delta}(\mathbf{w},\mathfrak{D}_1) + \lambda_4\operatorname{tr}(\nabla^2\mathcal{L}_{\text{ce}}).
\]

\begin{small}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Метод инициализации} & \textbf{Точность} & \textbf{FSGM-атака} & \textbf{Шум в параметрах} \\
\hline
Xavier & 0.68±0.08 & 0.42±0.04 & 0.58±0.06 \\
Zero Pad & \textbf{0.86±0.02} & 0.50±0.01 & 0.71±0.03 \\
Uniform Pad & 0.85±0.04 & 0.52±0.03 & \textbf{0.73±0.03} \\
Transfer Learning & 0.74±0.09 & 0.50±0.06 & 0.53±0.05 \\
Net2Net & 0.85±0.04 & 0.51±0.02 & 0.70±0.03 \\
With Data Noise & 0.81±0.07 & 0.51±0.03 & 0.70±0.05 \\
Anti-Distillation ($\lambda_4=0$) & \textbf{0.86±0.05} & 0.53±0.03 & \textbf{0.73±0.04} \\
Anti-Distillation & \textbf{0.86±0.05} & \textbf{0.57±0.03} & 0.67±0.03 \\
\hline
\end{tabular}
\end{small}

\textbf{Результаты:} Anti-Distillation дает лучшую точность и устойчивость к адверсарным атакам (FSGM) и шуму в параметрах.
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Радамахеровская сложность многозадачного обучения}
\justifying
\begin{rustheorem}[Грабовой-Грицай-Ремизова, 2025]
    \justifying
    Пусть $S_t=\{x_i\}_{i=1}^n$~--- выборка фиксированной целевой задачи $t$ с $\|x_i\|_2 \le R$.
    Пусть $\phi(\cdot;w)$~--- энкодер, и рассмотрим линейные головы
    $f_{w,h}(x)=h^\top \phi(x;w)$ с $\|h\|_2 \le B_{\mathrm{head}}$.
    Предположим:
    \begin{enumerate}
        \item Ограничение на признаки: для всех $x,w$ 
        $\|\phi(x;w)\|_2 \le L\,\|w\|\,\|x\|_2$.
        \item Энкодер STL удовлетворяет $\|w_{\mathrm{enc}}\|\le B_{\mathrm{enc}}$,
        общий энкодер MTL удовлетворяет $\|w_{\mathrm{shared}}\|\le B_{\mathrm{shared}}$.
        \item Многозадачное масштабирование: $B_{\mathrm{shared}} \le B_{\mathrm{enc}}/\sqrt{T}$.
    \end{enumerate}

    Тогда справедлива оценка сложности: $\widehat{\mathfrak R}_n\big(\mathcal{F}_{\mathrm{MTL}}^{(t)};S_t\big) \le \frac{1}{\sqrt{T}}\widehat{\mathfrak R}_n\big(\mathcal{F}_{\mathrm{STL}}^{(t)};S_t\big).$
\end{rustheorem}

\begin{rusremark}
    \justifying
    В постановке Бакстера, количество задач $T$ само по себе способствует оценке общего индуктивного смещения: с ростом $T$ сложность данных на задачу уменьшается пропорционально $1/T$.
    В отличие от этого, данный анализ сохраняет размер выборки целевой задачи $n$ фиксированным и сравнивает STL и MTL на одном и том же $n$, поэтому улучшение проявляется как множитель $1/\sqrt{T}$ в сложности Радемахера на задачу.
\end{rusremark}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{LoRA-адаптеры в многозадачном обучении}
\justifying

\textbf{Цель:} исследовать зависимость качества модели от ранга $r$ LoRA-адаптеров и сопоставить с теоретическими предсказаниями.

\begin{figure}[h!t]\center
    \includegraphics[width=0.6\textwidth]{thesis/figures/chapter-5/rademacher/lora_rank_vs_f1-1}
\end{figure}

\begin{itemize}
    \item Наилучшая производительность достигается при $r=8$; дальнейшее увеличение ранга не даёт выигрыша.
    \item Подтверждает эффективность низкоранговой параметризации и теорему о состоятельности LoRA.
    \item Согласуется с теоретическим снижением сложности Радемахера в многозадачном обучении.
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Декодирование фМРТ-изображений}
\justifying

\textbf{Цель:} применить методы снижения сложности данных для реконструкции фМРТ-снимков по видеоряду и оценить влияние сжатия на качество и скорость.

\begin{figure}[h!t]\center
    \includegraphics[width=0.7\textwidth]{thesis/figures/chapter-5/fmri/scheme}
\end{figure}

\begin{itemize}
    \item Сжатие фМРТ-изображений в 2, 4 и 8 раз сокращает время обучения с 36 с до 6.7 с, 1.6 с и 1.4 с соответственно.
    \item Качество реконструкции (MSE) не ухудшается; оптимальный коэффициент регуляризации $\alpha \approx 1000$ сохраняется при всех степенях сжатия.
    \item Метод демонстрирует эффективность управления сложностью данных в реальной задаче нейровизуализации.
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\section{Выносится на защиту}
\begin{frame}{Положения, выносимые на защиту}
\justifying
\begin{enumerate}
    \item \textbf{Единый теоретический аппарат} оценки сложности моделей и данных на основе теории мер и анализа ландшафта, включающий формальные определения мер сложности $\mu_D$, $\mu_f$ и критерий обучаемости $\mu_f \le \mu_D$.
    \item \textbf{Ландшафтная мера сложности} модели $\mu_f(f|D) = \mathbb{E}\|\mathbf{H}_i - \mathbb{E}\mathbf{H}_i\|$, определяемая через спектральные свойства матриц Гессе, и её связь с условной сложностью выборки.
    \item \textbf{Теоретические оценки} спектральных норм матриц Гессе для полносвязных, свёрточных и трансформерных архитектур (асимптотики вида $L(hM)^{2L}$, $L(C^2w^2kd)^L$ и др.).
    \item \textbf{Методы оценки достаточного объёма выборки}: D- и M-достаточность (на основе стабильности функции потерь), KL- и S-достаточность (на основе близости апостериорных распределений) с доказательством сходимости.
    \item \textbf{Методы снижения сложности}: прореживание на основе ковариации градиентов, мультидоменная дистилляция, анти-дистилляция для передачи знаний между моделями и доменами.
\end{enumerate}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Соответствие паспорту специальности 1.2.1}
\justifying
\begin{tabular}{|p{0.7\textwidth}|c|}
\hline
\textbf{Пункт паспорта} & \textbf{Положения} \\
\hline
2. Исследования в области оценки качества и эффективности алгоритмических и программных решений для систем искусственного интеллекта и машинного обучения. Методики сравнения и выбора алгоритмических и программных решений при многих критериях. & 1, 4, 5 \\
\hline
4. Разработка методов, алгоритмов и создание систем искусственного интеллекта и машинного обучения для обработки и анализа текстов на естественном языке, для изображений, речи, биомедицины и других специальных видов данных. & 5 \\
\hline
16. Исследования в области специальных методов оптимизации, проблем сложности и элиминации перебора, снижения размерности. & 1, 2, 4, 5 \\
\hline
17. Исследования в области многослойных алгоритмических конструкций, в том числе – многослойных нейросетей. & 2, 3 \\
\hline
\end{tabular}
\end{frame}
%----------------------------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------------------------
\begin{frame}[allowframebreaks]{Публикации по теме диссертации}
\justifying
{
\tiny
\begin{enumerate}
    \item Грицай Г. М., Грабовой А. В. Интерпретация классификаторов на основе архитектуры трансформер с помощью кластеризации // Доклады Российской академии наук. Математика, информатика, процессы управления.~--- 2025.~--- Т. 527.~--- С. 432–448.
    \item Дорин Д. Д., Варламова К. Д., Грабовой А. В. Попарное сравнение изображений для обнаружения плагиата // Доклады Российской академии наук. Математика, информатика, процессы управления.~--- 2025.~--- Т. 527.~--- С. 68–83.
    \item Дорин Д. Д., Грабовой А. В., Стрижов В. В. Улучшение декодирования данных ФМРТ с использованием пространственно-временных характеристик в условиях ограниченного набора данных // Доклады Российской академии наук. Математика, информатика, процессы управления.~--- 2025.~--- Т. 527.~--- С. 11–30.
    \item Варламова К. Д., Дорин Д. Д., Грабовой А. В. За чертой знакомых доменов: исследование обобщающей способности детекторов машинно сгенерированных изображений // Доклады Российской академии наук. Математика, информатика, процессы управления.~--- 2025.~--- Т. 527.~--- С. 103–116.
    \item Зверева А. К., Грабовой А. В., Каприелова М. С. Динамическое разделение труда в гибридном ии: стратегии кодировщиков и их воздействие на модуляторы на основе сетей долгой краткосрочной памяти // Доклады Российской академии наук. Математика, информатика, процессы управления.~--- 2025.~--- Т. 527.~--- С. 117–133.
    \item Kiselev N. S., Grabovoy A. V. Sample size determination: Likelihood bootstrapping // Computational Mathematics and Mathematical Physics.~--- 2025.~--- Vol. 65, no. 2.~--- P. 416–423.
    \item Kiselev N., Grabovoy A. Sample size determination: posterior distributions proximity // Computational Management Science.~--- 2025.~--- Vol. 22, no. 1.~--- P. 1.
    \item Zvereva A. K., Kaprielova M., Grabovoy A. Anomlite: Efficient binary and multiclass video anomaly detection // Results in Engineering.~--- 2025.~--- Vol. 25.~--- P. 104162.
    \item G. Gritsai, A. Voznuyk, I. Khabutdinov, A. Grabovoy. Advacheck at genai detection task 1: Ai detection powered by domain-aware multi-tasking // Proceedings of the 1st Workshop on GenAI Content Detection (GenAIDetect).~--- Abu Dhabi, UAE: International Conference on Computational Linguistics, 2025.~--- P. 236–243.
    \item Voznyuk A., Gritsai G., Grabovoy A. Advacheck at semeval-2025 task 3: Combining ner and rag to spot hallucinations in llm answers // Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025).~--- Vienna, Austria: Association for Computational Linguistics, 2025.~--- P. 1204–1210.
    \item Voznyuk A., Gritsai G., Grabovoy A. Team advacheck at pan: multitasking does all the magic // Working Notes of CLEF 2025 - Conference and Labs of the Evaluation Forum.~--- Vol. 4038.~--- CEUR-WS.org, 2025.~--- P. 4007–4014.
    \item A. Levikin, I. Khabutdinov, A. Grabovoy, K. Vorontsov. The methodology of multi-criteria evaluation of text markup models based on inconsistent expert markup  // Computational Linguistics and Intellectual Technologies: Proceedings of the International Conference “Dialogue 2025”.~--- Moscow: JINR, 2025.
    \item I. Kopanichuk, A. Chashchin, A. Grabovoy et al. Structure extractor: Multilingual extraction of sections from scientific document // 2025 37th Conference of Open Innovations Association (FRUCT).~--- IEEE, 2025.~--- P. 122–128.
    \item G. M. Gritsay, A. V. Grabovoy, A. S. Kildyakov, Y. V. Chekhovich Artificially generated text fragments search in academic documents // Doklady Mathematics.~--- 2024.
    \item Gritsay G., Grabovoy A. Automated text identification on languages of the iberian peninsula: Llm and bert-based models aggregation // Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2024) co-located with the Conference of the Spanish Society for Natural Language Processing (SEPLN 2024).~--- Vol. 3756.~--- CEUR-WS.org, 2024.
    \item Meshkov V., Kiselev N., Grabovoy A. Convnets landscape convergence: Hessian-based analysis of matricized networks // 2024 Ivannikov Ispras Open Conference (ISPRAS).~--- IEEE, 2024.~--- P. 1–10.
    \item D. Dorin, N. Kiselev, A. Grabovoy, V. Strijov. Forecasting fmri images from video sequences: linear model analysis // Health Information Science and Systems.~--- 2024.~--- Vol. 12, no. 1.~--- P. 55.
    \item Chekhovich Y., Grabovoy A., Gritsai G. Generative ai models with their full reveal* // 2024 4th International Conference on Technology Enhanced Learning in Higher Education (TELE).~--- Vol. 1.~--- IEEE, 2024.~--- P. 17–22.
    \item M. Kaprielova, A. Grabovoy, K. Varlamova et al. Image plagiarism detection pipeline for vast databases // 2024 35th Conference of Open Innovations Association (FRUCT).~--- IEEE, 2024.~--- P. 328–335.
    \item Gritsai G., Khabutdinov I., Grabovoy A. Multi-head span-based detector for ai-generated fragments in scientific papers // Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024).~--- Bangkok, Thailand: Association for Computational Linguistics, 2024.~--- P. 220–225.
    \item Asvarov A., Grabovoy A. Neural machine translation system for lezgian, russian and azerbaijani languages // 2024 Ivannikov Ispras Open Conference (ISPRAS).~--- IEEE, 2024.~--- P. 1–7.
    \item Poimanov D., Mestetsky L., Grabovoy A. N-gram perplexity-based ai-generated text detection // 2024 Ivannikov Ispras Open Conference (ISPRAS).~--- IEEE, 2024.~--- P. 1–8.
    \item I. A. Khabutdinov, A. V. Chashchin, A. V. Grabovoy et al. Rugector: Rule-based neural network model for russian language grammatical error correction // Programming and Computer Software.~--- 2024.~--- Vol. 50, no. 4.~--- P. 315–321.
    \item Грицай Г. М., Хабутдинов И. А., Грабовой А. В. Stack more llms: эффективное обнаружение машинно-сгенерированных текстов с помощью аппроксимации значений перплексии // Доклады Российской академии наук. Математика, информатика, процессы управления.~--- 2024.~--- Т. 520, № 2.~--- С. 228–237.
    \item Boeva G., Gritsay G., Grabovoy A. Team ap-team at pan: Llm adapters for various datasets // Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024).~--- Vol. 3740.~--- CEUR-WS.org, 2024.~--- P. 2527–2535.
    \item A. V. Grabovoy, M. S. Kaprielova, A. S. Kildyakov et al. Text reuse detection in handwritten documents // Doklady Mathematics.~--- 2024.
    \item Asvarov A., Grabovoy A. The impact of multilinguality and tokenization on statistical machine translation // 2024 35th Conference of Open Innovations Association (FRUCT).~--- IEEE, 2024.~--- P. 149–157.
    \item Киселев Н. С., Грабовой А. В. Раскрытие Гессиана: ключ к плавной сходимости поверхности функции потерь // Доклады Российской академии наук. Математика, информатика, процессы управления.~--- 2024.~--- Т. 520, № 2.~--- С. 57–70.
    \item G. Gritsay, A. Grabovoy, A. Kildyakov, Y. Chekhovich. Automated text identification: Multilingual transformer-based models approach // Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2023) co-located with the Conference of the Spanish Society for Natural Language Processing (SEPLN 2023).~--- Vol. 3496.~--- CEUR-WS.org, 2023.
    \item Varlamova K., Khabutdinov I., Grabovoy A. Automatic spelling correction for russian: Multiple error approach // 2023 Ivannikov Ispras Open Conference (ISPRAS).~--- IEEE, 2023.~--- P. 169–175.
    \item O. Bakhteev, Y. Chekhovich, A. Grabovoy et al. Cross-language plagiarism detection: A case study of european languages academic works // Academic Integrity: Broadening Practices, Technologies, and the Role of Students.~--- Vol. 4 of Ethics and Integrity in Educational Contexts.~--- New York: Springer International Publishing, 2023.~--- P. 143–161.
    \item Avetisyan K., Gritsay G., Grabovoy A. Cross-lingual plagiarism detection: Two are better than one // Programming and Computer Software.~--- 2023.~--- Vol. 49, no. 4.~--- P. 346–354.
    \item D. Shodiev, I. Kopanichuk, A. Chashchin et al. Ensembling models for the generation of queries to an altering search engine using reinforcement learning // 2023 Ivannikov Ispras Open Conference (ISPRAS).~--- IEEE, 2023.~--- P. 144–149.
    \item I. Potyashin, M. Kaprielova, Y. Chekhovich et al. Hwr200: New open access dataset of handwritten texts images in russian // Proceedings of the International Conference “Dialogue 2023”.~--- 2023.
    \item Grashchenkov K., Grabovoy A., Khabutdinov I. A method of multilingual summarization for scientific documents // 2022 Ivannikov Ispras Open Conference (ISPRAS).~--- IEEE, 2022.
    \item K. Petrushina, O. Bakhteev, A. Grabovoy, V. Strijov Anti-distillation: Knowledge transfer from a simple model to the complex one // 2022 Ivannikov Ispras Open Conference (ISPRAS).~--- IEEE, 2022.
    \item Gritsay G., Grabovoy A., Chekhovich Y. Automatic detection of machine generated texts: Need more tokens // Ivannikov Memorial Workshop Proceedings 2022.~--- 2022.
    \item A. V. Grabovoy, T. T. Gadaev, A. P. Motrenko, V. V. Strijov. Numerical methods of sufficient sample size estimation for generalised linear models // Lobachevskii Journal of Mathematics.~--- 2022.~--- Vol. 43, no. 9.~--- P. 2453–2462.
    \item Грабовой А. В., Стрижов В. В. Вероятностная интерпретация задачи дистилляции // Автоматика и телемеханика.~--- 2022.~--- № 1.~--- С. 150–168.
    \item Базарова А. И., Грабовой А. В., Стрижов В. В. Анализ свойств вероятностных моделей в задачах обучения с экспертом // Автоматика и телемеханика.~--- 2022.~--- № 10.
    \item Grabovoy A. V., Strijov V. V. Bayesian distillation of deep learning models // Automation and Remote Control.~--- 2021.~--- Vol. 82, no. 11.~--- P. 1846–1856.
    \item Grabovoy A. V., Strijov V. V. Prior distribution selection for a mixture of experts // Computational Mathematics and Mathematical Physics.~--- 2021.~--- Vol. 61, no. 7.~--- P. 1140–1152.
    \item Grabovoy A., Bakhteev O., Chekhovich Y. The automatic approach for scientific papers dating // Proceedings of the 2020 Ivannikov Ispras Open Conference.~--- Los Alamitos, CA: IEEE Computer Society Press, 2021.
    \item Грабовой А. В., Стрижов В. В. Анализ выбора априорного распределения для смеси экспертов // Журнал вычислительной математики и математической физики.~--- 2021.~--- Т. 61, № 7.~--- С. 1149–1161.
    \item Грабовой А. В., Стрижов В. В. Байесовская дистилляция моделей глубокого обучения // Автоматика и телемеханика.~--- 2021.~--- № 11.~--- С. 16–29.
    \item Грабовой А. В., Бахтеев О. Ю., Стрижов В. В. Введение отношения порядка на множестве параметров аппроксимирующих моделей // Информатика и ее применения.~--- 2020.~--- Т. 14, № 2.
    \item Grabovoy A. V., Strijov V. V. Quasi-periodic time series clustering for human activity recognition // Lobachevskii Journal of Mathematics.~--- 2020.~--- Vol. 41, no. 3.~--- P. 333–339.
    \item Грабовой А. В., Бахтеев О. Ю., Стрижов В. В. Определение релевантности параметров нейросети // Информатика и ее применения.~--- 2019.
\end{enumerate}
}
По теме диссертации опубликовано 56 научных работ, из
которых 17 статей в научно-технических журналах, входящих в перечень ВАК, 32~--- в изданиях, входящих в международные наукометрические базы Scopus и Web of Science. В трудах российских и международных конференций опубликовано 39 работ. Также на основе работ автора зарегистрировано 13 программ для ЭВМ.
\end{frame}
%----------------------------------------------------------------------------------------------------------
\end{document}