\documentclass{dissert}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumerate}

\usepackage[all]{xy}
% colors
\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0.0, 0.2, 0.13}
\definecolor{darkcyan}{rgb}{0.0, 0.55, 0.55}

\usepackage{geometry}
\geometry{left=2.5cm}
\geometry{right=1.0cm}
\geometry{top=2.0cm}
\geometry{bottom=2.0cm}
\renewcommand{\baselinestretch}{1.0}

\newcommand{\paragraph}[1]{\noindent\textbf{#1}\quad}


%https://tex.stackexchange.com/questions/163451/total-number-of-citations
\usepackage{totcount}
\newtotcounter{citnum} %From the package documentation
\def\oldbibitem{} \let\oldbibitem=\bibitem
\def\bibitem{\stepcounter{citnum}\oldbibitem}

\makeatletter
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{#1.~#2}%
  \ifdim \wd\@tempboxa >\hsize
    #1.~#2\par
  \else
    \global \@minipagefalse
    \hb@xt@\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother

\reversemarginpar

\renewcommand{\contentsname}{Содержание}
\renewcommand{\contentsdesc}{Стр.}
\renewcommand{\chaptername}{Глава}

%%% Библиография %%%
\makeatletter
\bibliographystyle{utf8gost71u}     % Оформляем библиографию по ГОСТ 7.1 (ГОСТ Р 7.0.11-2011, 5.6.7)
\makeatother


% Нужные мне пакеты
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{caption}
\usepackage{color}
\usepackage{bm}
\usepackage{tabularx}
\usepackage{url}
\usepackage{multirow}

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{definition}{Определение}

\usepackage{comment}
\usepackage{rotating}

\usepackage{autonum}

\begin{document}

\begin{titlepage}
\begin{flushright}
{На правах рукописи}
\end{flushright}
\vspace{1.5cm}
\begin{center}
{Грабовой Андрей Валериевич}
\par
\vspace{2cm}
\textsc{О сложности моделей и данных \\ в современных моделях глубокого обучения}
\par
\vspace{2cm}
{1.2.1~--- Искусственный интеллект и машинное обучение}
\par
\vspace{2cm}
{АВТОРЕФЕРАТ\\
диссертации на соискание ученой степени\\
доктора физико-математических наук}
\end{center}
\par
\vspace{3.5cm}
\begin{center}
{Москва~--- 2026}
\end{center}
\end{titlepage}

%\clearpage\maketitle
\setcounter{page}{2}
%\pretolerance=10000
%\thispagestyle{empty}
\noindent {Работа выполнена в Лаборатории 42 <<Интеллектуального анализа данных>> Института проблем управления им. В.А.Трапезникова РАН.

\vspace{0.1cm}

%\begin{sloppy}
%\fontdimen2\font=3pt

\vskip1ex\noindent
\begin{tabularx}{\linewidth}{@{}lX@{}}
  Научный консультант: & \textbf{Воронцов Константин Вячеславович}\\
  & доктор физико-математических наук, профессор РАН, Федеральный исследовательский~центр <<Информатика и управление>> Российской академии наук, отдел интеллектуальных систем, ведущий научный сотрудник.
  \\[2pt]
  Официальные оппоненты: & \textbf{TODO}\\
  & TODO\\[2pt]
  & \textbf{TODO}\\
  & TODO\\[2pt]
  & \textbf{TODO}\\
  & TODO\\[2pt]
  Ведущая организация: & Автономная некоммерческая организация высшего образования «Университет Иннополис».
\end{tabularx}
\vskip2ex\noindent

\vspace{0.2cm}
\noindent Защита состоится~ДЕНЬ~МЕСЯЦ ГОД года~в~14:00 на~заседании диссертационного совета Д 002.073.05 при Федеральном исследовательском центре <<Информатика и управление>> Российской академии наук (ФИЦ~ИУ~РАН) по адресу: 119333, г.\,Москва, ул.\,Вавилова, д.\,40.

\vspace{0.2cm}
\noindent С диссертацией можно ознакомиться в библиотеке Федерального государственного учреждения Федеральный исследовательский центр <<Информатика и управление>> Российской академии наук и на сайте http://www.frccsc.ru/

\vspace{0.2cm}
\noindent Автореферат разослан  \qquad \qquad \qquad \qquad 2022 года.

\vspace{0.3cm}
\noindent Ученый секретарь\\
диссертационного совета Д 002.073.05\\
к.т.н.
\hspace{12cm} И.\,А.\;Рейер
}

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pretolerance=-1

%\setcounter{page}{0}

\section*{Общая характеристика работы}
\paragraph{Актуальность темы.}
С начала XXI века наблюдается экспоненциальный рост производительности вычислительных систем, измеряемой в операциях с плавающей запятой~(англ. FLOPS): от терафлопсов в начале столетия до экзафлопсов в настоящий момент.
Параллельно происходит сопоставимый рост сложности моделей глубокого обучения, выраженный в увеличении количества обучаемых параметров на несколько порядков~--- от тысяч в начале нулевых годов до миллиардов и сотен миллиардов в двадцатых, с прогнозируемым переходом к триллионам параметров в ближайшем десятилетии.
Современные исследования анализа сложности таких моделей преимущественно опираются на эмпирические корреляции, связывающие эффективность моделей с количественными метриками~--- числом параметров и вычислительными затратами на обучение или применение.
Отсутствие строгой теоретической основы для предсказания поведения моделей при масштабировании делает процесс разработки экономически и энергетически неэффективным и зачастую приводит к получению необоснованных и противоречивых результатов.

Особую остроту проблема приобретает при разработке больших языковых моделей~(англ. LLM), обучение которых требует значительных вычислительных, энергетических и финансовых ресурсов.
Для снижения непредсказуемости процесса обучения на предварительной стадии~(англ. pretrain) эмпирически подбираются оптимальные соотношения между размером модели в параметрах и объемом обучающих данных в токенах при заданном вычислительном ресурсе.
Однако эмпирические оценки, полученные для одной архитектуры, не переносятся на другие модели, что делает подобные подходы несостоятельными и приводит к непредвиденным результатам при полномасштабном обучении.
В связи с этим разработка теоретических оценок сложности моделей становится критически важной, поскольку такие оценки позволяют связать сложность модели со сложностью выборки еще на этапе проектирования архитектуры, обеспечивая рациональный выбор параметров модели и объема данных.
Отметим, что глубоким теоретическим анализом выбора и порождения моделей на этапе проектирования архитектур моделей глубокого обучения занимается Вадим Викторович Стрижов~\cite{strijov2014doctoral}.

Классические подходы к оценке сложности моделей машинного обучения, разработанные для традиционных методов, находят ограниченное применение при анализе моделей глубокого обучения.
Фундаментальный вклад в теорию оценивания сложности моделей внесли работы Владимира Наумовича Вапника и Алексея Яковлевича Червоненкиса, заложившие основы статистической теории обучения~\cite{vapnik1974StatTheory}, а также Лесли Вэлианта, предложившего подход приближенно правильного обучения~(англ. PAC-Learning)~\cite{valiant1984LearnableTheory}.
Современные подходы к определению сложности основаны на Радемахеровской сложности, предложенной Владимиром Кольчинским и Дмитрием Панченко~\cite{koltchinskii2000RadamakherTheory}, и комбинаторном подходе к оценке обучаемости алгоритмов Константина Вячеславовича Воронцова~\cite{vorontsov04qualdan}, в котором разработан математический аппарат на основе комбинаторных оценок вероятности переобучения моделей.
Однако современные исследования в области теории машинного обучения редко рассматривают нейросетевые модели глубокого обучения ввиду их высокой сложности и трудности получения адекватных оценок сложности таких моделей.
Значительная часть современных исследований на ведущих конференциях посвящена классическим методам машинного обучения и улучшению оценок для известных методов, так как текущие оценки сложности даже для классических моделей являются сильно завышенными.

Отдельным направлением в исследованиях являются оценки репрезентативной способности моделей глубокого обучения к аппроксимации по прецедентам.
Основополагающий результат принадлежит Джорджу Цибенко~\cite{cybenko1989ApproximationBS}, который доказал, что нейронные сети аппроксимируют непрерывные функции с некоторыми ограничениями сколь угодно большим качеством.
Йохану Хестад~\cite{hastad1987phd} принадлежат первые оценки, указывающие на рост аппроксимирующей способности моделей глубокого обучения с глубиной модели.
В более современных работах Яна Лекуна~\cite{bengioLecun2007:scaling}, Йошуа Бенджио~\cite{begio2011exponetialdepth}, Надава Коэна~\cite{cohen2015OnTE} эти оценки получены с более мягкими ограничениями на класс рассматриваемых функций.
В целом все эти оценки наряду с работой Охада Шамира~\cite{eldan2016exponentialcomplexity} указывают на экспоненциальное увеличение аппроксимирующей способности моделей глубокого обучения с ростом числа слоев.

В настоящей работе предлагается теоретический анализ нейросетевых моделей, опирающийся на анализ их матриц Гессе.
Матрица Гессе функции потерь по параметрам модели содержит информацию о локальной кривизне оптимизационного ландшафта и используется для анализа сложности моделей и данных, а также для оценки важности~\cite{bakhteev2020ordering609999528, bakhteev2019estimation609999169} параметров в задачах прореживания нейросетевых моделей.

\paragraph{Степень разработанности темы диссертационного исследования.}
Классические подходы к оценке сложности моделей машинного обучения, основанные на VC-размерности, PAC-обучаемости и радемахеровской сложности, разработаны для моделей с ограниченным числом параметров и ориентированы на анализ худшего случая~(англ. worst-case), что делает их оценки слишком консервативными для перепараметризованных нейронных сетей~\cite{macKay2003}.
Эти оценки не учитывают специфику параметрических моделей глубокого обучения, такие как сверточные фильтры, остаточные связи и механизмы внимания, а также не отражают влияние регуляризации, ранней остановки и особенностей процесса оптимизации на обобщающую способность моделей.
Комбинаторный подход К.В. Воронцова~\cite{vorontsov04qualdan} позволяет снизить оценки Вапника--Червоненкиса, но остается применимым лишь к ограниченным классам моделей.

Что касается оценки сложности данных, существующие подходы преимущественно сводятся к анализу размера выборки для обучения~\cite{motrenko2022numerical613055643, demidenko2007, joseph1997, joseph1995, kloek1975, lindley1997, motrenko2014, qumsiyeh2013, rubin1998, self1988, self1992, shieh2000, shieh2005, wang2002}, что является неполным, так как сложность выборки также определяется сложностью каждого объекта.
Современные методы оценки сложности объектов выборки опираются на исследования сложности многообразий, аппроксимирующих элементы выборки.
Для больших языковых моделей используются эмпирические законы масштабирования~\cite{hoffmann2022Chinchila,kaplan2020ScalingLaws}, полученные Джаредом Капланом и Джорданом Хофманом~\cite{rae2022scalinglanguagemodelsmethods}, однако эти законы не имеют строгого теоретического обоснования и не объясняют механизмы, лежащие в основе зависимостей между параметрами, данными и качеством модели.

В области снижения сложности моделей существуют два основных направления: квантизация параметров моделей и дистилляция знаний~\cite{hinton2015, lopez2016, grabovoi2021bayesian609999432}.
Однако существующие методы не опираются на строгие теоретические оценки сложности моделей и данных, что ограничивает их эффективность и предсказуемость.

Таким образом, на текущий момент в исследованиях отсутствует единый теоретический аппарат для описания сложности моделей и данных, позволяющий получить асимптотические оценки и установить формальные критерии соответствия между сложностью модели и сложностью данных.
Разработка такого аппарата позволит проводить сравнительный анализ различных параметрических моделей глубокого для выбора оптимального решения для заданной задачи, характеризуемой выборкой определенной сложности.

В настоящей работе разработан комплексный теоретический подход к оценке сложности моделей и данных.
Предложен новый математический аппарат для анализа сложности на основе анализа ландшафта функции потерь нейросетевых моделей через матрицы Гессе.
Получены теоретические оценки ландшафтных мер для различных моделей глубокого обучения.
Разработан анализ связи объема и сложности выборки со сложностью модели, введены частные случаи общей теории сложности, позволяющие получать практические оценки для прикладных задач.

\paragraph{Объектом исследования} являются параметрические семейства функций, задаваемые суперпозициями линейных и нелинейных преобразований, а также конечные выборки данных, применяемые для оценки параметров указанных семейств в рамках задачи минимизации эмпирического риска.

\paragraph{Предмет исследования:} разработка теоретического аппарата для оценки и анализа сложности моделей глубокого обучения и сложности данных, а также установление формальных критериев соответствия между сложностью модели и сложностью выборки, необходимой для ее обучения.

\paragraph{Цель и задачи исследования.}
Целью исследования является построение единого теоретического аппарата для оценки сложности моделей глубокого обучения и сложности данных, а также установление формальных критериев соответствия между сложностью модели и сложностью выборки, необходимой для ее обучения.

Для достижения цели были поставлены и решены следующие задачи:
\begin{itemize}
    \item[--] введение определений мер сложности моделей и данных в рамках теории мер и установление критерия обучаемости модели на выборке;
    \item[--] получение теоретических оценок ландшафтных мер на основе матриц Гессе для полносвязных, сверточных и трансформерных архитектур моделей глубокого обучения;
    \item[--] установление связи ландшафтной меры с условной сложностью выборки;
    \item[--] построение методов оценки достаточного объема выборок из простой генеральной совокупности;
    \item[--] построение методов снижения сложности моделей глубокого обучения;
    \item[--] демонстрация практического применения построенного теоретического аппарата в прикладных задачах.
\end{itemize}

\paragraph{Методы исследования.}
Для решения поставленных задач в диссертации используются методы теории мер, линейной алгебры, матричного анализа, включая матричное дифференцирование и спектральный анализ матриц, методы теории вероятностей, математической статистики, включая байесовский анализ, анализ сходимости случайных процессов, методы теории оптимизации, анализа функций многих переменных, методы статистической теории обучения.

\paragraph{Научная новизна.}
\begin{enumerate}
    \item Введены формальные определения меры сложности выборки $\mu_D(D)$ и меры сложности модели $\mu_f(f)$ в рамках теории мер, установлен критерий обучаемости $\mu_f(f) \le \mu_D(D)$.
    \item Введена ландшафтная мера сложности модели $\mu_f(f|D) = \mathbb{E}\|\mathbf{H}_i - \mathbb{E}\mathbf{H}_i\|$, определяемая через спектральные свойства матриц Гессе.
    \item Впервые получены строгие теоретические оценки ландшафтной меры на основе спектральных норм матриц Гессе для полносвязных, сверточных и трансформерных архитектур.
    \item Доказана связь между достаточным объемом выборки и ландшафтной сложностью моделей.
    \item Предложены методы снижения сложности: прореживание на основе ковариации градиентов, мультидоменная дистилляция и анти-дистилляция.
    \item Доказаны теоремы о состоятельности LoRA-адаптеров и снижении сложности Радемахера в многозадачном обучении.
\end{enumerate}

\paragraph{Теоретическая значимость работы.}
Диссертационная работа представляет собой теоретический вклад в теорию машинного обучения, расширяющий классические подходы к оценке сложности моделей на случай перепараметризованных нейронных сетей.
В работе рассматриваются вопросы о сложности параметрических моделей глубокого обучения, для которых получены строгие теоретические результаты, связывающие сложность модели со сложностью данных в рамках единого математического аппарата.
Полученные оценки ландшафтной меры, которые открывают новые направления исследований в теории выбора моделей машинного обучения, теории оптимизации нейронных сетей и анализе обобщающей способности моделей глубокого обучения.

\paragraph{Практическая значимость работы.}
Разработанный теоретический аппарат применен к решению прикладных задач машинного обучения: многозадачного обучения, декодирования фМРТ-изображений и детекции машинно-генерированного контента.
Полученные методы оценки и управления сложностью моделей и данных экспериментально подтверждены на реальных задачах компьютерного зрения, обработки естественного языка и классификации.

\paragraph{Положения, выносимые на защиту:}
\begin{enumerate}
    \item Разработанный единый теоретический аппарат оценки сложности моделей глубокого обучения и сложности данных на основе теории мер и анализа ландшафта оптимизационной задачи, включающий формальные определения мер сложности и критерий обучаемости модели на выборке.
    \item Введенная ландшафтная мера сложности модели, определяемая через спектральные свойства матриц Гессе функции потерь, задает количественную связь между архитектурными характеристиками модели и условной сложностью выборки.
    \item Полученные теоретические оценки ландшафтной меры сложности для полносвязных, сверточных и трансформерных архитектур раскрывают характер зависимости сложности моделей от их глубины, ширины и иных структурных параметров.
    \item Получены теоретические оценки сходимости методов определения достаточного объема выборки, для которых установлена связь со сложностью моделей
    \item Предложенные методы снижения сложности моделей глубокого обучения на основе анализа ковариационной матрицы градиентов, дистилляции на многодоменных данных и анти-дистилляции обеспечивают эффективное сокращение числа параметров и передачу знаний между моделями различной сложности и доменами данных.
\end{enumerate}

\paragraph{Степень достоверности результатов.}
Достоверность научных результатов работы подтверждается непротиворечивостью и согласованностью с известными фактами и исследованиями в рассматриваемой области, высокой степенью сходимости теоретических результатов с данными экспериментов и определяется применением теоретических и методологических основ разработок ведущих ученых в области обработки естественного языка, корректным и обоснованным использованием математического аппарата, экспериментальными исследованиями разработанных моделей и методов.

\paragraph{Соответствие диссертации паспорту специальности.}
Тема и основные результаты диссертации соответствуют следующим областям исследований паспорта специальности 1.2.1~--- Искусственный интеллект и машинное обучение.

2 Исследования в области оценки качества и эффективности алгоритмических и программных решений для систем искусственного интеллекта и машинного обучения. Методики сравнения и выбора алгоритмических и программных решений при многих критериях.

4 Разработка методов, алгоритмов и создание систем искусственного интеллекта и машинного обучения для обработки и анализа текстов на естественном языке, для изображений, речи, биомедицины и других специальных видов данных.

16 Исследования в области специальных методов оптимизации, проблем сложность и элиминации перебора, снижения размерности.

17 Исследования в области многослойных алгоритмических конструкций, в том числе – многослойных нейросетей.

\paragraph{Апробация результатов диссертации.}
Основные результаты работы докладывались и обсуждались на Всероссийской конференции с международным участием «Математические методы распознавания образов»~(Москва, 2019, Москва, 2021, Муром, 2025), Международной конференции «Интеллектуализация обработки информации»~(Гаэта, 2018, Москва, 2020, Москва, 2022), Всероссийской научной конференции МФТИ~(Москва, 2018, 2019, 2020, 2021, 2023, 2024, 2025), Ivannikov Ispras Open Conference~(Москва, 2021, 2022, 2023, 2024), Ivannikov Memorial Workshop~(Казань, 2022), Iberian Languages Evaluation Forum co-located with the Conference of the Spanish Society for Natural Language Processing~(Андалусия, 2023, 2024), 35th Conference of Open Innovations Association~(Тампере, 2024), Fourth Workshop on Scholarly Document Processing~(Бангкок, 2024), 1st Workshop on GenAI Content Detection (GenAIDetect)~(Абу-Даби 2025), 19th International Workshop on Semantic Evaluation~(Вена, 2025).

\paragraph{Публикации.}
По теме диссертации опубликовано 56 научных работ, из
которых 17 статей в научно-технических журналах, входящих в перечень ВАК, 32~--- в изданиях, входящих в международные наукометрические базы Scopus и Web of Science.
В трудах российских и международных конференций опубликовано 39 работ. Также на основе работ автора зарегистрировано 13 программ для ЭВМ.

\paragraph{Личный вклад соискателя.}
Все выносимые на защиту результаты и положения, составляющие основное содержание диссертационного исследования, разработаны и получены лично автором или при его непосредственном участии вместе с учениками.
В работах, опубликованных в соавторстве, соискателю принадлежит определяющая роль в построении теоретических методов и направлении исследований.

\paragraph{Структура и объем работы.}
Диссертация состоит из оглавления, введения, шести разделов, заключения, списка иллюстраций, списка таблиц, перечня основных обозначений и списка литературы из~$176$ наименований. Основной текст занимает~$30$страницы.

\end{document}